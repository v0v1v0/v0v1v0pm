<div class="container">

<table style="width: 100%;"><tr>
<td>gelman.prior</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Prior Covariance Matrix for Fixed Effects.</h2>

<h3>Description</h3>

<p>Prior Covariance Matrix for Fixed Effects.</p>


<h3>Usage</h3>

<pre><code class="language-R">gelman.prior(formula, data, scale=1, intercept=scale, singular.ok=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p><code>formula</code> for the fixed effects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p><code>data.frame</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>intercept</code></td>
<td>
<p>prior standard deviation for the intercept</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>prior standard deviation for regression parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>singular.ok</code></td>
<td>
<p>logical: if <code>FALSE</code> linear dependencies in the fixed effects are removed. if <code>TRUE</code> they are left in an estimated, although all information comes form the prior</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Gelman et al. (2008) suggest that the input variables of a categorical regression are standardised and that the associated regression parameters are assumed independent in the prior.  Gelman et al. (2008) recommend a scaled t-distribution with a single degree of freedom (scaled Cauchy) and a scale of 10 for the intercept and 2.5 for the regression parameters. If the degree of freedom is infinity (i.e. a normal distribution) then a prior covariance matrix <code>B$V</code> can be defined for the regression parameters without input standardisation that corresponds to a diagonal prior <code class="reqn">{\bf D}</code> for the regression parameters had the inputs been standardised.  The diagonal elements of <code class="reqn">{\bf D}</code> are set to <code>scale^2</code> except the first which is set to <code>intercept^2</code>.  With logistic regression <code class="reqn">D=\pi^{2}/3+\sigma^{2}</code> gives a prior that is approximately flat on the probability scale, where <code class="reqn">\sigma^{2}</code> is the total variance due to the random effects. For probit regression it is <code class="reqn">D=1+\sigma^{2}</code>.
</p>


<h3>Value</h3>

<p>prior covariance matrix
</p>


<h3>Author(s)</h3>

<p>Jarrod Hadfield <a href="mailto:j.hadfield@ed.ac.uk">j.hadfield@ed.ac.uk</a></p>


<h3>References</h3>

<p>Gelman, A. et al. (2008) The Annals of Appled Statistics 2 4  1360-1383
</p>


<h3>Examples</h3>

<pre><code class="language-R">dat&lt;-data.frame(y=c(0,0,1,1), x=gl(2,2))
# data with complete separation

#####################
# probit regression #
#####################

prior1&lt;-list(
  B=list(mu=c(0,0), V=gelman.prior(~x, data=dat, scale=sqrt(1+1))), 
  R=list(V=1,fix=1))

m1&lt;-MCMCglmm(y~x, prior=prior1, data=dat, family="ordinal", verbose=FALSE)

c2&lt;-1
p1&lt;-pnorm(m1$Sol[,1]/sqrt(1+c2)) # marginal probability when x=1

#######################
# logistic regression #
#######################

prior2&lt;-list(B=list(mu=c(0,0), V=gelman.prior(~x, data=dat, scale=sqrt(pi^2/3+1))),
             R=list(V=1,fix=1))

m2&lt;-MCMCglmm(y~x, prior=prior2, data=dat, family="categorical", verbose=FALSE)

c2 &lt;- (16 * sqrt(3)/(15 * pi))^2
p2&lt;-plogis(m2$Sol[,1]/sqrt(1+c2)) # marginal probability when x=1

plot(mcmc.list(p1,p2))


</code></pre>


</div>