<div class="container">

<table style="width: 100%;"><tr>
<td>MCMCirtHier1d</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Markov Chain Monte Carlo for Hierarchical One Dimensional Item Response
Theory Model, Covariates Predicting Latent Ideal Point (Ability)</h2>

<h3>Description</h3>

<p>This function generates a sample from the posterior distribution of a one
dimensional item response theory (IRT) model, with multivariate Normal
priors on the item parameters, and a Normal-Inverse Gamma hierarchical prior
on subject ideal points (abilities).  The user supplies item-response data,
subject covariates, and priors. Note that this identification strategy
obviates the constraints used on theta in <code>MCMCirt1d</code>.
A sample from the posterior distribution is returned as an mcmc object,
which can be subsequently analyzed with functions provided in the coda
package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">MCMCirtHier1d(
  datamatrix,
  Xjdata,
  burnin = 1000,
  mcmc = 20000,
  thin = 1,
  verbose = 0,
  seed = NA,
  theta.start = NA,
  a.start = NA,
  b.start = NA,
  beta.start = NA,
  b0 = 0,
  B0 = 0.01,
  c0 = 0.001,
  d0 = 0.001,
  ab0 = 0,
  AB0 = 0.25,
  store.item = FALSE,
  store.ability = TRUE,
  drop.constant.items = TRUE,
  marginal.likelihood = c("none", "Chib95"),
  px = TRUE,
  px_a0 = 10,
  px_b0 = 10,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>datamatrix</code></td>
<td>
<p>The matrix of data.  Must be 0, 1, or missing values.  The
rows of <code>datamatrix</code> correspond to subjects and the columns correspond
to items.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Xjdata</code></td>
<td>
<p>A <code>data.frame</code> containing second-level predictor
covariates for ideal points <code class="reqn">\theta</code>. Predictors are modeled as a
linear regression on the mean vector of <code class="reqn">\theta</code>; the posterior
sample contains regression coefficients <code class="reqn">\beta</code> and common
variance <code class="reqn">\sigma^2</code>. See Rivers (2003) for a thorough
discussion of identification of IRT models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burnin</code></td>
<td>
<p>The number of burn-in iterations for the sampler.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mcmc</code></td>
<td>
<p>The number of Gibbs iterations for the sampler.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>thin</code></td>
<td>
<p>The thinning interval used in the simulation.  The number of
Gibbs iterations must be divisible by this value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>A switch which determines whether or not the progress of the
sampler is printed to the screen. If <code>verbose</code> is greater than 0 then
every <code>verbose</code>th iteration will be printed to the screen.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>The seed for the random number generator.  If NA, the Mersenne
Twister generator is used with default seed 12345; if an integer is passed
it is used to seed the Mersenne twister.  The user can also pass a list of
length two to use the L'Ecuyer random number generator, which is suitable
for parallel computation.  The first element of the list is the L'Ecuyer
seed, which is a vector of length six or NA (if NA a default seed of
<code>rep(12345,6)</code> is used).  The second element of list is a positive
substream number. See the MCMCpack specification for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>theta.start</code></td>
<td>
<p>The starting values for the subject abilities (ideal
points). This can either be a scalar or a column vector with dimension equal
to the number of voters.  If this takes a scalar value, then that value will
serve as the starting value for all of the thetas.  The default value of NA
will choose the starting values based on an eigenvalue-eigenvector
decomposition of the agreement score matrix formed from the
<code>datamatrix</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a.start</code></td>
<td>
<p>The starting values for the <code class="reqn">a</code> difficulty parameters.
This can either be a scalar or a column vector with dimension equal to the
number of items.  If this takes a scalar value, then that value will serve
as the starting value for all <code class="reqn">a</code>.  The default value of NA will set
the starting values based on a series of probit regressions that condition
on the starting values of theta.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b.start</code></td>
<td>
<p>The starting values for the <code class="reqn">b</code> discrimination
parameters. This can either be a scalar or a column vector with dimension
equal to the number of items.  If this takes a scalar value, then that value
will serve as the starting value for all <code class="reqn">b</code>.  The default value of
NA will set the starting values based on a series of probit regressions that
condition on the starting values of theta.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta.start</code></td>
<td>
<p>The starting values for the <code class="reqn">\beta</code> regression
coefficients that predict the means of ideal points <code class="reqn">\theta</code>.
This can either be a scalar or a column vector with length equal to the
number of covariates. If this takes a scalar value, then that value will
serve as the starting value for all of the betas.  The default value of NA
will set the starting values based on a linear regression of the covariates
on (either provided or generated) <code>theta.start</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b0</code></td>
<td>
<p>The prior mean of <code class="reqn">\beta</code>. Can be either a scalar or a
vector of length equal to the number of subject covariates. If a scalar all
means with be set to the passed value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>B0</code></td>
<td>
<p>The prior precision of <code class="reqn">\beta</code>.  This can either be a
scalar or a square matrix with dimensions equal to the number of betas.  If
this takes a scalar value, then that value times an identity matrix serves
as the prior precision of beta. A default proper but diffuse value of .01
ensures finite marginal likelihood for model comparison.  A value of 0 is
equivalent to an improper uniform prior for beta.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>c0</code></td>
<td>
<p><code class="reqn">c_0/2</code> is the shape parameter for the inverse Gamma
prior on <code class="reqn">\sigma^2</code> (the variance of <code class="reqn">\theta</code>). The
amount of information in the inverse Gamma prior is something
like that from <code class="reqn">c_0</code> pseudo-observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>d0</code></td>
<td>
<p><code class="reqn">d_0/2</code> is the scale parameter for the inverse Gamma
prior on <code class="reqn">\sigma^2</code> (the variance of <code class="reqn">\theta</code>). In
constructing the inverse Gamma prior, <code class="reqn">d_0</code> acts like the sum of
squared errors from the <code class="reqn">c_0</code> pseudo-observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ab0</code></td>
<td>
<p>The prior mean of <code>(a, b)</code>. Can be either a scalar or a
2-vector. If a scalar both means will be set to the passed value. The prior
mean is assumed to be the same across all items.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AB0</code></td>
<td>
<p>The prior precision of <code>(a, b)</code>.This can either be ascalar
or a 2 by 2 matrix. If this takes a scalar value, then that value times an
identity matrix serves as the prior precision. The prior precision is
assumed to be the same across all items.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>store.item</code></td>
<td>
<p>A switch that determines whether or not to store the item
parameters for posterior analysis.  <em>NOTE: In situations with many
items storing the item parameters takes an enormous amount of memory, so
<code>store.item</code> should only be <code>TRUE</code> if the chain is thinned
heavily, or for applications with a small number of items</em>.  By default, the
item parameters are not stored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>store.ability</code></td>
<td>
<p>A switch that determines whether or not to store the
ability parameters for posterior analysis.  <em>NOTE: In situations with
many individuals storing the ability parameters takes an enormous amount of
memory, so <code>store.ability</code> should only be <code>TRUE</code> if the chain is
thinned heavily, or for applications with a small number of individuals</em>.
By default, ability parameters are stored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>drop.constant.items</code></td>
<td>
<p>A switch that determines whether or not items
that have no variation should be deleted before fitting the model. Default =
TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>marginal.likelihood</code></td>
<td>
<p>Should the marginal likelihood of the
second-level model on ideal points be calculated using the method of Chib
(1995)? It is stored as an attribute of the posterior <code>mcmc</code> object and
suitable for comparison using <code>BayesFactor</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>px</code></td>
<td>
<p>Use Parameter Expansion to reduce autocorrelation in the chain?
PX introduces an unidentified parameter <code class="reqn">alpha</code> for the residual
variance in the latent data (Liu and Wu 1999). Default = TRUE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>px_a0</code></td>
<td>
<p>Prior shape parameter for the inverse-gamma distribution on
<code class="reqn">alpha</code>, the residual variance of the latent data. Default=10.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>px_b0</code></td>
<td>
<p>Prior scale parameter for the inverse-gamma distribution on
<code class="reqn">alpha</code>, the residual variance of the latent data. Default = 10</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>further arguments to be passed</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>If you are interested in fitting K-dimensional item response theory models,
or would rather identify the model by placing constraints on the item
parameters, please see <code>MCMCirtKd</code>.
</p>
<p><code>MCMCirtHier1d</code> simulates from the posterior distribution using
standard Gibbs sampling using data augmentation (a Normal draw for the
subject abilities, a multivariate Normal draw for (second-level) subject
ability predictors, an Inverse-Gamma draw for the (second-level) variance of
subject abilities, a multivariate Normal draw for the item parameters, and a
truncated Normal draw for the latent utilities). The simulation proper is
done in compiled C++ code to maximize efficiency.  Please consult the coda
documentation for a comprehensive list of functions that can be used to
analyze the posterior sample.
</p>
<p>The model takes the following form.  We assume that each subject has an
subject ability (ideal point) denoted <code class="reqn">\theta_j</code> and that each
item has a difficulty parameter <code class="reqn">a_i</code> and discrimination parameter
<code class="reqn">b_i</code>.  The observed choice by subject <code class="reqn">j</code> on item
<code class="reqn">i</code> is the observed data matrix which is <code class="reqn">(I \times J)</code>.
We assume that the choice is dictated by an unobserved utility:
</p>
<p style="text-align: center;"><code class="reqn">z_{i,j} = -\alpha_i + \beta_i \theta_j + \varepsilon_{i,j}</code>
</p>

<p>Where the errors are assumed to be distributed standard Normal.
This constitutes the measurement or level-1 model. The subject abilities
(ideal points) are modeled by a second level Normal linear predictor for
subject covariates <code>Xjdata</code>, with common variance
<code class="reqn">\sigma^2</code>. The parameters of interest are the subject
abilities (ideal points), item parameters, and second-level coefficients.
</p>
<p>We assume the following priors.  For the subject abilities (ideal points):
</p>
<p style="text-align: center;"><code class="reqn">\theta_j \sim \mathcal{N}(\mu_{\theta} ,T_{0}^{-1})</code>
</p>

<p>For the item parameters, the prior is:
</p>
<p style="text-align: center;"><code class="reqn">\left[a_i, b_i \right]' \sim \mathcal{N}_2 (ab_{0},AB_{0}^{-1})</code>
</p>

<p>The model is identified by the proper priors on the item parameters and
constraints placed on the ability parameters.
</p>
<p>As is the case with all measurement models, make sure that you have plenty
of free memory, especially when storing the item parameters.
</p>


<h3>Value</h3>

<p>An <code>mcmc</code> object that contains the sample from the posterior
distribution. This object can be summarized by functions provided by the
coda package. If <code>marginal.likelihood = "Chib95"</code> the object will have
attribute <code>logmarglike</code>.
</p>


<h3>Author(s)</h3>

<p>Michael Malecki, <a href="mailto:mike@crunch.io">mike@crunch.io</a>,
<a href="https://github.com/malecki">https://github.com/malecki</a>.
</p>


<h3>References</h3>

<p>James H. Albert. 1992. “Bayesian Estimation of Normal Ogive
Item Response Curves Using Gibbs Sampling." <em>Journal of Educational
Statistics</em>.  17: 251–269.
</p>
<p>Joshua Clinton, Simon Jackman, and Douglas Rivers. 2004. “The Statistical
Analysis of Roll Call Data."  <em>American Political Science Review</em> 98:
355–370.
</p>
<p>Valen E. Johnson and James H. Albert. 1999. “Ordinal Data Modeling."
Springer: New York.
</p>
<p>Liu, Jun S. and Ying Nian Wu. 1999. “Parameter Expansion for Data
Augmentation.” <em>Journal of the American Statistical Association</em> 94:
1264–1274.
</p>
<p>Andrew D. Martin, Kevin M. Quinn, and Jong Hee Park. 2011.  “MCMCpack:
Markov Chain Monte Carlo in R.”, <em>Journal of Statistical Software</em>.
42(9): 1-21.  <a href="https://doi.org/10.18637/jss.v042.i09">doi:10.18637/jss.v042.i09</a>.
</p>
<p>Daniel Pemstein, Kevin M. Quinn, and Andrew D. Martin.  2007.  <em>Scythe
Statistical Library 1.0.</em> <a href="http://scythe.wustl.edu.s3-website-us-east-1.amazonaws.com/">http://scythe.wustl.edu.s3-website-us-east-1.amazonaws.com/</a>.
</p>
<p>Martyn Plummer, Nicky Best, Kate Cowles, and Karen Vines. 2006.  “Output
Analysis and Diagnostics for MCMC (CODA)”, <em>R News</em>. 6(1): 7-11.
<a href="https://CRAN.R-project.org/doc/Rnews/Rnews_2006-1.pdf">https://CRAN.R-project.org/doc/Rnews/Rnews_2006-1.pdf</a>.
</p>
<p>Douglas Rivers.  2004.  “Identification of Multidimensional Item-Response
Models."  Stanford University, typescript.
</p>


<h3>See Also</h3>

<p><code>plot.mcmc</code>,<code>summary.mcmc</code>,
<code>MCMCirtKd</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
   ## Not run: 
data(SupremeCourt)

Xjdata &lt;- data.frame(presparty= c(1,1,0,1,1,1,1,0,0),
                     sex= c(0,0,1,0,0,0,0,1,0))

## Parameter Expansion reduces autocorrelation.
  posterior1 &lt;- MCMCirtHier1d(t(SupremeCourt),
                   burnin=50000, mcmc=10000, thin=20,
                   verbose=10000,
                   Xjdata=Xjdata,
                   marginal.likelihood="Chib95",
		   px=TRUE)

## But, you can always turn it off.
  posterior2 &lt;- MCMCirtHier1d(t(SupremeCourt),
                   burnin=50000, mcmc=10000, thin=20,
                   verbose=10000,
                   Xjdata=Xjdata,
                   #marginal.likelihood="Chib95",
		   px=FALSE)
## Note that the hierarchical model has greater autocorrelation than
## the naive IRT model.
  posterior0 &lt;- MCMCirt1d(t(SupremeCourt),
                        theta.constraints=list(Scalia="+", Ginsburg="-"),
                        B0.alpha=.2, B0.beta=.2,
                        burnin=50000, mcmc=100000, thin=100, verbose=10000,
                        store.item=FALSE)

## Randomly 10% Missing -- this affects the expansion parameter, increasing
## the variance of the (unidentified) latent parameter alpha.

   scMiss &lt;- SupremeCourt
   scMiss[matrix(as.logical(rbinom(nrow(SupremeCourt)*ncol(SupremeCourt), 1, .1)),
      dim(SupremeCourt))] &lt;- NA

   posterior1.miss &lt;- MCMCirtHier1d(t(scMiss),
                   burnin=80000, mcmc=10000, thin=20,
                   verbose=10000,
                   Xjdata=Xjdata,
                   marginal.likelihood="Chib95",
		   px=TRUE)

   
## End(Not run)

</code></pre>


</div>