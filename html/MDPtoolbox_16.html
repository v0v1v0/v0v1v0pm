<div class="container">

<table style="width: 100%;"><tr>
<td>mdp_policy_iteration</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Solves discounted MDP using policy iteration algorithm
</h2>

<h3>Description</h3>

<p>Solves discounted MDP with policy iteration algorithm
</p>


<h3>Usage</h3>

<pre><code class="language-R">mdp_policy_iteration(P, R, discount, policy0, max_iter, eval_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>P</code></td>
<td>

<p>transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>

<p>reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>discount</code></td>
<td>

<p>discount factor.
discount is a real which belongs to ]0; 1[.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>policy0</code></td>
<td>
<p>(optional) starting policy. policy0 is a S length vector. By default, policy0 is the policy which maximizes the expected immediate reward.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_iter</code></td>
<td>
<p>(optional)
maximum number of iterations to be done.
max_iter is an integer greater than 0. 
By default, max_iter is set to 1000.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eval_type</code></td>
<td>
<p>(optional)
define function used to evaluate a policy.
eval_type is 0 for mdp_eval_policy_matrix use, mdp_eval_policy_iterative is used in all other cases. 
By default, eval_type is set to 0.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>mdp_policy_iteration applies the policy iteration algorithm to solve discounted MDP. The algorithm consists in improving the policy iteratively, using the evaluation of the current policy. Iterating is stopped when two successive policies are identical or when a specified number (max_iter) of iterations have been performed.
</p>


<h3>Value</h3>



<table>
<tr style="vertical-align: top;">
<td><code>V</code></td>
<td>
<p>optimal value fonction. V is a S length vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>policy</code></td>
<td>
<p>optimal policy. policy is a S length vector. Each element is an integer corre-
sponding to an action which maximizes the value function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>number of iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cpu_time</code></td>
<td>
<p>CPU time used to run the program</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R"># With a non-sparse matrix
P &lt;- array(0, c(2,2,2))
P[,,1] &lt;- matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE)
P[,,2] &lt;- matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE)
R &lt;- matrix(c(5, 10, -1, 2), 2, 2, byrow=TRUE)
mdp_policy_iteration(P, R, 0.9)

# With a sparse matrix
P &lt;- list()
P[[1]] &lt;- Matrix(c(0.5, 0.5, 0.8, 0.2), 2, 2, byrow=TRUE, sparse=TRUE)
P[[2]] &lt;- Matrix(c(0, 1, 0.1, 0.9), 2, 2, byrow=TRUE, sparse=TRUE)
mdp_policy_iteration(P, R, 0.9)

</code></pre>


</div>