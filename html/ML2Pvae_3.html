<div class="container">

<table style="width: 100%;"><tr>
<td>build_vae_correlated</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Build a VAE that fits to a normal, full covariance N(m,S) latent distribution</h2>

<h3>Description</h3>

<p>Build a VAE that fits to a normal, full covariance N(m,S) latent distribution
</p>


<h3>Usage</h3>

<pre><code class="language-R">build_vae_correlated(
  num_items,
  num_skills,
  Q_matrix,
  mean_vector = rep(0, num_skills),
  covariance_matrix = diag(num_skills),
  model_type = 2,
  enc_hid_arch = c(ceiling((num_items + num_skills)/2)),
  hid_enc_activations = rep("sigmoid", length(enc_hid_arch)),
  output_activation = "sigmoid",
  kl_weight = 1,
  learning_rate = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>num_items</code></td>
<td>
<p>an integer giving the number of items on the assessment; also the number of nodes in the input/output layers of the VAE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_skills</code></td>
<td>
<p>an integer giving the number of skills being evaluated; also the dimensionality of the distribution learned by the VAE</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q_matrix</code></td>
<td>
<p>a binary, <code>num_skills</code> by <code>num_items</code> matrix relating the assessment items with skills</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mean_vector</code></td>
<td>
<p>a vector of length <code>num_skills</code> specifying the mean of each latent trait; the default of <code>rep(0, num_skills)</code> should almost always be used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>covariance_matrix</code></td>
<td>
<p>a symmetric, positive definite, <code>num_skills</code> by <code>num_skills</code> matrix giving the covariance of the latent traits</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model_type</code></td>
<td>
<p>either 1 or 2, specifying a 1 parameter (1PL) or 2 parameter (2PL) model; if 1PL, then all decoder weights are fixed to be equal to one</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>enc_hid_arch</code></td>
<td>
<p>a vector detailing the size of hidden layers in the encoder; the number of hidden layers is determined by the length of this vector</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>hid_enc_activations</code></td>
<td>
<p>a vector specifying the activation function in each hidden layer in the encoder; must be the same length as <code>enc_hid_arch</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output_activation</code></td>
<td>
<p>a string specifying the activation function in the output of the decoder; the ML2P model always used 'sigmoid'</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>kl_weight</code></td>
<td>
<p>an optional weight for the KL divergence term in the loss function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>learning_rate</code></td>
<td>
<p>an optional parameter for the adam optimizer</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>returns three keras models: the encoder, decoder, and vae
</p>


<h3>Examples</h3>

<pre><code class="language-R">
Q &lt;- matrix(c(1,0,1,1,0,1,1,0), nrow = 2, ncol = 4)
cov &lt;- matrix(c(.7,.3,.3,1), nrow = 2, ncol = 2)
models &lt;- build_vae_correlated(4, 2, Q,
          mean_vector = c(-0.5, 0), covariance_matrix = cov,
          enc_hid_arch = c(6, 3), hid_enc_activation = c('sigmoid', 'relu'),
          output_activation = 'tanh',
          kl_weight = 0.1)
vae &lt;- models[[3]]

</code></pre>


</div>