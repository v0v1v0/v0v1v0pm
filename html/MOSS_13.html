<div class="container">

<table style="width: 100%;"><tr>
<td>ssvdEN</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Sparse Singular Value Decomposition via Elastic Net.</h2>

<h3>Description</h3>

<p>This function performs sparse singular value decomposition (SVD)
on a matrix 'x' via Elastic Net types of penalties.
For one PC (rank 1 case), the algorithm finds left and right Eigenvectors
(u and w, respectively), that minimize:
||x - u w'||_F^2 +
lambda_w (alpha_w||w||_1 +
(1 - alpha_w)||w||_F^2) +
lambda_u (alpha||u||_1 + (1 - alpha_u)||u||_F^2)
such that ||u|| = 1.
The right Eigen vector is obtained from v = w / ||w|| and the
corresponding Eigen value = u^T x v.
The penalties lambda_u and lambda_w are mapped from
specified desired degree of sparsity
(dg.spar.features &amp; dg.spar.subjects).
</p>


<h3>Usage</h3>

<pre><code class="language-R">ssvdEN(
  O,
  n.PC = 1,
  dg.spar.features = NULL,
  dg.spar.subjects = NULL,
  maxit = 500,
  tol = 0.001,
  scale.arg = TRUE,
  center.arg = TRUE,
  approx.arg = FALSE,
  alpha.f = 1,
  alpha.s = 1,
  svd.0 = NULL,
  s.values = TRUE,
  ncores = 1,
  exact.dg = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>O</code></td>
<td>
<p>Numeric matrix of n subjects (rows) and p features (columns).
It can be a File-backed Big Matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.PC</code></td>
<td>
<p>Number of desired principal axes. Numeric. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dg.spar.features</code></td>
<td>
<p>Degree of sparsity at the features level.
Numeric. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dg.spar.subjects</code></td>
<td>
<p>Degree of sparsity at the subjects level.
Numeric. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Maximum number of iterations for the sparse SVD algorithm.
Numeric. Defaults to 500.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>
<p>Convergence tolerance for the sparse SVD algorithm.
Numeric. Defaults to 0.001.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale.arg</code></td>
<td>
<p>Should O be scaled? Logical. Defaults to TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center.arg</code></td>
<td>
<p>Should O be centered? Logical. Defaults to TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>approx.arg</code></td>
<td>
<p>Should we use standard SVD or random approximations?
Defaults to FALSE. If TRUE &amp; is(O,'matrix') == TRUE, irlba is called.
If TRUE &amp; is(O, "FBM") == TRUE, big_randomSVD is called.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha.f</code></td>
<td>
<p>Elastic net mixture parameter at the features level.
Measures the compromise between lasso (alpha = 1) and
ridge (alpha = 0) types of sparsity. Numeric. Defaults to 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha.s</code></td>
<td>
<p>Elastic net mixture parameter at the subjects level.
Defaults to alpha.s = 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>svd.0</code></td>
<td>
<p>List containing an initial SVD. Defaults to NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s.values</code></td>
<td>
<p>Should the singular values be calculated? Logical.
Defaults to TRUE.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncores</code></td>
<td>
<p>Number of cores used by big_randomSVD.
Default does not use parallelism. Ignored when class(O)!=FBM.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exact.dg</code></td>
<td>
<p>Should we compute exact degrees of sparsity? Logical.
Defaults to FALSE. Only relevant When alpha.s or alpha.f are
in the (0,1) interval and exact.dg = TRUE.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function allows the use of the base svd function
for relatively small problems.
For larger problems, functions for fast-partial SVD
(irlba and big_randomSVD, from irlba and bigstatsr packages) are used.
</p>


<h3>Value</h3>

<p>A list with the results of the (sparse) SVD, containing:
</p>

<ul>
<li>
<p> u: Matrix with left eigenvectors.
</p>
</li>
<li>
<p> v: Matrix with right eigenvectors.
</p>
</li>
<li>
<p> d: Matrix with singular values.
</p>
</li>
</ul>
<h3>Note</h3>

<p>When elastic net is used ('alpha.s' or 'alpha.f' in the (0,1)
interval), the resulting number of non-zero subjects or features
is larger than the 'dg.spar.subjects' or 'dg.spar.features' values.
This allows to rapidly increase the number of non-zero elements when
tuning the degrees of sparsity with function ssvdEN_sol_path. 
In order to get exact values for the degrees
of sparsity at subjects or features levels, the user needs to
set the value of 'exact.dg' parameter from 'FALSE' (the default)
to 'TRUE'.
</p>


<h3>References</h3>


<ul>
<li>
<p> Shen, Haipeng, and Jianhua Z. Huang. 2008.
Sparse Principal Component Analysis via Regularized Low
Rank Matrix Approximation.
Journal of Multivariate Analysis 99 (6).
Academic Press:1015_34.
</p>
</li>
<li>
<p> Baglama, Jim, Lothar Reichel, and B W Lewis. 2018.
Irlba: Fast Truncated Singular Value Decomposition and
Principal Components Analysis for Large Dense and Sparse Matrices.
</p>
</li>
</ul>
<h3>Examples</h3>

<pre><code class="language-R">library("MOSS")

# Extracting simulated omic blocks.
sim_blocks &lt;- simulate_data()$sim_blocks

X &lt;- sim_blocks$`Block 3`

# Equal to svd solution: exact singular vectors and values.
out &lt;- ssvdEN(X, approx.arg = FALSE)

# Uses irlba to get approximated singular vectors and values.
library(irlba)
out &lt;- ssvdEN(X, approx.arg = TRUE)
# Uses bigstatsr to get approximated singular vectors and values
# of a Filebacked Big Matrix.
library(bigstatsr)
out &lt;- ssvdEN(as_FBM(X), approx.arg = TRUE)

# Sampling a number of subjects and features for a fix sparsity degree.
s.u &lt;- sample(1:nrow(X), 1)
s.v &lt;- sample(1:ncol(X), 1)

# Lasso penalties.
all.equal(sum(ssvdEN(X, dg.spar.features = s.v)$v != 0), s.v)
all.equal(
  unique(colSums(ssvdEN(X, dg.spar.features = s.v, n.PC = 5)$v
  != 0)),
  s.v
)

all.equal(sum(ssvdEN(X, dg.spar.subjects = s.u)$u != 0), s.u)
all.equal(
  unique(colSums(ssvdEN(X, dg.spar.subjects = s.u, n.PC = 5)$u
  != 0)),
  s.u
)

out &lt;- ssvdEN(X, dg.spar.features = s.v, dg.spar.subjects = s.u)
all.equal(sum(out$u != 0), s.u)
all.equal(sum(out$v != 0), s.v)

out &lt;- ssvdEN(X,
  dg.spar.features = s.v, dg.spar.subjects = s.u,
  n.PC = 10
)
all.equal(unique(colSums(out$u != 0)), s.u)
all.equal(unique(colSums(out$v != 0)), s.v)

# Ridge penalties.
all.equal(
  sum(ssvdEN(X, dg.spar.features = s.v, alpha.f = 0)$v != 0),
  ncol(X)
)
all.equal(
  unique(colSums(ssvdEN(X,
    dg.spar.features = s.v, n.PC = 5,
    alpha.f = 0
  )$v != 0)),
  ncol(X)
)

all.equal(
  sum(ssvdEN(X, dg.spar.subjects = s.u, alpha.s = 0)$u != 0),
  nrow(X)
)
all.equal(
  unique(colSums(ssvdEN(X,
    dg.spar.subjects = s.u, n.PC = 5,
    alpha.s = 0
  )$u != 0)),
  nrow(X)
)

out &lt;- ssvdEN(X,
  dg.spar.features = s.v, dg.spar.subjects = s.u,
  alpha.f = 0, alpha.s = 0
)
all.equal(sum(out$u != 0), nrow(X))
all.equal(sum(out$v != 0), ncol(X))

out &lt;- ssvdEN(X,
  dg.spar.features = s.v, dg.spar.subjects = s.u,
  n.PC = 10, alpha.f = 0,
  alpha.s = 0
)
all.equal(unique(colSums(out$u != 0)), nrow(X))
all.equal(unique(colSums(out$v != 0)), ncol(X))

# Elastic Net penalties.
sum(ssvdEN(X, dg.spar.features = s.v, alpha.f = 0.5)$v != 0) &gt;= s.v
all(unique(colSums(ssvdEN(X,
  dg.spar.features = s.v, n.PC = 5,
  alpha.f = 0.5
)$v != 0)) &gt;= s.v)

sum(ssvdEN(X, dg.spar.subjects = s.u, alpha.s = 0.5)$u != 0) &gt;= s.u
all(unique(colSums(ssvdEN(X,
  dg.spar.subjects = s.u, n.PC = 5,
  alpha.s = 0.5
)$u != 0)) &gt;= s.u)

# Elastic Net penalties with exact degrees of sparsity.
sum(ssvdEN(X,
  dg.spar.features = s.v, alpha.f = 0.5,
  exact.dg = TRUE
)$v != 0) == s.v
all(unique(colSums(ssvdEN(X,
  dg.spar.features = s.v, n.PC = 5,
  alpha.f = 0.5, exact.dg = TRUE
)$v != 0)) == s.v)

sum(ssvdEN(X,
  dg.spar.subjects = s.u, alpha.s = 0.5,
  exact.dg = TRUE
)$u != 0) == s.u
all(unique(colSums(ssvdEN(X,
  dg.spar.subjects = s.u, n.PC = 5,
  alpha.s = 0.5, exact.dg = TRUE
)$u != 0)) == s.u)

</code></pre>


</div>