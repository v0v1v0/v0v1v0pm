<div class="container">

<table style="width: 100%;"><tr>
<td>MXM-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
This is an R package that currently implements feature selection methods for identifying minimal, 
statistically-equivalent and equally-predictive feature subsets. Additionally, the package includes two algorithms for 
constructing the skeleton of a Bayesian network.
</h2>

<h3>Description</h3>

<p>'MXM' stands for Mens eX Machina, meaning 'Mind from the Machine' in Latin. The package provides source code for the SES algorithm and for some appropriate statistical conditional independence tests. (Fisher and Spearman correlation, 
G-square test are some examples. Currently the response variable can be univariate or multivariate Euclidean, 
proportions within 0 and 1, compositional data without zeros and  ones, binary, nominal or ordinal multinomial, 
count data (handling also overdispersed and with more zeros than expected), longitudinal, clustered data, survival 
and case-control. Robust versions are also available in some cases and a K-fold cross validation is offered. 
Bayesian network related algorithms and ridge reression are also included. Read the package's help pages for more details.
</p>
<p>MMPC and SES can handle even thousands of variables and for some tests, even many sample sizes of tens of thousands. 
The user is best advised to check his variables in the beginning. For some regressions, logistic and Poisson for example, we have used C++ codes for speed reasons. 
</p>
<p>For more information the reader is addressed to 
</p>
<p>Lagani V., Athineou G., Farcomeni A., Tsagris M. and Tsamardinos I. (2017). Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. Journal of Statistical Software, 80(7), doi:10.18637/jss.v080.i07 
and 
</p>
<p>Tsagris, M. and Tsamardinos, I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505.
</p>


<h3>Details</h3>


<table>
<tr>
<td style="text-align: left;">
Package: </td>
<td style="text-align: left;"> MXM</td>
</tr>
<tr>
<td style="text-align: left;">
Type: </td>
<td style="text-align: left;"> Package</td>
</tr>
<tr>
<td style="text-align: left;">
Version: </td>
<td style="text-align: left;"> 1.5.5</td>
</tr>
<tr>
<td style="text-align: left;">
Date: </td>
<td style="text-align: left;"> 2022-08-24</td>
</tr>
<tr>
<td style="text-align: left;">
License: </td>
<td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<h3>Maintainer</h3>

<p>Konstantina Biza <a href="mailto:kbiza@csd.uoc.gr">kbiza@csd.uoc.gr</a>.
</p>


<h3>Note</h3>

<p>Acknowledgments:
The research leading to these results has received funding from the European Research Council under the 
European Union's Seventh Framework Programme (FP/2007-2013) / ERC Grant Agreement n. 617393.
</p>
<p>Michail Tsagris would like to express his acknowledgments to Marios Dimitriadis and Manos Papadakis, undergraduate students in the department of computer science, university of Crete, for their programming tips and advice. Their help has been very valuable. Dr Uwe Ligges and Prof Kurt Hornik from the CRAN team are greatly acknowledged for their assistance. Prof Achim Zeileis is greatly acknowledged for this help with the quasi Poisson and quasi binomial regression models. Christina Chatzipantsiou and Kleio Maria Verrou are acknowledged for their suggestions. Nikolaos Pandis from the University of Bern is acknowledged for his suggestion of the AFT (regression) models and for his suggestions. Michail is grateful to James Dalgleish from Columbia University who suggested that we mention, in various places, that most algorithms return the logarithm of the p-values and not the p-values. Stavros Lymperiadis provided a very useful example where weights are used in a regression model; in surveys when stratified random sampling has been applied. Dr David Gomez Cabrero Lopez is also acknowledged. Margarita Rebolledo is acknowledged for spotting a bug. Zurab Khasidashvili from Intel Israel is acknowledged for spotting a bug in the function mmmb(). Teny Handhayani (PhD student at the University of York) spotted a bug in the conditional independence tests with mixed data and she is acknowledged for that. Dr. Kjell Jorner (Postdoctoral Fellow at the Department of Chemistry, University of Toronto) spotted a bug in two performance metrics of the bbc() function and he is acknowledged for that.
</p>
<p><b>Disclaimer:</b> Professor Tsamardinos is the creator of this package and Dr Lagani supervised Mr Athineou build it. Dr Tsagris is the current maintainer.
</p>


<h3>Author(s)</h3>

<p>Ioannis Tsamardinos &lt;tsamard@csd.uoc.gr&gt;, Vincenzo Lagani &lt;vlagani@csd.uoc.gr&gt;, Giorgos Athineou &lt;athineou@csd.uoc.gr&gt;, Michail Tsagris &lt;mtsagris@uoc.gr&gt;, Giorgos Borboudakis &lt;borbudak@csd.uoc.gr&gt;, Anna Roumpelaki &lt;anna.roumpelaki@gmail.com&gt;, Konstantina Biza &lt;kbiza@csd.uoc.gr&gt;.
</p>


<h3>References</h3>

<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K., &amp; Tsamardinos, I. (2022). The <code class="reqn">\gamma</code>-OMP algorithm for feature selection with application to gene expression data. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(2): 1214-1224.
</p>
<p>Tsagris, M. and Tsamardinos, I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsagris, M. (2019). Bayesian Network Learning with the PC Algorithm: An Improved and Correct Variation. Applied Artificial Intelligence, 33(2):101-123.
</p>
<p>Tsagris, M., Lagani, V. and Tsamardinos, I. (2018). Feature selection for high-dimensional temporal data. 
BMC Bioinformatics, 19:17. 
</p>
<p>Tsagris, M., Borboudakis, G., Lagani, V. and Tsamardinos, I. (2018). Constraint-based causal discovery with mixed data. International Journal of Data Science and Analytics, 6(1): 19-30. 
</p>
<p>Tsagris, M., Papadovasilakis, Z., Lakiotaki, K. and Tsamardinos, I. (2018). Efficient feature selection on gene expression data: Which algorithm to use? BioRxiv preprint. 
</p>
<p>Lagani V., Athineou G., Farcomeni A., Tsagris M. and Tsamardinos I. (2017). Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. Journal of Statistical Software, 80(7), doi:10.18637/jss.v080.i07.
</p>
<p>Chen S., Billings S. A., and Luo W. (1989). Orthogonal least squares methods and their application to non-linear system identification. International Journal of control, 50(5), 1873-1896.
http://eprints.whiterose.ac.uk/78100/1/acse
</p>
<p>Davis G. (1994). Adaptive Nonlinear Approximations. PhD thesis. 
http://www.geoffdavis.net/papers/dissertation.pdf
</p>
<p>Demidenko E. (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>
<p>Gharavi-Alkhansari M., anz Huang T. S. (1998, May). A fast orthogonal matching pursuit algorithm. In Acoustics, Speech and Signal Processing, 1998. 
Proceedings of the 1998 IEEE International Conference on (Vol. 3, pp. 1389-1392). 
</p>
<p>Lagani V., Kortas G. and Tsamardinos I. (2013), Biomarker signature identification in "omics" with multiclass outcome. Computational and Structural Biotechnology Journal, 6(7):1-7.
</p>
<p>Liang  K.Y.  and  Zeger  S.L. (1986). Longitudinal data analysis using generalized linear models. 
Biometrika, 73(1): 13-22.
</p>
<p>Mallat S. G. &amp; Zhang Z. (1993). Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 41(12), 3397-3415.
https://www.di.ens.fr/~mallat/papiers/MallatPursuit93.pdf
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in
Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Pati Y. C., Rezaiifar R. and Krishnaprasad P. S. (1993). Orthogonal matching pursuit: Recursive function approximation with applications to wavelet 
decomposition. In Signals, Systems and Computers. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on. IEEE.
</p>
<p>Prentice R.L. and Zhao L.P. (1991). Estimating equations for parameters in means and covariances
of multivariate discrete and continuous responses. Biometrics, 47(3): 825-839.
</p>
<p>Spirtes P.,  Glymour C. and Scheines R. (2001). Causation, Prediction, and Search. The MIT Press, Cambridge, MA, USA, 3nd edition.
</p>
<p>Tsamardinos I., Greasidou E. and Borboudakis G. (2018). Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation. Machine Learning 107(12): 1895-1922.  
</p>
<p>Tsamardinos I., Lagani V. and Pappas D. (2012) Discovering multiple, equivalent biomarker signatures. In proceedings of the 7th conference of the Hellenic Society for Computational Biology &amp; Bioinformatics - HSCBB12.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 65(1), 31-78.
</p>
<p>Tsamardinos I., Aliferis C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal relations. In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining p. 673-678. 
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874
</p>
<p>Zhang, Jiji. (2008). On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artificial Intelligence 172(16): 1873â€“1896.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>


<h3>See Also</h3>

<p><code>SES, MMPC, fbed.reg, gomp, pc.sel, censIndCR,testIndFisher, testIndLogistic, gSquare, testIndRQ</code>
</p>


</div>