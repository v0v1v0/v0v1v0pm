<div class="container">

<table style="width: 100%;"><tr>
<td>Symmetric conditional independence test with clustered data</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Symmetric conditional independence test with clustered data
</h2>

<h3>Description</h3>

<p>Symmetric conditional independence test with clustered data.
</p>


<h3>Usage</h3>

<pre><code class="language-R">glmm.ci.mm(ind1, ind2, cs = NULL, dat, group) 
gee.ci.mm(ind1, ind2, cs = NULL, dat, group, se = "jack") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ind1</code></td>
<td>

<p>The index of the one variable to be considered. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ind2</code></td>
<td>

<p>The index of the other variable to be considered. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cs</code></td>
<td>

<p>The index or indices of the conditioning set of variable(s). If you have no variables set this equal to 0.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dat</code></td>
<td>

<p>A numerical matrix with data. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>

<p>This is a numerical vector denoting the groups or the subjects.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se</code></td>
<td>

<p>The method for estimating standard errors. This is very important and crucial. The available options for Gaussian, Logistic and Poisson regression are: a) 'san.se': the usual robust estimate. b) 'jack': approximate jackknife variance estimate. c) 'j1s': if 1-step jackknife variance estimate and d) 'fij': fully iterated jackknife variance estimate. If you have many clusters (sets of repeated measurements) "san.se" is fine as it is asympotically correct, plus jacknife estimates will take longer. If you have a few clusters, then maybe it's better to use jacknife estimates. 
</p>
<p>The jackknife variance estimator was suggested by Paik (1988), which is quite suitable for cases when the number of subjects is small (K &lt; 30), as in many biological studies. The simulation studies conducted by Ziegler et al. (2000) and Yan and Fine (2004) showed that the approximate jackknife estimates are in many cases in good agreement with the fully iterated ones. 
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Two linear random intercept models are fitted, one for each variable and the p-value of the hypothesis test that the other variable is significant 
is calculated. These two p-values are combined in a meta-analytic way. The models fitted are either linear, logistic and Poisson regression.
</p>


<h3>Value</h3>

<p>A vector including the test statistic, it's associated p-value and the relevant degrees of freedom. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>
</p>


<h3>References</h3>

<p>Tsagris M. (2019). Bayesian network learning with the PC algorithm: an improved and correct variation. 
Applied Artificial Intelligence, 33(2): 101-123.
</p>
<p>Tsagris M., Borboudakis G., Lagani V. and Tsamardinos I. (2018). Constraint-based Causal Discovery with Mixed Data. 
International Journal of Data Science and Analytics. 
</p>
<p>Paik M.C. (1988). Repeated measurement analysis for nonnormal data in small samples. Communications in Statistics-Simulation and Computation, 17(4): 1155-1171.
</p>
<p>Ziegler A., Kastner C., Brunner D. and Blettner M. (2000). Familial associations of lipid profiles: A generalised estimating equations approach. Statistics in medicine, 19(24): 3345-3357
</p>
<p>Yan J. and Fine J. (2004). Estimating equations for association structures. Statistics in medicine, 23(6): 859-874.
</p>
<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, 2nd Edition. New Jersey: Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code> pc.skel, condi, testIndGLMMReg </code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## we generate two independent vectors of clustered data
s1 &lt;- matrix(1.5, 4, 4)
diag(s1) &lt;- 2.5
s2 &lt;- matrix(1.5, 4, 4)
diag(s2) &lt;- 2
x1 &lt;- MASS::mvrnorm(10, rnorm(4), s1)  
x1 &lt;- as.vector( t(x1) )
x2 &lt;- MASS::mvrnorm(10, rnorm(4), s2)  
x2 &lt;- as.vector( t(x2) )
id &lt;- rep(1:10, each = 4)
glmm.ci.mm(1, 2, dat = cbind(x1,x2), group = id)
gee.ci.mm(1, 2, dat = cbind(x1,x2), group = id)
</code></pre>


</div>