<div class="container">

<table style="width: 100%;"><tr>
<td>MLControl</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Resampling Controls</h2>

<h3>Description</h3>

<p>Structures to define and control sampling methods for estimation of model
predictive performance in the <span class="pkg">MachineShop</span> package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">BootControl(
  samples = 25,
  weights = TRUE,
  seed = sample(.Machine$integer.max, 1)
)

BootOptimismControl(
  samples = 25,
  weights = TRUE,
  seed = sample(.Machine$integer.max, 1)
)

CVControl(
  folds = 10,
  repeats = 1,
  weights = TRUE,
  seed = sample(.Machine$integer.max, 1)
)

CVOptimismControl(
  folds = 10,
  repeats = 1,
  weights = TRUE,
  seed = sample(.Machine$integer.max, 1)
)

OOBControl(
  samples = 25,
  weights = TRUE,
  seed = sample(.Machine$integer.max, 1)
)

SplitControl(
  prop = 2/3,
  weights = TRUE,
  seed = sample(.Machine$integer.max, 1)
)

TrainControl(weights = TRUE, seed = sample(.Machine$integer.max, 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>samples</code></td>
<td>
<p>number of bootstrap samples.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>logical indicating whether to return case weights in resampled
output for the calculation of performance metrics.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>integer to set the seed at the start of resampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>folds</code></td>
<td>
<p>number of cross-validation folds (K).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>repeats</code></td>
<td>
<p>number of repeats of the K-fold partitioning.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prop</code></td>
<td>
<p>proportion of cases to include in the training set
(<code>0 &lt; prop &lt; 1</code>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>BootControl</code> constructs an <code>MLControl</code> object for simple bootstrap
resampling in which models are fit with bootstrap resampled training sets and
used to predict the full data set (Efron and Tibshirani 1993).
</p>
<p><code>BootOptimismControl</code> constructs an <code>MLControl</code> object for
optimism-corrected bootstrap resampling (Efron and Gong 1983, Harrell et al.
1996).
</p>
<p><code>CVControl</code> constructs an <code>MLControl</code> object for repeated K-fold
cross-validation (Kohavi 1995).  In this procedure, the full data set is
repeatedly partitioned into K-folds.  Within a partitioning, prediction is
performed on each of the K folds with models fit on all remaining folds.
</p>
<p><code>CVOptimismControl</code> constructs an <code>MLControl</code> object for
optimism-corrected cross-validation resampling (Davison and Hinkley 1997,
eq. 6.48).
</p>
<p><code>OOBControl</code> constructs an <code>MLControl</code> object for out-of-bootstrap
resampling in which models are fit with bootstrap resampled training sets and
used to predict the unsampled cases.
</p>
<p><code>SplitControl</code> constructs an <code>MLControl</code> object for splitting data
into a separate training and test set (Hastie et al. 2009).
</p>
<p><code>TrainControl</code> constructs an <code>MLControl</code> object for training and
performance evaluation to be performed on the same training set (Efron 1986).
</p>


<h3>Value</h3>

<p>Object that inherits from the <code>MLControl</code> class.
</p>


<h3>References</h3>

<p>Efron, B., &amp; Tibshirani, R. J. (1993). <em>An introduction to the
bootstrap</em>. Chapman &amp; Hall/CRC.
</p>
<p>Efron, B., &amp; Gong, G. (1983). A leisurely look at the bootstrap, the
jackknife, and cross-validation. <em>The American Statistician</em>,
<em>37</em>(1), 36-48.
</p>
<p>Harrell, F. E., Lee, K. L., &amp; Mark, D. B. (1996). Multivariable prognostic
models: Issues in developing models, evaluating assumptions and adequacy, and
measuring and reducing errors. <em>Statistics in Medicine</em>, <em>15</em>(4),
361-387.
</p>
<p>Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy
estimation and model selection. In <em>IJCAI'95: Proceedings of the 14th
International Joint Conference on Artificial Intelligence</em> (vol. 2, pp.
1137-1143). Morgan Kaufmann Publishers Inc.
</p>
<p>Davison, A. C., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and their
application</em>. Cambridge University Press.
</p>
<p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements of
statistical learning: data mining, inference, and prediction</em> (2nd ed.).
Springer.
</p>
<p>Efron, B. (1986). How biased is the apparent error rate of a prediction rule?
<em>Journal of the American Statistical Association</em>, <em>81</em>(394),
461-70.
</p>


<h3>See Also</h3>

<p><code>set_monitor</code>, <code>set_predict</code>,
<code>set_strata</code>,
<code>resample</code>, <code>SelectedInput</code>,
<code>SelectedModel</code>, <code>TunedInput</code>,
<code>TunedModel</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Bootstrapping with 100 samples
BootControl(samples = 100)

## Optimism-corrected bootstrapping with 100 samples
BootOptimismControl(samples = 100)

## Cross-validation with 5 repeats of 10 folds
CVControl(folds = 10, repeats = 5)

## Optimism-corrected cross-validation with 5 repeats of 10 folds
CVOptimismControl(folds = 10, repeats = 5)

## Out-of-bootstrap validation with 100 samples
OOBControl(samples = 100)

## Split sample validation with 2/3 training and 1/3 testing
SplitControl(prop = 2/3)

## Training set evaluation
TrainControl()

</code></pre>


</div>