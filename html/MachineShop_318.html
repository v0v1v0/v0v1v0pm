<div class="container">

<table style="width: 100%;"><tr>
<td>XGBModel</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Extreme Gradient Boosting Models</h2>

<h3>Description</h3>

<p>Fits models with an efficient implementation of the gradient boosting
framework from Chen &amp; Guestrin.
</p>


<h3>Usage</h3>

<pre><code class="language-R">XGBModel(
  nrounds = 100,
  ...,
  objective = character(),
  aft_loss_distribution = "normal",
  aft_loss_distribution_scale = 1,
  base_score = 0.5,
  verbose = 0,
  print_every_n = 1
)

XGBDARTModel(
  eta = 0.3,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  max_delta_step = .(0.7 * is(y, "PoissonVariate")),
  subsample = 1,
  colsample_bytree = 1,
  colsample_bylevel = 1,
  colsample_bynode = 1,
  alpha = 0,
  lambda = 1,
  tree_method = "auto",
  sketch_eps = 0.03,
  scale_pos_weight = 1,
  refresh_leaf = 1,
  process_type = "default",
  grow_policy = "depthwise",
  max_leaves = 0,
  max_bin = 256,
  num_parallel_tree = 1,
  sample_type = "uniform",
  normalize_type = "tree",
  rate_drop = 0,
  one_drop = 0,
  skip_drop = 0,
  ...
)

XGBLinearModel(
  alpha = 0,
  lambda = 0,
  updater = "shotgun",
  feature_selector = "cyclic",
  top_k = 0,
  ...
)

XGBTreeModel(
  eta = 0.3,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  max_delta_step = .(0.7 * is(y, "PoissonVariate")),
  subsample = 1,
  colsample_bytree = 1,
  colsample_bylevel = 1,
  colsample_bynode = 1,
  alpha = 0,
  lambda = 1,
  tree_method = "auto",
  sketch_eps = 0.03,
  scale_pos_weight = 1,
  refresh_leaf = 1,
  process_type = "default",
  grow_policy = "depthwise",
  max_leaves = 0,
  max_bin = 256,
  num_parallel_tree = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>nrounds</code></td>
<td>
<p>number of boosting iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>model parameters as described below and in the XGBoost
<a href="https://xgboost.readthedocs.io/en/latest/parameter.html">documentation</a>
and arguments passed to <code>XGBModel</code> from the other constructors.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>objective</code></td>
<td>
<p>optional character string defining the learning task and
objective.  Set automatically if not specified according to the following
values available for supported response variable types.
</p>

<dl>
<dt>
<code>factor</code>:</dt>
<dd>
<p><code>"multi:softprob"</code>, <code>"binary:logistic"</code>
(2 levels only)</p>
</dd>
<dt>
<code>numeric</code>:</dt>
<dd>
<p><code>"reg:squarederror"</code>, <code>"reg:logistic"</code>,
<code>"reg:gamma"</code>, <code>"reg:tweedie"</code>, <code>"rank:pairwise"</code>,
<code>"rank:ndcg"</code>, <code>"rank:map"</code></p>
</dd>
<dt>
<code>PoissonVariate</code>:</dt>
<dd>
<p><code>"count:poisson"</code></p>
</dd>
<dt>
<code>Surv</code>:</dt>
<dd>
<p><code>"survival:aft"</code>, <code>"survival:cox"</code></p>
</dd>
</dl>
<p>The first values listed are the defaults for the corresponding response
types.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aft_loss_distribution</code></td>
<td>
<p>character string specifying a distribution for
the accelerated failure time objective (<code>"survival:aft"</code>) as
<code>"extreme"</code>, <code>"logistic"</code>, or <code>"normal"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>aft_loss_distribution_scale</code></td>
<td>
<p>numeric scaling parameter for the
accelerated failure time distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>base_score</code></td>
<td>
<p>initial prediction score of all observations, global bias.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>numeric value controlling the amount of output printed
during model fitting, such that 0 = none, 1 = performance information, and
2 = additional information.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>print_every_n</code></td>
<td>
<p>numeric value designating the fitting iterations at
at which to print output when <code>verbose &gt; 0</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eta</code></td>
<td>
<p>shrinkage of variable weights at each iteration to prevent
overfitting.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>gamma</code></td>
<td>
<p>minimum loss reduction required to split a tree node.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_depth</code></td>
<td>
<p>maximum tree depth.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_child_weight</code></td>
<td>
<p>minimum sum of observation weights required of nodes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_delta_step, tree_method, sketch_eps, scale_pos_weight, updater, refresh_leaf, process_type, grow_policy, max_leaves, max_bin, num_parallel_tree</code></td>
<td>
<p>other tree booster parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subsample</code></td>
<td>
<p>subsample ratio of the training observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>colsample_bytree, colsample_bylevel, colsample_bynode</code></td>
<td>
<p>subsample ratio of
variables for each tree, level, or split.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha, lambda</code></td>
<td>
<p>L1 and L2 regularization terms for variable weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_type, normalize_type</code></td>
<td>
<p>type of sampling and normalization
algorithms.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rate_drop</code></td>
<td>
<p>rate at which to drop trees during the dropout procedure.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>one_drop</code></td>
<td>
<p>integer indicating whether to drop at least one tree during
the dropout procedure.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>skip_drop</code></td>
<td>
<p>probability of skipping the dropout procedure during a
boosting iteration.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>feature_selector, top_k</code></td>
<td>
<p>character string specifying the feature
selection and ordering method, and number of top variables to select in the
<code>"greedy"</code> and <code>"thrifty"</code> feature selectors.</p>
</td>
</tr>
</table>
<h3>Details</h3>


<dl>
<dt>Response types:</dt>
<dd>
<p><code>factor</code>, <code>numeric</code>,
<code>PoissonVariate</code>, <code>Surv</code></p>
</dd>
<dt>Automatic tuning of grid parameters:</dt>
<dd>

<ul>
<li>
<p> XGBModel: <code>NULL</code>
</p>
</li>
<li>
<p> XGBDARTModel: <code>nrounds</code>, <code>eta</code>*, <code>gamma</code>*,
<code>max_depth</code>, <code>min_child_weight</code>*, <code>subsample</code>*,
<code>colsample_bytree</code>*, <code>rate_drop</code>*, <code>skip_drop</code>*
</p>
</li>
<li>
<p> XGBLinearModel: <code>nrounds</code>, <code>alpha</code>, <code>lambda</code>
</p>
</li>
<li>
<p> XGBTreeModel: <code>nrounds</code>, <code>eta</code>*, <code>gamma</code>*,
<code>max_depth</code>, <code>min_child_weight</code>*, <code>subsample</code>*,
<code>colsample_bytree</code>*
</p>
</li>
</ul>
</dd>
</dl>
<p>* excluded from grids by default
</p>
<p>The booster-specific constructor functions <code>XGBDARTModel</code>,
<code>XGBLinearModel</code>, and <code>XGBTreeModel</code> are special cases of
<code>XGBModel</code> which automatically set the XGBoost <code>booster</code>
<a href="https://xgboost.readthedocs.io/en/latest/parameter.html">parameter</a>.
These are called directly in typical usage unless <code>XGBModel</code> is needed
to specify a more general model.
</p>
<p>Default argument values and further model details can be found in the source
See Also link below.
</p>
<p>In calls to <code>varimp</code> for <code>XGBTreeModel</code>, argument
<code>type</code> may be specified as <code>"Gain"</code> (default) for the fractional
contribution of each predictor to the total gain of its splits, as
<code>"Cover"</code> for the number of observations related to each predictor, or
as <code>"Frequency"</code> for the percentage of times each predictor is used in
the trees.  Variable importance is automatically scaled to range from 0 to
100.  To obtain unscaled importance values, set <code>scale = FALSE</code>.  See
example below.
</p>


<h3>Value</h3>

<p><code>MLModel</code> class object.
</p>


<h3>See Also</h3>

<p><code>xgboost</code>, <code>fit</code>,
<code>resample</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## Requires prior installation of suggested package xgboost to run

model_fit &lt;- fit(Species ~ ., data = iris, model = XGBTreeModel)
varimp(model_fit, method = "model", type = "Frequency", scale = FALSE)


</code></pre>


</div>