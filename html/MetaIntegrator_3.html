<div class="container">

<table style="width: 100%;"><tr>
<td>calculateROC</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate ROC Curve Statistics</h2>

<h3>Description</h3>

<p>Calculates receiver operating characteristic curve data, including AUC (using trapezoidal method). Takes only a vector of labels and a vector of predictions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">calculateROC(labels, predictions, AUConly = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>labels</code></td>
<td>
<p>Vector of labels; must have exactly two unique values (ie, cases and controls).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictions</code></td>
<td>
<p>Vector of predictions (for instance, test scores) to be evaluated for ability to separate the two classes. Must be exactly the same length as labels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>AUConly</code></td>
<td>
<p>Return all ROC values, or just the AUC.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The code borrows its core ROC calculations from the ROCR package. AUC is calculated by the trapezoidal method. AUC standard errors are calculated according to Hanley's method.
</p>


<h3>Value</h3>

<p>Assuming AUConly=F, returns a list of values:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>roc</code></td>
<td>
<p>dataframe consisting of two columns, FPR and TPR, meant for plotting</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auc</code></td>
<td>
<p>area under the curve</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>auc.CI</code></td>
<td>
<p>95% confidence interval for AUC</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Timothy E. Sweeney
</p>


<h3>References</h3>

<p>The code borrows its core ROC calculations from the ROCR package.
</p>


<h3>See Also</h3>

<p><code>calculateScore</code>, <code>rocPlot</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># expect an AUC near 0.5 with random test
labels &lt;- c(rep(0, 500), rep(1, 500))
scores &lt;- runif(1000)
calculateROC(labels, scores)
#With the real data, AUC should be around 0.85606
scoreResults &lt;- calculateScore(tinyMetaObject$filterResults[[1]], tinyMetaObject$originalData[[1]]) 
rocRes &lt;- calculateROC(predictions=scoreResults, labels=tinyMetaObject$originalData[[1]]$class)
print(rocRes$auc[[1]])
</code></pre>


</div>