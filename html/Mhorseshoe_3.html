<div class="container">

<table style="width: 100%;"><tr>
<td>exact_horseshoe</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Run exact MCMC algorithm for horseshoe prior</h2>

<h3>Description</h3>

<p>The exact MCMC algorithm for the horseshoe prior introduced in section 2.1
of Johndrow et al. (2020).
</p>


<h3>Usage</h3>

<pre><code class="language-R">exact_horseshoe(
  y,
  X,
  burn = 1000,
  iter = 5000,
  a = 1/5,
  b = 10,
  s = 0.8,
  tau = 1,
  sigma2 = 1,
  w = 1,
  alpha = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Response vector, <code class="reqn">y \in \mathbb{R}^{N}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Design matrix, <code class="reqn">X \in \mathbb{R}^{N \times p}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burn</code></td>
<td>
<p>Number of burn-in samples. The default is 1000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Number of samples to be drawn from the posterior. The default is
5000.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>a</code></td>
<td>
<p>A tuning parameter of the rejection sampler, where the default
value is <code class="reqn">a = 1/5</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b</code></td>
<td>
<p>A tuning parameter of the rejection sampler, where the default
value is <code class="reqn">b = 10</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p><code class="reqn">s^{2}</code> is the variance of tau's MH proposal distribution.
0.8 is a good default. If set to 0, the algorithm proceeds by fixing the
global shrinkage parameter <code class="reqn">\tau</code> to the initial setting value.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tau</code></td>
<td>
<p>Initial value of the global shrinkage parameter <code class="reqn">\tau</code> when
starting the algorithm. The default is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma2</code></td>
<td>
<p>Initial value of error variance <code class="reqn">\sigma^{2}</code>. The default
is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w</code></td>
<td>
<p>A hyperparameter of gamma prior for <code class="reqn">\sigma^{2}</code>. The default
is 1.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p><code class="reqn">100(1-\alpha)\%</code> credible interval setting argument.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The exact MCMC algorithm introduced in Section 2.1 of Johndrow et al. (2020)
is implemented in this function. This algorithm uses a blocked-Gibbs
sampler for <code class="reqn">(\tau, \beta, \sigma^2)</code>, where the global shrinkage
parameter <code class="reqn">\tau</code> is updated by an Metropolis-Hastings algorithm. The
local shrinkage parameter <code class="reqn">\lambda_{j},\ j = 1,2,...,p</code> is updated by
the rejection sampler.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>BetaHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\beta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LeftCI</code></td>
<td>
<p>Lower bound of <code class="reqn">100(1-\alpha)\%</code> credible interval for
<code class="reqn">\beta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RightCI</code></td>
<td>
<p>Upper bound of <code class="reqn">100(1-\alpha)\%</code> credible interval for
<code class="reqn">\beta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Sigma2Hat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\sigma^{2}</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TauHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\tau</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LambdaHat</code></td>
<td>
<p>Posterior mean of <code class="reqn">\lambda_{j},\ j=1,2,...p.</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>BetaSamples</code></td>
<td>
<p>Samples from the posterior of <code class="reqn">\beta</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>LambdaSamples</code></td>
<td>
<p>Lambda samples through rejection sampling.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TauSamples</code></td>
<td>
<p>Tau samples through MH algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Sigma2Samples</code></td>
<td>
<p>Samples from the posterior of the parameter
<code class="reqn">sigma^{2}</code>.</p>
</td>
</tr>
</table>
<h3>References</h3>

<p>Johndrow, J., Orenstein, P., &amp; Bhattacharya, A. (2020). Scalable
Approximate MCMC Algorithms for the Horseshoe Prior. In Journal of Machine
Learning Research, 21, 1-61.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Making simulation data.
set.seed(123)
N &lt;- 50
p &lt;- 100
true_beta &lt;- c(rep(1, 10), rep(0, 90))

X &lt;- matrix(1, nrow = N, ncol = p) # Design matrix X.
for (i in 1:p) {
  X[, i] &lt;- stats::rnorm(N, mean = 0, sd = 1)
}

y &lt;- vector(mode = "numeric", length = N) # Response variable y.
e &lt;- rnorm(N, mean = 0, sd = 2) # error term e.
for (i in 1:10) {
  y &lt;- y + true_beta[i] * X[, i]
}
y &lt;- y + e

# Run exact_horseshoe
result &lt;- exact_horseshoe(y, X, burn = 0, iter = 100)

# posterior mean
betahat &lt;- result$BetaHat

# Lower bound of the 95% credible interval
leftCI &lt;- result$LeftCI

# Upper bound of the 95% credible interval
RightCI &lt;- result$RightCI

</code></pre>


</div>