<div class="container">

<table style="width: 100%;"><tr>
<td>matrixmixture</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fit a matrix variate mixture model</h2>

<h3>Description</h3>

<p>Clustering by fitting a mixture model using EM with <code>K</code> groups
and unconstrained covariance matrices for a matrix variate normal or
matrix variate t distribution (with specified degrees of freedom <code>nu</code>).
</p>


<h3>Usage</h3>

<pre><code class="language-R">matrixmixture(
  x,
  init = NULL,
  prior = NULL,
  K = length(prior),
  iter = 1000,
  model = "normal",
  method = NULL,
  row.mean = FALSE,
  col.mean = FALSE,
  tolerance = 0.1,
  nu = NULL,
  ...,
  verbose = 0,
  miniter = 5,
  convergence = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>data, <code class="reqn">p \times q \times n</code> array</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>init</code></td>
<td>
<p>a list containing an array of <code>K</code> of
<code class="reqn">p \times q</code> means labeled <code>centers</code>,
and optionally <code class="reqn">p \times p</code> and <code class="reqn">q \times q</code>
positive definite variance matrices labeled <code>U</code> and <code>V</code>.
By default, those are presumed to be identity if not provided.
If <code>init</code> is missing, it will be provided using the <code>prior</code>
or <code>K</code> by <code>init_matrixmix</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prior</code></td>
<td>
<p>prior for the <code>K</code> classes, a vector that adds to unity</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>number of classes - provide either this or the prior. If this is
provided, the prior will be of uniform distribution among the classes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>whether to use the <code>normal</code> or <code>t</code> distribution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>what method to use to fit the distribution.
Currently no options.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>row.mean</code></td>
<td>
<p>By default, <code>FALSE</code>. If <code>TRUE</code>, will fit a
common mean within each row. If both this and <code>col.mean</code> are
<code>TRUE</code>, there will be a common mean for the entire matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>col.mean</code></td>
<td>
<p>By default, <code>FALSE</code>. If <code>TRUE</code>, will fit a
common mean within each row. If both this and <code>row.mean</code> are
<code>TRUE</code>, there will be a common mean for the entire matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tolerance</code></td>
<td>
<p>convergence criterion, using Aitken acceleration of the
log-likelihood by default.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nu</code></td>
<td>
<p>degrees of freedom parameter. Can be a vector of length <code>K</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>pass additional arguments to <code>MLmatrixnorm</code> or
<code>MLmatrixt</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>whether to print diagnostic output, by default <code>0</code>.
Higher  numbers output more results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>miniter</code></td>
<td>
<p>minimum number of iterations</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>convergence</code></td>
<td>
<p>By default, <code>TRUE</code>, using Aitken acceleration
to determine convergence. If false, it instead checks if the change in
log-likelihood is less than <code>tolerance</code>. Aitken acceleration may
prematurely end in the first few steps, so you may wish to set
<code>miniter</code> or select <code>FALSE</code> if this is an issue.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list of class <code>MixMatrixModel</code> containing the following
components:
</p>

<dl>
<dt><code>prior</code></dt>
<dd>
<p>the prior probabilities used.</p>
</dd>
<dt><code>init</code></dt>
<dd>
<p>the initialization used.</p>
</dd>
<dt><code>K</code></dt>
<dd>
<p>the number of groups</p>
</dd>
<dt><code>N</code></dt>
<dd>
<p>the number of observations</p>
</dd>
<dt><code>centers</code></dt>
<dd>
<p>the group means.</p>
</dd>
<dt><code>U</code></dt>
<dd>
<p>the between-row covariance matrices</p>
</dd>
<dt><code>V</code></dt>
<dd>
<p>the between-column covariance matrix</p>
</dd>
<dt><code>posterior</code></dt>
<dd>
<p>the posterior probabilities for each
observation</p>
</dd>
<dt><code>pi</code></dt>
<dd>
<p> the final proportions</p>
</dd>
<dt><code>nu</code></dt>
<dd>
<p>The degrees of freedom parameter if the t distribution
was used.</p>
</dd>
<dt><code>convergence </code></dt>
<dd>
<p>whether the model converged</p>
</dd>
<dt><code>logLik</code></dt>
<dd>
<p>a vector of the log-likelihoods
of each iteration ending in
the final log-likelihood of the model</p>
</dd>
<dt><code>model</code></dt>
<dd>
<p>the model used</p>
</dd>
<dt><code>method</code></dt>
<dd>
<p>the method used</p>
</dd>
<dt><code>call</code></dt>
<dd>
<p>The (matched) function call.</p>
</dd>
</dl>
<h3>References</h3>

<div class="sourceCode"><pre>Andrews, Jeffrey L., Paul D. McNicholas, and Sanjeena Subedi. 2011.
  "Model-Based Classification via Mixtures of Multivariate
  T-Distributions." Computational Statistics &amp; Data Analysis 55 (1):
  520–29. \doi{10.1016/j.csda.2010.05.019}.

Fraley, Chris, and Adrian E Raftery. 2002. "Model-Based Clustering,
   Discriminant Analysis, and Density Estimation." Journal of the
   American Statistical Association 97 (458). Taylor &amp; Francis: 611–31.
   \doi{10.1198/016214502760047131}.

McLachlan, Geoffrey J, Sharon X Lee, and Suren I Rathnayake. 2019.
      "Finite Mixture Models." Annual Review of Statistics and Its
      Application 6. Annual Reviews: 355–78.
      \doi{10.1146/annurev-statistics-031017-100325}.

Viroli, Cinzia. 2011. "Finite Mixtures of Matrix Normal Distributions
      for Classifying Three-Way Data." Statistics and Computing 21 (4):
      511–22. \doi{10.1007/s11222-010-9188-x}.
</pre></div>


<h3>See Also</h3>

<p><code>init_matrixmixture()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(20180221)
A &lt;- rmatrixt(20,mean=matrix(0,nrow=3,ncol=4), df = 5)
# 3x4 matrices with mean 0
B &lt;- rmatrixt(20,mean=matrix(1,nrow=3,ncol=4), df = 5)
# 3x4 matrices with mean 1
C &lt;- array(c(A,B), dim=c(3,4,40)) # combine into one array
prior &lt;- c(.5,.5) # equal probability prior
# create an intialization object, starts at the true parameters
init = list(centers = array(c(rep(0,12),rep(1,12)), dim = c(3,4,2)),
              U = array(c(diag(3), diag(3)), dim = c(3,3,2))*20,
              V = array(c(diag(4), diag(4)), dim = c(4,4,2))
 )
# fit model
 res&lt;-matrixmixture(C, init = init, prior = prior, nu = 5,
                    model = "t", tolerance = 1e-3, convergence = FALSE)
print(res$centers) # the final centers
print(res$pi) # the final mixing proportion
plot(res) # the log likelihood by iteration
logLik(res) # log likelihood of final result
BIC(res) # BIC of final result
predict(res, newdata = C[,,c(1,21)]) # predicted class membership
</code></pre>


</div>