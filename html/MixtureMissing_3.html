<div class="container">

<table style="width: 100%;"><tr>
<td>evaluation_metrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Binary Classification Evaluation</h2>

<h3>Description</h3>

<p>Evaluate the performance of a classification model by comparing its predicted
labels to the true labels. Various metrics are returned to give an insight on
how well the model classifies the observations. This function is added to aid
outlier detection evaluation of MCNM and MtM in case that true outliers are
known in advance.
</p>


<h3>Usage</h3>

<pre><code class="language-R">evaluation_metrics(true_labels, pred_labels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true_labels</code></td>
<td>
<p>An 0-1 or logical vector denoting the true labels. The
meaning of 0 and 1 (or TRUE and FALSE) is up to the user.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred_labels</code></td>
<td>
<p>An 0-1 or logical vector denoting the true labels. The
meaning of 0 and 1 (or TRUE and FALSE) is up to the user.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with the following slots:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>matr</code></td>
<td>
<p>The confusion matrix built upon true labels and predicted labels.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TN</code></td>
<td>
<p>True negative.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FP</code></td>
<td>
<p>False positive (type I error).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FN</code></td>
<td>
<p>False negative (type II error).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TP</code></td>
<td>
<p>True positive.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TPR</code></td>
<td>
<p>True positive rate (sensitivy).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FPR</code></td>
<td>
<p>False positive rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>TNR</code></td>
<td>
<p>True negative rate (specificity).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FNR</code></td>
<td>
<p>False negative rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>precision</code></td>
<td>
<p>Precision or positive predictive value (PPV).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>accuracy</code></td>
<td>
<p>Accuracy.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>error_rate</code></td>
<td>
<p>Error rate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>FDR</code></td>
<td>
<p>False discovery rate.</p>
</td>
</tr>
</table>
<h3>Examples</h3>

<pre><code class="language-R">
#++++ Inputs are 0-1 vectors ++++#

evaluation_metrics(
  true_labels = c(1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1),
  pred_labels = c(1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1)
)

#++++ Inputs are logical vectors ++++#

evaluation_metrics(
  true_labels = c(TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, FALSE),
  pred_labels = c(FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE)
)

</code></pre>


</div>