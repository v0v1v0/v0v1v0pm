<div class="container">

<table style="width: 100%;"><tr>
<td>MoE_entropy</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Entropy of a fitted MoEClust model</h2>

<h3>Description</h3>

<p>Calculates the normalised entropy of a fitted MoEClust model.
</p>


<h3>Usage</h3>

<pre><code class="language-R">MoE_entropy(x,
            group = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>An object of class <code>"MoEClust"</code> generated by <code>MoE_clust</code>, or an object of class <code>"MoECompare"</code> generated by <code>MoE_compare</code>. Models with gating and/or expert covariates and/or a noise component are facilitated here too.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>group</code></td>
<td>
<p>A logical (defaults to <code>FALSE</code>) indicating whether component-specific average entropies should be returned instead.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When <code>group</code> is <code>FALSE</code>, this function calculates the normalised entropy via </p>
<p style="text-align: center;"><code class="reqn">H=-\frac{1}{n\log(G)}\sum_{i=1}^n\sum_{g=1}^G\hat{z}_{ig}\log(\hat{z}_{ig})</code>
</p>
<p>,
where <code class="reqn">n</code> and <code class="reqn">G</code> are the sample size and number of components, respectively, and <code class="reqn">\hat{z}_{ig}</code> is the estimated posterior probability at convergence that observation <code class="reqn">i</code> belongs to component <code class="reqn">g</code>. Note that <code>G=x$G</code> for models without a noise component and <code>G=x$G + 1</code> for models with a noise component. 
</p>
<p>When <code>group</code> is <code>TRUE</code>, </p>
<p style="text-align: center;"><code class="reqn">H_i=-\frac{1}{\log(G)}\sum_{g=1}^G\hat{z}_{ig}\log(\hat{z}_{ig})</code>
</p>
<p> is computed for each observation and averaged according to the MAP classification.
</p>


<h3>Value</h3>

<p>When <code>group</code> is <code>FALSE</code>, a single number, given by <code class="reqn">1-H</code>, in the range [0,1], such that <em>larger</em> values indicate clearer separation of the clusters. Otherwise, a vector of length <code>G</code> containing the per-component averages of the observation-specific entries is returned.
</p>


<h3>Note</h3>

<p>This function will always return a normalised entropy of <code>1</code> for models fitted using the <code>"CEM"</code> algorithm (see <code>MoE_control</code>), or models with only one component.
</p>


<h3>Author(s)</h3>

<p>Keefe Murphy - &lt;<a href="mailto:keefe.murphy@mu.ie">keefe.murphy@mu.ie</a>&gt;
</p>


<h3>References</h3>

<p>Murphy, K. and Murphy, T. B. (2020). Gaussian parsimonious clustering models with covariates and a noise component. <em>Advances in Data Analysis and Classification</em>, 14(2): 293-325. &lt;<a href="https://doi.org/10.1007/s11634-019-00373-8">doi:10.1007/s11634-019-00373-8</a>&gt;.
</p>


<h3>See Also</h3>

<p><code>MoE_clust</code>, <code>MoE_control</code>, <code>MoE_AvePP</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(ais)
res &lt;- MoE_clust(ais[,3:7], G=3, gating= ~ BMI + sex, 
                 modelNames="EEE", network.data=ais)

# Calculate the normalised entropy
MoE_entropy(res)

# Calculate the normalised entropy per cluster
MoE_entropy(res, group=TRUE)
</code></pre>


</div>