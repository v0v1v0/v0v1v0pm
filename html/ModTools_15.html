<div class="container">

<table style="width: 100%;"><tr>
<td>LogitBoost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>LogitBoost Classification Algorithm</h2>

<h3>Description</h3>

<p>Train logitboost classification algorithm using decision
stumps (one node decision trees) as weak learners.  </p>


<h3>Usage</h3>

<pre><code class="language-R">LogitBoost(x, ...)

## S3 method for class 'formula'
LogitBoost(formula, data, ..., subset, na.action)

## Default S3 method:
LogitBoost(x, y, nIter=ncol(x), ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p>a formula expression as for regression models, of the form <code>response ~ predictors</code>. The response should be a factor or a matrix with K columns, which will be interpreted as counts for each of K classes. See the documentation of <code>formula()</code> for other details.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p>an optional data frame in which to interpret the variables occurring in formula.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>additional arguments for nnet
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>
<p>expression saying which subset of the rows of the data should be used in the fit. All observations are included by default.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>a function to filter missing data.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A matrix or data frame with training data. Rows contain samples
and columns contain features</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>Class labels for the training data samples.
A response vector with one label for each row/component of <code>xlearn</code>.
Can be either a factor, string or a numeric vector.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nIter</code></td>
<td>
<p>An integer, describing the number of iterations for
which boosting should be run, or number of decision stumps that will be
used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function was adapted from logitboost.R function written by Marcel
Dettling. See references and "See Also" section. The code was modified in
order to make it much faster for very large data sets. The speed-up was
achieved by implementing a internal version of decision stump classifier
instead of using calls to <code>rpart</code>. That way, some of the most time
consuming operations were precomputed once, instead of performing them at
each iteration. Another difference is that training and testing phases of the
classification process were split into separate functions.
</p>


<h3>Value</h3>

<p>An object of class "LogitBoost" including components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>Stump</code></td>
<td>
<p>List of decision stumps (one node decision trees) used:
</p>

<ul>
<li>
<p> column 1: feature numbers or each stump, or which column each stump
operates on
</p>
</li>
<li>
<p> column 2: threshold to be used for that column
</p>
</li>
<li>
<p> column 3: bigger/smaller info: 1 means that if values in the column
are above threshold than corresponding samples will be labeled as
<code>lablist[1]</code>. Value "-1" means the opposite.
</p>
</li>
</ul>
<p>If there are more than two classes, than several "Stumps" will be
<code>cbind</code>'ed
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lablist</code></td>
<td>
<p>names of each class</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jarek Tuszynski (SAIC) <a href="mailto:jaroslaw.w.tuszynski@saic.com">jaroslaw.w.tuszynski@saic.com</a></p>


<h3>References</h3>

<p>Dettling and Buhlmann (2002), <em>Boosting for Tumor Classification of Gene
Expression Data</em>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># basic interface
r.lb &lt;- LogitBoost(Species ~ ., data=iris, nIter=20)
pred &lt;- predict(r.lb)
prob &lt;- predict(r.lb, type="prob")
d.res &lt;- data.frame(pred, prob)
d.res[1:10, ]

# accuracy increases with nIter (at least for train set)
table(predict(r.lb, iris, type="class", nIter= 2), iris$Species)
table(predict(r.lb, iris, type="class", nIter=10), iris$Species)
table(predict(r.lb, iris, type="class"),           iris$Species)

# example of spliting the data into train and test set
d.set &lt;- SplitTrainTest(iris)
r.lb &lt;- LogitBoost(Species ~ ., data=d.set$train, nIter=10)
table(predict(r.lb, d.set$test, type="class", nIter=2), d.set$test$Species)
table(predict(r.lb, d.set$test, type="class"),          d.set$test$Species)
</code></pre>


</div>