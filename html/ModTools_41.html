<div class="container">

<table style="width: 100%;"><tr>
<td>Tune</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tune Classificators
</h2>

<h3>Description</h3>

<p>Some classifiers benefit more from adjusted parameters to a particular dataset than others. However, it is often not clear from the beginning how the parameters have to be determined.  What often only remains is a grid search when several parameters have to be found in combination. The present function uses a grid search approch for the decisive arguments (typically for a neural network, a random forest or a classification tree). However it's not restricted to these models, any model fulfilling weak interface standards could be provided.
</p>


<h3>Usage</h3>

<pre><code class="language-R">Tune(x, ..., testset = NULL, keepmod = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>the model to be tuned, best (but not necessarily) trained with <code>FitMod</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>a list of parameters, containing the values to be used for a grid search.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testset</code></td>
<td>
<p>a testset containing all variables required in the model to be used for calculating independently the accuracy (normally a subset of the original dataset).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keepmod</code></td>
<td>
<p>logical, defining if all fitted models should be returned in the result set. Default is <code>TRUE</code>. (Keep an eye on your RAM!)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The function creates a n-dimensional grid according to the given parameters and calculates the model with the combinations of all the parameters. The accuracy for the models are calculated insample and on a test set, if one has been provided.
</p>
<p>It makes sense to avoid overfitting to provide a test set to also be evaluated.
A matrix with all combination of the values for the given parameters, sorted by accuracy, either by the accuracy achieved in the test set or the insample accuracy is returned.
</p>


<h3>Value</h3>

<p>a matrix with all supplied parameters and a column <code>"acc"</code> and <code>"test_acc"</code> (if a test set has been provided) </p>


<h3>Author(s)</h3>

<p>Andri Signorell &lt;andri@signorell.net&gt;</p>


<h3>Examples</h3>

<pre><code class="language-R">d.pim &lt;- SplitTrainTest(d.pima, p = 0.2)
mdiab &lt;- formula(diabetes ~ pregnant + glucose + pressure + triceps
                 + insulin + mass + pedigree + age)

# tune a neural network for size and decay
r.nn &lt;- FitMod(mdiab, data=d.pim$train, fitfn="nnet")
(tu &lt;- Tune(r.nn, size=12:17, decay = 10^(-4:-1), testset=d.pim$test))

# tune a random forest
r.rf &lt;- FitMod(mdiab, data=d.pim$train, fitfn="randomForest")
(tu &lt;- Tune(r.rf, mtry=seq(2, 20, 2), testset=d.pim$test))

# tune a SVM model
r.svm &lt;- FitMod(mdiab, data=d.pim$train, fitfn="svm")

tu &lt;- Tune(r.svm,
           kernel=c("radial", "sigmoid"),
           cost=c(0.1,1,10,100,1000),
           gamma=c(0.5,1,2,3,4), testset=d.pim$test)

# let's get some more quality measures
tu$modpar$Sens &lt;- sapply(tu$mods, Sens)     # Sensitivity
tu$modpar$Spec &lt;- sapply(tu$mods, Spec)     # Specificity
Sort(tu$modpar, ord="test_acc", decreasing=TRUE)

</code></pre>


</div>