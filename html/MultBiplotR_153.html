<div class="container">

<table style="width: 100%;"><tr>
<td>RidgeBinaryLogistic</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Ridge Binary Logistic Regression for Binary data
</h2>

<h3>Description</h3>

<p>This function performs a logistic regression between a dependent binary variable <code>y</code> and some independent
variables <code>x</code>, solving the separation problem in this type of regression using ridge 
penalization.
</p>


<h3>Usage</h3>

<pre><code class="language-R">RidgeBinaryLogistic(y, X = NULL, data = NULL, freq = NULL, 
tolerance = 1e-05, maxiter = 100, penalization = 0.2, 
cte = FALSE, ref = "first", bootstrap = FALSE, nmB = 100, 
RidgePlot = FALSE, MinLambda = 0, MaxLambda = 2, StepLambda = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A binary dependent variable or a formula
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>A set of independent variables when y is not a formula.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>data frame for the formula
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>freq</code></td>
<td>

<p>frequencies for each observation (usually 1)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tolerance</code></td>
<td>

<p>Tolerance for convergence
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxiter</code></td>
<td>

<p>Maximum number of iterations
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalization</code></td>
<td>

<p>Ridige penalization: a non negative constant. Penalization used in the diagonal matrix to avoid singularities.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cte</code></td>
<td>

<p>Should the model have a constant? 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ref</code></td>
<td>

<p>Category of reference
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bootstrap</code></td>
<td>

<p>Should bootstrap confidence intervals be calculated?
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nmB</code></td>
<td>

<p>Number of bootstrap samples.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>RidgePlot</code></td>
<td>

<p>Should the ridge plot be plotted?
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MinLambda</code></td>
<td>

<p>Minimum value of lambda for the rigge plot
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MaxLambda</code></td>
<td>

<p>Maximum value of lambda for the rigge plot
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>StepLambda</code></td>
<td>

<p>Step for increasing the values of lambda
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Logistic Regression is a widely used technique in applied work when a binary, nominal or ordinal response variable is available, due to the fact that classical regression methods are not applicable to this kind of variables. The method is available in most of the statistical packages, commercial or free. Maximum Likelihood together with a numerical method as Newton-Raphson, is used to estimate the parameters of the model.
In logistic regression, when in the space generated by the independent variables there are hyperplanes that separate among the individuals belonging to the different groups defined by the response, maximum likelihood does not converge and the estimations tend to the infinity. That is known in the literature as the separation problem in logistic regression. Even when the separation is not complete, the numerical solution of the maximum likelihood has stability problems.
From a practical point of view, that means the estimated model is not accurate precisely when there should be a perfect, or almost perfect, fit to the data.
</p>
<p>The problem of the existence of the estimators in logistic regression can be seen in Albert (1984), a solution for the binary case, based on the Firth method, Firth (1993) is proposed by Heinze(2002).
The extension to nominal logistic model was made by Bull (2002). All the procedures were initially
developed to remove the bias but work well to avoid the problem of separation. Here we have chosen
a simpler solution based on ridge estimators for logistic regression Cessie(1992).
</p>
<p>Rather than maximizing <code class="reqn">{L_j}(\left. {\bf{G}} \right|{{\bf{b}}_{j0}},{{\bf{B}}_j})</code> we maximize
</p>
<p style="text-align: center;"><code class="reqn">{{L_j}(\left. {\bf{G}} \right|{{\bf{b}}_{j0}},{{\bf{B}}_j})} - \lambda \left( {\left\| {{{\bf{b}}_{j0}}} \right\| + \left\| {{{\bf{B}}_j}} \right\|} \right)</code>
</p>

<p>Changing the values of <code class="reqn">\lambda</code> we obtain slightly different solutions not affected by the separation problem.
</p>


<h3>Value</h3>

<p>An object of class <code>RidgeBinaryLogistic</code> with the following components

</p>
<table>
<tr style="vertical-align: top;">
<td><code>beta </code></td>
<td>
<p>Estimates of the coefficients</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fitted </code></td>
<td>
<p>Fitted probabilities</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>residuals </code></td>
<td>
<p>Residuals of the model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Prediction </code></td>
<td>
<p>Predictions of presences and absences</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Covariances </code></td>
<td>
<p>Covariances among the estimates</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Deviance </code></td>
<td>
<p>Deviance of the current model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>NullDeviance </code></td>
<td>
<p>Deviance of the null model</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Dif </code></td>
<td>
<p>Difference between the deviances of the cirrent and null models</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df </code></td>
<td>
<p>Degrees of freedom of the difference</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p </code></td>
<td>
<p>p-value</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>CoxSnell </code></td>
<td>
<p>Cox-Snell pseudo R-squared</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Nagelkerke </code></td>
<td>
<p>Nagelkerke pseudo R-squared</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>MacFaden </code></td>
<td>
<p>MacFaden pseudo R-squared</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R2 </code></td>
<td>
<p>Pseudo R-squared using the residuals</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Classification </code></td>
<td>
<p>Classification table</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>PercentCorrect </code></td>
<td>
<p>Percentage of correct classification</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jose Luis Vicente Villardon
</p>


<h3>References</h3>

<p>Agresti, A. (1990) An Introduction to Categorical Data Analysis. John Wiley and Sons, Inc.
</p>
<p>Albert, A. and Anderson, J. A.  (1984) On the existence of maximum likelihood estimates in logistic regression models. Biometrika, 71(1): 1-10.  
</p>
<p>Anderson, J. A. (1972), Separate sample logistic discrimination. Biometrika, 59(1): 19-35.
</p>
<p>Anderson, J. A. &amp; Philips P. R. (1981)  Regression, discrimination and measurement models for ordered categorical variables. Appl. Statist, 30: 22-31.
</p>
<p>Bull, S. B., Mk, C. &amp; Greenwood, C. M. (2002) A modified score function for multinomial logistic regression. Computational Statistics and data Analysis, 39: 57-74.
</p>
<p>Cortinhas Abrantes, J. &amp; Aerts, M. (2012) A solution to separation for clustered binary data. Statistical Modelling, 12 (1): 3-27.
</p>
<p>Cox, D. R. (1970), Analysis of Binary Data. Methuen. London.
</p>
<p>Demey, J., Vicente-Villardon, J. L., Galindo, M.P.  AND Zambrano, A. (2008) Identifying Molecular Markers Associated With Classification Of Genotypes Using External Logistic Biplots. Bioinformatics, 24(24): 2832-2838.
</p>
<p>Firth D, (1993) Bias Reduction of Maximum Likelihood Estimates, Biometrika, Vol, 80, No, 1, (Mar,, 1993), pp, 27-38.
</p>
<p>Fox, J. (1984) Linear Statistical Models and Related Methods. Wiley. New York.
</p>
<p>Harrell, F. E. (2012). rms: Regression Modeling Strategies. R package version 3.5-0. http://CRAN.R-project.org/package=rms
</p>
<p>Harrell, F. E. (2001). Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis (Springer Series in Statistics). Springer. New York.
</p>
<p>Heinze G, and Schemper M, (2002) A solution to the problem of separation in logistic regresion. Statist. Med., 21:2409-2419
</p>
<p>Heinze G. and Ploner M. (2004) Fixing the nonconvergence bug in logistic regression with SPLUS and SAS. Computer Methods and Programs in Biomedicine 71 p, 181-187
</p>
<p>Heinze, G. (2006) A comparative investigation of methods for logistic regression with separated or nearly separated data. Statist. Med., 25:4216-4226.
</p>
<p>Heinze, G. and Puhr, R. (2010) Bias-reduced and separation-proof conditional logistic regression with small or sparse data sets. Statist. Med. 29: 770-777.
</p>
<p>Hoerl, A. E. and Kennard, R.W. (1971) Rige Regression: biased estimators for nonorthogonal problems. Technometrics, 21: 55 67.
</p>
<p>Sun, H. and Wang S.  Penalized logistic regression for high-dimensional DNA methylation data with case-control studies. Bioinformatics. 28 (10): 1368-1375.
</p>
<p>Hosmer, D. and  Lemeshow, L. (1989) Applied Logistic Regression. John Wiley and Sons. Inc.   
</p>
<p>Le Cessie, S. and Van Houwelingen, J.C. (1992) Ridge Estimators in Logistic Regression.  Appl. Statist. 41 (1): 191-201.
</p>
<p>Malo, N., Libiger, O. and Schork, N. J. (2008) Accommodating Linkage Disequilibrium in Genetic-Association Analyses via Ridge Regression. Am J Hum Genet. 82(2): 375-385.
</p>
<p>Silvapulle, M. J. (1981) On the existence of maximum likelihood estimates for the binomial response models. J. R. Statist. Soc. B 43: 310-3.
</p>
<p>Vicente-Villardon, J. L., Galindo, M. P. and Blazquez, A. (2006) Logistic Biplots. In Multiple Correspondence An√°lisis And Related Methods. Grenacre, M &amp; Blasius, J, Eds,  Chapman and Hall, Boca Raton.
</p>
<p>Walter, S. and Duncan, D. (1967) Estimation of the probability of an event as a function of several variables. Biometrika. 54:167-79.
</p>
<p>Wedderburn, R. W. M. (1976) On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models. Biometrika 63, 27-32.
</p>
<p>Zhu, J. and Hastie, T. (2004) Classification of gene microarrays by penalized logistic regression. Biostatistics. 5(3):427-43.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># not yet
</code></pre>


</div>