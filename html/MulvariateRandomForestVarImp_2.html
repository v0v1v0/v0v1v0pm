<div class="container">

<table style="width: 100%;"><tr>
<td>MeanSplitImprovement</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Mean Split Improvement Importance Function</h2>

<h3>Description</h3>

<p>Mean Split Improvement Importance Function
</p>


<h3>Usage</h3>

<pre><code class="language-R">MeanSplitImprovement(
  X,
  Y,
  sample_size = trunc(nrow(X) * 0.8),
  num_trees = 100,
  m_feature = ncol(X),
  min_leaf = 10,
  alpha_threshold = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>Feature matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Target matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sample_size</code></td>
<td>
<p>Size of random subset for each tree generation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>num_trees</code></td>
<td>
<p>Number of Trees to generate</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m_feature</code></td>
<td>
<p>Number of randomly selected features considered for a split in each regression tree node, which
must be positive integer and less than N (number of input features)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>min_leaf</code></td>
<td>
<p>Minimum number of samples in the leaf node. If a node has less than or equal
to min_leaf samples, then there will be no splitting in that node and this node
will be considered as a leaf node. Valid input is positive integer, which is less
than or equal to M (number of training samples)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha_threshold</code></td>
<td>
<p>threshold for split significant testing. If default value of 0 is specified,
all the node splits will contribute to result, otherwise only those splits with improvement greater
than 1-alpha critical value of an f-statistic do.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The mean split improvement importance function follows directly from Segal (1992) definition of the
mean structure based split function. For each split defined by feature j, it calculates the difference between the within
parent node sum of squares and the within children-nodes (left and right nodes) measured on either training or testing samples.
If feature j is used in splitting M nodes of the tree, the resulting tree-specific importance measure is the sum of the
node-specific differences calculated across all M nodes. The mean split improvement measure for feature j for the multivariate
random forest is the average of the tree-specific measures across all trees in the forest.
</p>
<p>If the alpha threshold is 0 all the splits defined by feature j will be used in computing the importance measure. The user also has
the option of including only the significant node splits defined by feature j in the calculation of the importance measure.  The
significance of node splits is measured using an F-test. In this case, the user will need to threshold the alpha critical value of
the F-statistic based on the number of outcome variables in the target matrix and the number of left and right node samples
for the given node split.
</p>
<p>Segal MR (1992) Tree-structured methods for longitudinal data. J. American Stat. Assoc. 87(418), 407-418.
</p>


<h3>Value</h3>

<p>Vector of size N x 1
</p>


<h3>Examples</h3>

<pre><code class="language-R">X = matrix(runif(50*5), 50, 5)
Y = matrix(runif(50*2), 50, 2)
MeanSplitImprovement(X, Y)
</code></pre>


</div>