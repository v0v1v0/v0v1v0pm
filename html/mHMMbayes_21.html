<div class="container">

<table style="width: 100%;"><tr>
<td>vit_mHMM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Obtain hidden state sequence for each subject using the Viterbi
algorithm</h2>

<h3>Description</h3>

<p><code>vit_mHMM</code> obtains the most likely state sequence (for each subject)
from an object of class <code>mHMM</code> (generated by the function
<code>mHMM()</code>), using (an extended version of) the Viterbi algorithm. This is
also known as global decoding. Optionally, the state probabilities
themselves at each point in time can also be obtained.
</p>


<h3>Usage</h3>

<pre><code class="language-R">vit_mHMM(object, s_data, burn_in = NULL, return_state_prob = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>An object of class <code>mHMM</code>, generated
by the function <code>mHMM</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s_data</code></td>
<td>
<p>A matrix containing the observations to be modeled, where the
rows represent the observations over time. In <code>s_data</code>, the first
column indicates subject id number. Hence, the id number is repeated over
rows equal to the number of observations for that subject. The subsequent
columns contain the dependent variable(s). Note that in case of categorical
dependent  variable(s), input are integers (i.e., whole numbers) that range
from 1 to <code>q_emiss</code> (see below) and is numeric (i.e., not be a (set
of) factor variable(s)). The total number of rows are equal to the sum over
the number of observations of each subject, and the number of columns are
equal to the number of dependent variables (<code>n_dep</code>) + 1. The number
of observations can vary over subjects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>burn_in</code></td>
<td>
<p>The number of iterations to be discarded from the MCMC
algorithm when inferring the transition probability matrix gamma and the
emission distribution of (each of) the dependent variable(s) for each
subject from <code>s_data</code>. If omitted, defaults to <code>NULL</code> and
<code>burn_in</code> specified at the function <code>mHMM</code> will be used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>return_state_prob</code></td>
<td>
<p>A logical scaler. Should the function, in addition
to the most likely state sequence for each subject, also return the state
probabilities at each point in time for each subject
(<code>return_state_prob = TRUE</code>) or not (<code>return_state_prob =
FALSE</code>). Defaults to <code>return_state_prob = FALSE</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Note that local decoding is also possible, by inferring the most frequent
state at each point in time for each subject from the sampled state path at
each iteration of the MCMC algorithm. This information is contained in the
output object <code>return_path</code> of the function <code>mHMM()</code>.
</p>


<h3>Value</h3>

<p>The function <code>vit_mHMM</code> returns a matrix containing the most
likely state at each point in time. The first column indicates subject id
number. Hence, the id number is repeated over rows equal to the number of
observations for that subject. The second column contains the most likely
state at each point in time. If requested, the subsequent columns contain
the state probabilities at each point in time for each subject.
</p>


<h3>References</h3>

<p>Viterbi A (1967).
“Error bounds for convolutional codes and an asymptotically optimum decoding algorithm.”
<em>IEEE transactions on Information Theory</em>, <b>13</b>(2), 260–269.
</p>
<p>Rabiner LR (1989).
“A tutorial on hidden Markov models and selected applications in speech recognition.”
<em>Proceedings of the IEEE</em>, <b>77</b>(2), 257–286.
</p>


<h3>See Also</h3>

<p><code>mHMM</code> for analyzing multilevel hidden Markov data
and obtaining the input needed for <code>vit_mHMM</code>, and
<code>sim_mHMM</code> for simulating multilevel hidden Markov data.
</p>


<h3>Examples</h3>

<pre><code class="language-R">###### Example on package example categorical data, see ?nonverbal
# First fit the multilevel HMM on the example data

# specifying general model properties:
m &lt;- 2
n_dep &lt;- 4
q_emiss &lt;- c(3, 2, 3, 2)

# specifying starting values
start_TM &lt;- diag(.8, m)
start_TM[lower.tri(start_TM) | upper.tri(start_TM)] &lt;- .2
start_EM &lt;- list(matrix(c(0.05, 0.90, 0.05, 0.90, 0.05, 0.05), byrow = TRUE,
                        nrow = m, ncol = q_emiss[1]), # vocalizing patient
                 matrix(c(0.1, 0.9, 0.1, 0.9), byrow = TRUE, nrow = m,
                        ncol = q_emiss[2]), # looking patient
                 matrix(c(0.90, 0.05, 0.05, 0.05, 0.90, 0.05), byrow = TRUE,
                        nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                 matrix(c(0.1, 0.9, 0.1, 0.9), byrow = TRUE, nrow = m,
                        ncol = q_emiss[4])) # looking therapist

# Fit the multilevel HMM model:
# Note that for reasons of running time, J is set at a ridiculous low value.
# One would typically use a number of iterations J of at least 1000,
# and a burn_in of 200.
out_2st &lt;- mHMM(s_data = nonverbal, gen = list(m = m, n_dep = n_dep,
                q_emiss = q_emiss), start_val = c(list(start_TM), start_EM),
                mcmc = list(J = 3, burn_in = 1))

###### obtain the most likely state sequence with the Viterbi algorithm
states1 &lt;- vit_mHMM(s_data = nonverbal, object = out_2st)


###### Example on simulated multivariate continuous data
# simulating multivariate continuous data
n_t     &lt;- 100
n       &lt;- 10
m       &lt;- 3
n_dep   &lt;- 2

gamma   &lt;- matrix(c(0.8, 0.1, 0.1,
                    0.2, 0.7, 0.1,
                    0.2, 0.2, 0.6), ncol = m, byrow = TRUE)

emiss_distr &lt;- list(matrix(c( 50, 10,
                              100, 10,
                              150, 10), nrow = m, byrow = TRUE),
                    matrix(c(5, 2,
                             10, 5,
                             20, 3), nrow = m, byrow = TRUE))

data_cont &lt;- sim_mHMM(n_t = n_t, n = n, data_distr = 'continuous',
                      gen = list(m = m, n_dep = n_dep), gamma = gamma,
                      emiss_distr = emiss_distr, var_gamma = .1,
                      var_emiss = c(5^2, 0.2^2))

# Specify hyper-prior for the continuous emission distribution
manual_prior_emiss &lt;- prior_emiss_cont(
                        gen = list(m = m, n_dep = n_dep),
                        emiss_mu0 = list(matrix(c(30, 70, 170), nrow = 1),
                                         matrix(c(7, 8, 18), nrow = 1)),
                        emiss_K0 = list(1, 1),
                        emiss_V =  list(rep(5^2, m), rep(0.5^2, m)),
                        emiss_nu = list(1, 1),
                        emiss_a0 = list(rep(1.5, m), rep(1, m)),
                        emiss_b0 = list(rep(20, m), rep(4, m)))

# Run the model on the simulated data:
# Note that for reasons of running time, J is set at a ridiculous low value.
# One would typically use a number of iterations J of at least 1000,
# and a burn_in of 200.
out_3st_cont_sim &lt;- mHMM(s_data = data_cont$obs,
                    data_distr = "continuous",
                    gen = list(m = m, n_dep = n_dep),
                    start_val = c(list(gamma), emiss_distr),
                    emiss_hyp_prior = manual_prior_emiss,
                    mcmc = list(J = 11, burn_in = 5))

###### obtain the most likely state sequence with the Viterbi algorithm,
# including state probabilities at each time point.
states2 &lt;- vit_mHMM(s_data = data_cont$obs, object = out_3st_cont_sim,
                    return_state_prob = TRUE)



</code></pre>


</div>