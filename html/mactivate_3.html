<div class="container">

<table style="width: 100%;"><tr>
<td>f_control_mactivate</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Set Fitting Hyperparameters
</h2>

<h3>Description</h3>

<p>Allows user a single function to tune the mactivate fitting algorithms, <code>f_fit_gradient_01</code>, <code>f_fit_hybrid_01</code>, <code>f_fit_gradient_logistic_01</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">f_control_mactivate(
param_sensitivity = 10^9, 
bool_free_w = FALSE, 
w0_seed = 0.1, 
max_internal_iter = 500, 
w_col_search = "one", 
bool_headStart = FALSE, 
antifreeze = FALSE, 
ss_stop = 10^(-8), 
escape_rate = 1.004, 
step_size = 1/100, 
Wadj = 1/1, 
force_tries = 0, 
lambda = 0, 
tol = 10^(-8))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>param_sensitivity</code></td>
<td>

<p>Large positive scalar numeric.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bool_free_w</code></td>
<td>

<p>Scalar logical.  Allow values of <code>W</code> to wander outside [0,1]?
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w0_seed</code></td>
<td>

<p>Scalar numeric.  Usually in [0,1].  Initial value(s) for multiplicative activation layer, <code>W</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>max_internal_iter</code></td>
<td>

<p>Scalar non-negative integer. <b>Hybrid only</b>.  How many activation descent passes to make before refitting primary effects.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>w_col_search</code></td>
<td>

<p>Scalar character.  When <code>one</code>, locating <code>W</code> and corresponding coefficients is done (progressively) one column at a time; when <code>all</code>, locating <code>W</code> and corresponding coefficients is done for current column and all previous columns; When <code>alternate</code>, locating <code>W</code> and corresponding coefficients is done (progressively) one column at a time, however, after each column is fitted, an additonal pass is made fitting current column and all previous columns.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bool_headStart</code></td>
<td>

<p>Scalar logical. <b>Gradient only</b>. When <code>TRUE</code>, fitting first locates initial primary effects as a “head start” to the subsequent gradient fitting.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>antifreeze</code></td>
<td>

<p>Scalar logical. <b>Hybrid only</b>. New w/v0.6.5.  When <code>FALSE</code>, backwards compatible.  When <code>TRUE</code>, prevents hanging (non-convergence) that may rarely occur when input space is highly correlated.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ss_stop</code></td>
<td>

<p>Small positive scalar numeric.  Convergence tolerance.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>escape_rate</code></td>
<td>

<p>Scalar numeric no less than one and likely no greater than, say, 1.01.  Affinity for exiting a column search over <code>W</code>.  E.g., if 1, fitting may take a long time.  If 1.01, search for each column <code>W</code> will terminate relatively quickly.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>

<p>Positive scalar numeric.  Initial gradient step size (in both gradient and hybrid fitting algorithms) for all parameters.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Wadj</code></td>
<td>

<p>Positive scalar numeric.  Control gradient step size (in both gradient and hybrid fitting algorithms) of <code>W</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>force_tries</code></td>
<td>

<p>Scalar non-negative integer.  Force a minimum number of fitting recursions.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>Scalar numeric.  Ridge regularizer.  The actual diagonal loading imposed upon the precision matrix is equal to <code>lambda</code> times its original diagonal.  A value of <code>0</code> applies no loading; a value of <code>1</code> doubles the diagonal values of the precision matrix.  This is applied to primary effects only.  With gradient MLR fitting, i.e., <code>f_fit_gradient_01</code>, this only applies when arg <code>bool_headStart</code> is set to <code>TRUE</code> (otherwise there'd be nothing to regularize).  With hybrid MLR fitting, i.e., <code>f_fit_hybrid_01</code>, this regularization is applied at each LS step (see About vignette).  With logistic fitting, this arg <b>does nothing</b>.  Note that with logistic fitting, we can always add a small amount of white noise to <code>X</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>Small positive scalar numeric. <b>Hybrid only</b>. Similar to arg <code>ss_stop</code> above, but controls convergence tolerance after both recursions in hybrid fitting have completed.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Fitting a mactivate model to data can/will be dramatically affected by these tuning hyperparameters.  
On one extreme, one set of hyperparameters may result in the fitting algorithm fruitlessly exiting almost immediately.  Another set of hyperparameters may send the fitting algorithm to run and run for hours.
While an ideal hyperparameterization will expeditiously fit the data.
</p>


<h3>Value</h3>

<p>Named list to be passed to <code>mact_control</code> arg in fitting functions.
</p>


<h3>See Also</h3>

<p><code>f_fit_gradient_01</code>, <code>f_fit_hybrid_01</code>, <code>f_fit_gradient_logistic_01</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">


library(mactivate)

set.seed(777)

d &lt;- 20
N &lt;- 50000

X &lt;- matrix(rnorm(N*d, 0, 1), N, d)

colnames(X) &lt;- paste0("x", I(1:d))

############# primary effect slopes
b &lt;- rep_len( c(-1, 1), d )


ystar &lt;-
X %*% b +
1 * (X[ , 1]) * (X[ , 2]) * (X[ , 3]) -
1 * (X[ , 2]) * (X[ , 3]) * (X[ , 4]) * (X[ , 5])

Xall &lt;- X

errs &lt;- rnorm(N, 0, 1)
errs &lt;- 3 * (errs - mean(errs)) / sd(errs)

sd(errs)

y &lt;- ystar + errs ### response

yall &lt;- y
Nall &lt;- N



############# hybrid example


### this control setting will exit too quickly
### compare this with example below

xcmact &lt;-
f_control_mactivate(
param_sensitivity = 10^5,
w0_seed           = 0.1,
max_internal_iter = 500,
w_col_search      = "one",
ss_stop           = 10^(-5),
escape_rate       = 1.01,
Wadj              = 1/1,
lambda            = 1/1000,
tol               = 10^(-5)
)


m_tot &lt;- 4

Uall &lt;- Xall

xxnow &lt;- Sys.time()

xxls_out &lt;-
f_fit_hybrid_01(
X = Xall,
y = yall,
m_tot = m_tot,
U = Uall,
m_start = 1,
mact_control = xcmact,
verbosity = 1
)

cat( difftime(Sys.time(), xxnow, units="mins"), "\n" )

yhatG &lt;- predict(object=xxls_out, X0=Xall, U0=Uall, mcols=m_tot )

sqrt( mean( (yall  -  yhatG)^2 ) )





####################### this control setting should fit
####################### (will take a few minutes)

xcmact &lt;-
f_control_mactivate(
param_sensitivity = 10^10, ### make more sensitive
w0_seed           = 0.1,
max_internal_iter = 500,
w_col_search      = "one",
ss_stop           = 10^(-14), ### make stopping insensitive
escape_rate       = 1.001, #### discourage quitting descent
Wadj              = 1/1,
lambda            = 1/10000,
tol               = 10^(-14) ### make tolerance very small
)


m_tot &lt;- 4

Uall &lt;- Xall

xxnow &lt;- Sys.time()

xxls_out &lt;-
f_fit_hybrid_01(
X = Xall,
y = yall,
m_tot = m_tot,
U = Uall,
m_start = 1,
mact_control = xcmact,
verbosity = 1
)

cat( difftime(Sys.time(), xxnow, units="mins"), "\n" )

yhatG &lt;- predict(object=xxls_out, X0=Xall, U0=Uall, mcols=m_tot )

sqrt( mean( (yall  -  yhatG)^2 ) )


xxls_out

Xstar &lt;- f_mactivate(U=Uall, W=xxls_out[[ m_tot+1 ]][[ "What" ]])
colnames(Xstar) &lt;- paste0("xstar_", seq(1, m_tot))
Xall &lt;- cbind(Xall, Xstar)

xlm &lt;- lm(yall~Xall)
summary(xlm)



</code></pre>


</div>