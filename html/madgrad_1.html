<div class="container">

<table style="width: 100%;"><tr>
<td>optim_madgrad</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic
Optimization.</h2>

<h3>Description</h3>

<p><a href="https://arxiv.org/abs/2101.11075">MADGRAD</a> is a general purpose optimizer that
can be used in place of SGD or Adam may converge faster and generalize better.
Currently GPU-only. Typically, the same learning rate schedule that is used
for SGD or Adam may be used. The overall learning rate is not comparable to
either method and should be determined by a hyper-parameter sweep.
</p>


<h3>Usage</h3>

<pre><code class="language-R">optim_madgrad(params, lr = 0.01, momentum = 0.9, weight_decay = 0, eps = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>params</code></td>
<td>
<p>(list): List of parameters to optimize.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lr</code></td>
<td>
<p>(float): Learning rate (default: 1e-2).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>momentum</code></td>
<td>
<p>(float): Momentum value in  the range [0,1) (default: 0.9).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weight_decay</code></td>
<td>
<p>(float): Weight decay, i.e. a L2 penalty (default: 0).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>(float): Term added to the denominator outside of the root operation to improve numerical stability. (default: 1e-6).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>MADGRAD requires less weight decay than other methods, often as little as
zero. Momentum values used for SGD or Adam's beta1 should work here also.
</p>
<p>On sparse problems both weight_decay and momentum should be set to 0.
(not yet supported in the R implementation).
</p>


<h3>Value</h3>

<p>An optimizer object implementing the <code>step</code> and <code>zero_grad</code> methods.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (torch::torch_is_installed()) {
library(torch)
x &lt;- torch_randn(1, requires_grad = TRUE)
opt &lt;- optim_madgrad(x)
for (i in 1:100) {
  opt$zero_grad()
  y &lt;- x^2
  y$backward()
  opt$step()
}
all.equal(x$item(), 0, tolerance = 1e-9)
}

</code></pre>


</div>