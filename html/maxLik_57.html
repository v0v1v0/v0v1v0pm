<div class="container">

<table style="width: 100%;"><tr>
<td>numericGradient</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Functions to Calculate Numeric Derivatives</h2>

<h3>Description</h3>

<p>Calculate (central) numeric gradient and Hessian, including of
vector-valued functions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">numericGradient(f, t0, eps=1e-06, fixed, ...)
numericHessian(f, grad=NULL, t0, eps=1e-06, fixed, ...)
numericNHessian(f, t0, eps=1e-6, fixed, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>f</code></td>
<td>
<p>function to be differentiated.  The first argument must be
the parameter vector with respect to which it is differentiated.
For numeric gradient, <code>f</code> may return a (numeric) vector, for Hessian it
should return a numeric scalar</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>grad</code></td>
<td>
<p>function, gradient of <code>f</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>t0</code></td>
<td>
<p>vector, the parameter values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>numeric, the step for numeric differentiation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fixed</code></td>
<td>
<p>logical index vector, fixed parameters.
Derivative is calculated only with respect to the parameters
for which <code>fixed == FALSE</code>, <code>NA</code> is returned for the fixed
parameters.  If
missing, all parameters are treated as active.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>furter arguments for <code>f</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>numericGradient</code> numerically differentiates a (vector valued)
function with respect to it's (vector valued) argument.  If the
functions value is a <code class="reqn">N_{val} \times 1</code>
vector and the argument is
<code class="reqn">N_{par} \times 1</code> vector, the resulting
gradient
is a <code class="reqn">N_{val} \times N_{par}</code>
matrix. 
</p>
<p><code>numericHessian</code> checks whether a gradient function is present.
If yes, it calculates the gradient of the gradient, if not, it
calculates the full
numeric Hessian (<code>numericNHessian</code>).
</p>


<h3>Value</h3>

<p>Matrix.  For <code>numericGradient</code>, the number of rows is equal to the
length of the function value vector, and the number of columns is
equal to the length of the parameter vector.
</p>
<p>For the <code>numericHessian</code>, both numer of rows and columns is
equal to the length of the parameter vector.
</p>


<h3>Warning</h3>

<p>Be careful when using numerical differentiation in optimization
routines.  Although quite precise in simple cases, they may work very
poorly in more complicated conditions.
</p>


<h3>Author(s)</h3>

<p>Ott Toomet</p>


<h3>See Also</h3>

<p><code>compareDerivatives</code>, <code>deriv</code></p>


<h3>Examples</h3>

<pre><code class="language-R"># A simple example with Gaussian bell surface
f0 &lt;- function(t0) exp(-t0[1]^2 - t0[2]^2)
numericGradient(f0, c(1,2))
numericHessian(f0, t0=c(1,2))

# An example with the analytic gradient
gradf0 &lt;- function(t0) -2*t0*f0(t0)
numericHessian(f0, gradf0, t0=c(1,2))
# The results should be similar as in the previous case

# The central numeric derivatives are often quite precise
compareDerivatives(f0, gradf0, t0=1:2)
# The difference is around 1e-10
</code></pre>


</div>