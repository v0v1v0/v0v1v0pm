<div class="container">

<table style="width: 100%;"><tr>
<td>mboost-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
mboost: Model-Based Boosting
</h2>

<h3>Description</h3>

<p>Functional gradient descent algorithm
(boosting) for optimizing general risk functions utilizing
component-wise (penalized) least squares estimates or regression
trees as base-learners for fitting generalized linear, additive
and interaction models to potentially high-dimensional data.
</p>


<h3>Details</h3>

<p>This package is intended for modern regression modeling and stands
in-between classical generalized linear and additive models, as for example
implemented by <code>lm</code>, <code>glm</code>, or <code>gam</code>,
and machine-learning approaches for complex interactions models,
most prominently represented by <code>gbm</code> and
<code>randomForest</code>.
</p>
<p>All functionality in this package is based on the generic
implementation of the optimization algorithm  (function
<code>mboost_fit</code>) that allows for fitting linear, additive,
and interaction models (and mixtures of those) in low and
high dimensions. The response may be numeric, binary, ordered,
censored or count data.
</p>
<p>Both theory and applications are discussed by Buehlmann and Hothorn (2007).
UseRs without a basic knowledge of boosting methods are asked
to read this introduction before analyzing data using this package.
The examples presented in this paper are available as package vignette
<code>mboost_illustrations</code>.
</p>
<p>Note that the model fitting procedures in this package DO NOT automatically
determine an appropriate model complexity. This task is the responsibility
of the data analyst.
</p>
<p>A description of novel features that were introduced in version 2.0 is
given in Hothorn et. al (2010).
</p>
<p>Hofner et al. (2014) present a comprehensive hands-on tutorial for using the
package <code>mboost</code>, which is also available as
<code>vignette(package = "mboost", "mboost_tutorial")</code>.
</p>
<p>Ben Taieba and Hyndman (2013) used this package for fitting their model in the
Kaggle Global Energy Forecasting Competition 2012. The corresponding research
paper is a good starting point when you plan to analyze your data using
<code>mboost</code>.
</p>


<h3>NEWS in 2.9-series</h3>

<p>Series 2.9 provides a new family (<code>RCG</code>), uses <code>partykit::ctree</code> 
instead of <code>party::ctree</code> to be more flexible, allows for multivariate 
negative gradients, and leave-one-out crossvalidation. Further minor changes were 
introduces and quite some bugs were fixed. 
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.9-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.8-series</h3>

<p>Series 2.8 allows to fit models with zero boosting steps (i.e., models containing 
only the offset). Furthermore, cross-validation can now also select a model
without base-learners. In a <code>Binomial</code> family one can now specifiy
links via <code>make.link</code>. With <code>Binomial(type = "glm")</code> an alternative 
implementation of <code>Binomial</code> models is now existing and defines the model
along the lines of the <code>glm</code> implementation. Additionally, it works not only with a 
two-level factor but also with a two-column matrix containing the number of 
successes and number of failures. Finally, a new base-learner <code>bkernel</code> for
kernel boosting was added. The references were updated and a lot of bugs fixed. 
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.8-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.7-series</h3>

<p>Series 2.7 provides a new family (<code>Cindex</code>), variable importance measures
(<code>varimp</code>) and improved plotting facilities. The manual was updated in 
various places, vignettes were improved and a lot of bugs were fixed. 
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.7-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.6-series</h3>

<p>Series 2.6 includes a lot of bug fixes and improvements. Most notably,
the development of the package is now hosted entirely on github in the
project <a href="https://github.com/boost-R/mboost/">boost-R/mboost</a>.
Furthermore, the package is now maintained by Benjamin Hofner.
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.6-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.5-series</h3>

<p>Crossvaliation does not stop on errors in single folds anymore an was
sped up by setting <code>mc.preschedule = FALSE</code> if parallel
computations via <code>mclapply</code> are used. The
<code>plot.mboost</code> function is now documented. Values outside
the boundary knots are now better handeled (forbidden during fitting,
while linear extrapolation is used for prediction). Further perfomance
improvements and a lot of bug fixes have been added.
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.5-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.4-series</h3>

<p>Bootstrap confidence intervals have been implemented in the novel
<code>confint</code> function. The stability
selection procedure has now been moved to a stand-alone package called
<span class="pkg">stabs</span>, which now also implements an iterface to use stability
selection with other fitting functions. A generic function for
<code>"mboost"</code> models is implemented in <span class="pkg">mboost</span>.
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.4-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.3-series</h3>

<p>The stability selection procedure has been completely rewritten and
improved. The code base is now extensively tested. New options allow
for a less conservative error control.
</p>
<p>Constrained effects can now be fitted using quadratic programming
methods using the option <code>type = "quad.prog"</code> (default) for
highly improved speed. Additionally, new constraints have been added.
</p>
<p>Other important changes include:
</p>

<ul>
<li>
<p> A new replacement function <code>mstop(mod) &lt;- i</code> as an alternative to
<code>mod[i]</code> was added (as suggested by Achim Zeileis).
</p>
</li>
<li>
<p> We added new families <code>Hurdle</code> and <code>Multinomial</code>.
</p>
</li>
<li>
<p> We added a new argument <code>stopintern</code> for internal stopping
(based on out-of-bag data) during fitting to <code>boost_control</code>.
</p>
</li>
</ul>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.3-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.2-series</h3>

<p>Starting from version 2.2, the default for the degrees of freedom has
changed. Now the degrees of freedom are (per default) defined as
</p>
<p style="text-align: center;"><code class="reqn">\mathrm{df}(\lambda) = \mathrm{trace}(2S -
  S^{\top}S),</code>
</p>
<p> with smoother matrix
<code class="reqn">S = X(X^{\top}X + \lambda K)^{-1} X</code> (see Hofner et al., 2011). Earlier versions used the trace of the
smoother matrix <code class="reqn">\mathrm{df}(\lambda) = \mathrm{trace}(S)</code> as
degrees of freedom. One can change the old definition using
<code>options(mboost_dftraceS = TRUE)</code> (see also B. Hofner et al.,
2011 and <code>bols</code>).
</p>
<p>Other important changes include:
</p>

<ul>
<li>
<p> We switched from packages <code>multicore</code> and <code>snow</code> to
<code>parallel</code>
</p>
</li>
<li>
<p> We changed the behavior of <code>bols(x, intercept = FALSE)</code>
when <code>x</code> is a factor: now the intercept is simply dropped from
the design matrix and the coding can be specified as usually for
factors. Additionally, a new contrast is introduced:
<code>"contr.dummy"</code> (see <code>bols</code> for details).
</p>
</li>
<li>
<p> We changed the computation of B-spline basis at the
boundaries; B-splines now also use equidistant knots in the
boundaries (per default).
</p>
</li>
</ul>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.2-0" &amp; Version &lt; "2.3-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.1-series</h3>

<p>In the 2.1 series, we added multiple new base-learners including
<code>bmono</code> (monotonic effects), <code>brad</code> (radial
basis functions) and <code>bmrf</code> (Markov random fields), and
extended <code>bbs</code> to incorporate cyclic splines (via argument
<code>cyclic = TRUE</code>). We also changed the default <code>df</code> for
<code>bspatial</code> to <code>6</code>.
</p>
<p>Starting from this version, we now also automatically center the
variables in <code>glmboost</code> (argument <code>center = TRUE</code>).
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.1-0" &amp; Version &lt; "2.2-0", package  = "mboost")</code>
</p>


<h3>NEWS in 2.0-series</h3>

<p>Version 2.0 comes with new features, is faster and more accurate
in some aspects. In addition, some changes to the user interface
were necessary: Subsetting <code>mboost</code> objects changes the object.
At each time, a model is associated with a number of boosting iterations
which can be changed (increased or decreased) using the subset operator.
</p>
<p>The <code>center</code> argument in <code>bols</code> was renamed
to <code>intercept</code>. Argument <code>z</code> renamed to <code>by</code>.
</p>
<p>The base-learners <code>bns</code> and <code>bss</code> are deprecated
and replaced by <code>bbs</code> (which results in qualitatively the
same models but is computationally much more attractive).
</p>
<p>New features include new families (for example for ordinal regression)
and the <code>which</code> argument to the <code>coef</code> and <code>predict</code>
methods for selecting interesting base-learners. Predict
methods are much faster now.
</p>
<p>The memory consumption could be reduced considerably,
thanks to sparse matrix technology in package <code>Matrix</code>.
Resampling procedures run automatically in parallel
on OSes where parallelization via package <code>parallel</code> is available.
</p>
<p>The most important advancement is a generic implementation
of the optimizer in function <code>mboost_fit</code>.
</p>
<p><b>For more details and other changes see</b><br><code>news(Version &gt;= "2.0-0" &amp; Version &lt; "2.1-0", package  = "mboost")</code>
</p>


<h3>Author(s)</h3>

<p>Torsten Hothorn,Peter Buehlmann, Thomas Kneib, Matthias Schmid and
Benjamin Hofner &lt;<a href="mailto:Benjamin.Hofner@pei.de">Benjamin.Hofner@pei.de</a>&gt;
</p>


<h3>References</h3>

<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477–505. <br><a href="https://doi.org/10.1214/07-STS242">doi:10.1214/07-STS242</a>
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Matthias Schmid and
Benjamin Hofner (2010), Model-based Boosting 2.0. <em>Journal of
Machine Learning Research</em>, <b>11</b>, 2109–2113. <br><a href="https://jmlr.csail.mit.edu/papers/v11/hothorn10a.html">https://jmlr.csail.mit.edu/papers/v11/hothorn10a.html</a>
</p>
<p>Benjamin Hofner, Torsten Hothorn, Thomas Kneib, and Matthias Schmid (2011),
A framework for unbiased model selection based on boosting.
<em>Journal of Computational and Graphical Statistics</em>, <b>20</b>, 956–971. <br><a href="https://doi.org/10.1198/jcgs.2011.09220">doi:10.1198/jcgs.2011.09220</a>
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3–35.<br><a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost",
    "mboost_tutorial")</code>
</p>
<p>Souhaib Ben Taieba and Rob J. Hyndman (2014),
A gradient boosting approach to the Kaggle load forecasting competition.
<em>International Journal of Forecasting</em>, <b>30</b>, 382–394.<br><a href="https://doi.org/10.1016/j.ijforecast.2013.07.005">doi:10.1016/j.ijforecast.2013.07.005</a>
</p>


<h3>See Also</h3>

<p>The main fitting functions include:<br></p>

<ul>
<li> <p><code>gamboost</code> for boosted (generalized) additive models,
</p>
</li>
<li> <p><code>glmboost</code> for boosted linear models and
</p>
</li>
<li> <p><code>blackboost</code> for boosted trees.
</p>
</li>
</ul>
<p>Model tuning is done via cross-validation as implemented in <code>cvrisk</code>.<br>
See there for more details and further links.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
  data("bodyfat", package = "TH.data")
  set.seed(290875)

  ### model conditional expectation of DEXfat given
  model &lt;- mboost(DEXfat ~
      bols(age) +                 ### a linear function of age
      btree(hipcirc, waistcirc) + ### a smooth non-linear interaction of
                                  ### hip and waist circumference
      bbs(kneebreadth),           ### a smooth function of kneebreadth
      data = bodyfat, control = boost_control(mstop = 100))

  ### 10-fold cv for assessing `optimal' number of boosting iterations
  cvm &lt;- cvrisk(model, papply = lapply, 
                folds = cv(model.weights(model), type = "kfold"))
  ### probably needs larger initial mstop but the
  ### CRAN team is picky about running times for examples
  plot(cvm)

  ### restrict model to mstop(cvm)
  model[mstop(cvm), return = FALSE]
  mstop(model)

  ### plot age and kneebreadth
  layout(matrix(1:2, nc = 2))
  plot(model, which = c("age", "kneebreadth"))

  ### plot interaction of hip and waist circumference
  attach(bodyfat)
  nd &lt;- expand.grid(hipcirc = h &lt;- seq(from = min(hipcirc),
                                  to = max(hipcirc),
                                  length = 100),
                    waistcirc = w &lt;- seq(from = min(waistcirc),
                                  to = max(waistcirc),
                                  length = 100))
  plot(model, which = 2, newdata = nd)
  detach(bodyfat)

  ### customized plot
  layout(1)
  pr &lt;- predict(model, which = "hip", newdata = nd)
  persp(x = h, y = w, z = matrix(pr, nrow = 100, ncol = 100))

</code></pre>


</div>