<div class="container">

<table style="width: 100%;"><tr>
<td>blackboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Gradient Boosting with Regression Trees </h2>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions where regression
trees are utilized as base-learners.
</p>


<h3>Usage</h3>

<pre><code class="language-R">blackboost(formula, data = list(),
           weights = NULL, na.action = na.pass,
           offset = NULL, family = Gaussian(), 
           control = boost_control(),
           oobweights = NULL,
           tree_controls = partykit::ctree_control(
               teststat = "quad",
               testtype = "Teststatistic",
               mincriterion = 0,
               minsplit = 10, 
               minbucket = 4,
               maxdepth = 2, 
               saveinfo = FALSE),
           ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p> an optional vector of weights to be used in the fitting
process. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p> a numeric vector to be used as offset (optional).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>a <code>Family</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p> a list of parameters controlling the algorithm. For
more details see <code>boost_control</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>oobweights</code></td>
<td>
<p> an additional vector of out-of-bag weights, which is
used for the out-of-bag risk (i.e., if <code>boost_control(risk =
      "oobag")</code>). This argument is also used internally by
<code>cvrisk</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tree_controls</code></td>
<td>
<p> an object of class <code>"TreeControl"</code>, which
can be obtained using <code>ctree_control</code>. Defines
hyper-parameters for the trees which are used as base-learners. It
is wise to make sure to understand the consequences of altering any
of its arguments. By default, two-way interactions (but not deeper
trees) are fitted.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>  additional arguments passed to <code>mboost_fit</code>,
including <code>weights</code>, <code>offset</code>, <code>family</code> and
<code>control</code>. For default values see <code>mboost_fit</code>. </p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function implements the ‘classical’
gradient boosting utilizing regression trees as base-learners.
Essentially, the same algorithm is implemented in package
<code>gbm</code>. The
main difference is that arbitrary loss functions to be optimized
can be specified via the <code>family</code> argument to <code>blackboost</code> whereas
<code>gbm</code> uses hard-coded loss functions.
Moreover, the base-learners (conditional
inference trees, see <code>ctree</code>) are a little bit more flexible.
</p>
<p>The regression fit is a black box prediction machine and thus
hardly interpretable.
</p>
<p>Partial dependency plots are not yet available; see example section for
plotting of additive tree models.
</p>


<h3>Value</h3>

<p>An object of class <code>mboost</code> with <code>print</code>
and <code>predict</code> methods being available.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477–505.
</p>
<p>Torsten Hothorn, Kurt Hornik and Achim Zeileis (2006). Unbiased recursive
partitioning: A conditional inference framework. <em>Journal of
Computational and Graphical Statistics</em>, <b>15</b>(3), 651–674.
</p>
<p>Yoav Freund and Robert E. Schapire (1996),
Experiments with a new boosting algorithm.
In <em>Machine Learning: Proc. Thirteenth International Conference</em>,
148–156.
</p>
<p>Jerome H. Friedman (2001),
Greedy function approximation: A gradient boosting machine.
<em>The Annals of Statistics</em>, <b>29</b>, 1189–1232.
</p>
<p>Greg Ridgeway (1999), The state of boosting.
<em>Computing Science and Statistics</em>, <b>31</b>,
172–181.
</p>


<h3>See Also</h3>

<p>See <code>mboost_fit</code> for the generic boosting function, 
<code>glmboost</code> for boosted linear models, and
<code>gamboost</code> for boosted additive models. 
</p>
<p>See <code>baselearners</code> for possible base-learners. 
</p>
<p>See <code>cvrisk</code> for cross-validated stopping iteration. 
</p>
<p>Furthermore see <code>boost_control</code>, <code>Family</code> and
<code>methods</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
### a simple two-dimensional example: cars data
cars.gb &lt;- blackboost(dist ~ speed, data = cars,
                      control = boost_control(mstop = 50))
cars.gb

### plot fit
plot(dist ~ speed, data = cars)
lines(cars$speed, predict(cars.gb), col = "red")

### set up and plot additive tree model
if (require("partykit")) {
    ctrl &lt;- ctree_control(maxdepth = 3)
    viris &lt;- subset(iris, Species != "setosa")
    viris$Species &lt;- viris$Species[, drop = TRUE]
    imod &lt;- mboost(Species ~ btree(Sepal.Length, tree_controls = ctrl) +
                             btree(Sepal.Width, tree_controls = ctrl) +
                             btree(Petal.Length, tree_controls = ctrl) +
                             btree(Petal.Width, tree_controls = ctrl),
                   data = viris, family = Binomial())[500]
    layout(matrix(1:4, ncol = 2))
    plot(imod)
}
</code></pre>


</div>