<div class="container">

<table style="width: 100%;"><tr>
<td>mboost</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Gradient Boosting for Additive Models </h2>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions, where component-wise
arbitrary base-learners, e.g., smoothing procedures,  are utilized as additive
base-learners.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mboost(formula, data = list(), na.action = na.omit, weights = NULL, 
       offset = NULL, family = Gaussian(), control = boost_control(),
       oobweights = NULL, baselearner = c("bbs", "bols", "btree", "bss", "bns"), 
       ...)

gamboost(formula, data = list(), na.action = na.omit, weights = NULL, 
         offset = NULL, family = Gaussian(), control = boost_control(),
         oobweights = NULL, baselearner = c("bbs", "bols", "btree", "bss", "bns"),
         dfbase = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>
<p>a function which indicates what should happen when
the data contain <code>NA</code>s.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p> (optional) a numeric vector of weights to be used in 
the fitting process.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>offset</code></td>
<td>
<p> a numeric vector to be used as offset (optional).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>a <code>Family</code> object.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control</code></td>
<td>
<p> a list of parameters controlling the algorithm. For
more details see <code>boost_control</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>oobweights</code></td>
<td>
<p> an additional vector of out-of-bag weights, which is
used for the out-of-bag risk (i.e., if <code>boost_control(risk =
      "oobag")</code>). This argument is also used internally by
<code>cvrisk</code>. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>baselearner</code></td>
<td>
<p> a character specifying the component-wise base
learner to be used: <code>bbs</code> means P-splines with a
B-spline basis (see Schmid and Hothorn 2008), <code>bols</code>
linear models and <code>btree</code> boosts stumps. 
<code>bss</code> and <code>bns</code> are deprecated.
Component-wise smoothing splines have been considered in Buehlmann
and Yu (2003) and Schmid and Hothorn (2008) investigate P-splines
with a B-spline basis. Kneib, Hothorn and Tutz (2009) also utilize
P-splines with a B-spline basis, supplement them with their
bivariate tensor product version to estimate interaction surfaces
and spatial effects and also consider random effects base
learners.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dfbase</code></td>
<td>
<p> a single integer giving the degrees of freedom for P-spline 
base-learners (<code>bbs</code>) globally. </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p> additional arguments passed to <code>mboost_fit</code>; currently none.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>A (generalized) additive model is fitted using a boosting algorithm based on
component-wise base-learners. 
</p>
<p>The base-learners can either be specified via the <code>formula</code> object or via 
the <code>baselearner</code> argument. The latter argument is the default base-learner 
which is used for all variables in the formula, whithout explicit base-learner 
specification (i.e., if the base-learners are explicitly specified in <code>formula</code>, 
the <code>baselearner</code> argument will be ignored for this variable). 
</p>
<p>Of note, <code>"bss"</code> and <code>"bns"</code> are deprecated and only in the list for 
backward compatibility.
</p>
<p>Note that more base-learners (i.e., in addition to the ones provided
via <code>baselearner</code>) can be specified in <code>formula</code>. See 
<code>baselearners</code> for details.
</p>
<p>The only difference when calling <code>mboost</code> and <code>gamboost</code> is that the
latter function allows one to specify default degrees of freedom for smooth 
effects specified via <code>baselearner = "bbs"</code>. In all other cases, 
degrees of freedom need to be set manually via a specific definition of the 
corresponding base-learner.
</p>


<h3>Value</h3>

<p>An object of class <code>mboost</code> with <code>print</code>,
<code>AIC</code>, <code>plot</code> and <code>predict</code>
methods being available.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Bin Yu (2003),
Boosting with the L2 loss: regression and classification.
<em>Journal of the American Statistical Association</em>, <b>98</b>,
324–339.
</p>
<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477–505.
</p>
<p>Thomas Kneib, Torsten Hothorn and Gerhard Tutz (2009), Variable selection and
model choice in geoadditive regression models, <em>Biometrics</em>, <b>65</b>(2),
626–634.
</p>
<p>Matthias Schmid and Torsten Hothorn (2008),
Boosting additive models using component-wise P-splines as
base-learners. <em>Computational Statistics &amp; Data Analysis</em>,
<b>53</b>(2), 298–311.
</p>
<p>Torsten Hothorn, Peter Buehlmann, Thomas Kneib, Mattthias Schmid
and Benjamin Hofner (2010),
Model-based Boosting 2.0.
<em>Journal of Machine Learning Research</em>, <b>11</b>, 2109 – 2113.
</p>
<p>Benjamin Hofner, Andreas Mayr, Nikolay Robinzonov and Matthias Schmid
(2014). Model-based Boosting in R: A Hands-on Tutorial Using the R
Package mboost. <em>Computational Statistics</em>, <b>29</b>, 3–35.<br><a href="https://doi.org/10.1007/s00180-012-0382-5">doi:10.1007/s00180-012-0382-5</a>
</p>
<p>Available as vignette via: <code>vignette(package = "mboost", "mboost_tutorial")</code>
</p>


<h3>See Also</h3>

<p>See <code>mboost_fit</code> for the generic boosting function, 
<code>glmboost</code> for boosted linear models, and
<code>blackboost</code> for boosted trees. 
</p>
<p>See <code>baselearners</code> for possible base-learners. 
</p>
<p>See <code>cvrisk</code> for cross-validated stopping iteration. 
</p>
<p>Furthermore see <code>boost_control</code>, <code>Family</code> and
<code>methods</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
    ### a simple two-dimensional example: cars data
    cars.gb &lt;- gamboost(dist ~ speed, data = cars, dfbase = 4,
                        control = boost_control(mstop = 50))
    cars.gb
    AIC(cars.gb, method = "corrected")

    ### plot fit for mstop = 1, ..., 50
    plot(dist ~ speed, data = cars)
    tmp &lt;- sapply(1:mstop(AIC(cars.gb)), function(i)
        lines(cars$speed, predict(cars.gb[i]), col = "red"))
    lines(cars$speed, predict(smooth.spline(cars$speed, cars$dist),
                              cars$speed)$y, col = "green")

    ### artificial example: sinus transformation
    x &lt;- sort(runif(100)) * 10
    y &lt;- sin(x) + rnorm(length(x), sd = 0.25)
    plot(x, y)
    ### linear model
    lines(x, fitted(lm(y ~ sin(x) - 1)), col = "red")
    ### GAM
    lines(x, fitted(gamboost(y ~ x,
                    control = boost_control(mstop = 500))),
          col = "green")

</code></pre>


</div>