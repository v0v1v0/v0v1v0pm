<div class="container">

<table style="width: 100%;"><tr>
<td>types</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Build a 'types' object</h2>

<h3>Description</h3>

<p>This function builds an object of the class <code>types</code>.
</p>


<h3>Usage</h3>

<pre><code class="language-R">types(
  x,
  re_drop_line = NULL,
  line_glue = NULL,
  re_cut_area = NULL,
  re_token_splitter = re("[^_\\p{L}\\p{N}\\p{M}'-]+"),
  re_token_extractor = re("[_\\p{L}\\p{N}\\p{M}'-]+"),
  re_drop_token = NULL,
  re_token_transf_in = NULL,
  token_transf_out = NULL,
  token_to_lower = TRUE,
  perl = TRUE,
  blocksize = 300,
  verbose = FALSE,
  show_dots = FALSE,
  dot_blocksize = 10,
  file_encoding = "UTF-8",
  ngram_size = NULL,
  ngram_sep = "_",
  ngram_n_open = 0,
  ngram_open = "[]",
  as_text = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Either a list of filenames of the corpus files
(if <code>as_text</code> is <code>TRUE</code>) or the actual text of the corpus
(if <code>as_text</code> is <code>FALSE</code>).
</p>
<p>If <code>as_text</code> is <code>TRUE</code> and the length of the vector <code>x</code>
is higher than one, then each item in <code>x</code> is treated as a separate
line (or a separate series of lines) in the corpus text. Within each
item of <code>x</code>, the character <code>"\\n"</code> is also treated as
a line separator.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>re_drop_line</code></td>
<td>
<p><code>NULL</code> or character vector. If <code>NULL</code>, it is ignored.
Otherwise, a character vector (assumed to be of length 1)
containing a regular expression. Lines in <code>x</code>
that contain a match for <code>re_drop_line</code> are
treated as not belonging to the corpus and are excluded from the results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>line_glue</code></td>
<td>
<p><code>NULL</code> or character vector. If <code>NULL</code>, it is ignored.
Otherwise, all lines in a corpus file (or in <code>x</code>, if
<code>as_text</code> is <code>TRUE</code>), are glued together in one
character vector of length 1, with the string <code>line_glue</code>
pasted in between consecutive lines.
The value of <code>line_glue</code> can also be equal to the empty string <code>""</code>.
The 'line glue' operation is conducted immediately after the 'drop line' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>re_cut_area</code></td>
<td>
<p><code>NULL</code> or character vector. If <code>NULL</code>, it is ignored.
Otherwise, all matches in a corpus file (or in <code>x</code>,
if <code>as_text</code> is <code>TRUE</code>), are 'cut out' of the text prior
to the identification of the tokens in the text (and are therefore
not taken into account when identifying the tokens).
The 'cut area' operation is conducted immediately after the 'line glue' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>re_token_splitter</code></td>
<td>
<p>Regular expression or <code>NULL</code>.
Regular expression that identifies the locations where lines in the corpus
files are split into tokens. (See Details.)
</p>
<p>The 'token identification' operation is conducted immediately after the
'cut area' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>re_token_extractor</code></td>
<td>
<p>Regular expression that identifies the locations of the
actual tokens. This argument is only used if <code>re_token_splitter</code> is <code>NULL</code>.
(See Details.)
</p>
<p>The 'token identification' operation is conducted immediately after the
'cut area' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>re_drop_token</code></td>
<td>
<p>Regular expression or <code>NULL</code>. If <code>NULL</code>, it is ignored.
Otherwise, it identifies tokens that are to
be excluded from the results. Any token that contains a match for
<code>re_drop_token</code> is removed from the results.
The 'drop token' operation is conducted immediately after the 'token identification' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>re_token_transf_in</code></td>
<td>
<p>Regular expression that identifies areas in the
tokens that are to be transformed. This argument works together with the argument
<code>token_transf_out</code>.
</p>
<p>If both <code>re_token_transf_in</code> and <code>token_transf_out</code> differ
from <code>NA</code>, then all matches, in the tokens, for the
regular expression  <code>re_token_transf_in</code> are replaced with
the replacement string <code>token_transf_out</code>.
</p>
<p>The 'token transformation' operation is conducted immediately after the
'drop token' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>token_transf_out</code></td>
<td>
<p>Replacement string. This argument works together with
<code>re_token_transf_in</code> and is ignored if <code>re_token_transf_in</code>
is <code>NULL</code> or <code>NA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>token_to_lower</code></td>
<td>
<p>Logical. Whether tokens must be converted
to lowercase before returning the result.
The 'token to lower' operation is conducted immediately after the
'token transformation' operation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perl</code></td>
<td>
<p>Logical. Whether the PCRE regular expression
flavor is being used in the arguments that contain regular expressions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>blocksize</code></td>
<td>
<p>Number that indicates how many corpus files are read to memory
<code style="white-space: pre;">⁠at each individual step' during the steps in the procedure; normally the default value of ⁠</code>300' should not
be changed, but when one works with exceptionally small corpus files,
it may be worthwhile to use a higher number, and when one works with
exceptionally large corpus files, it may be worthwhile to use a lower number.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>If<code>TRUE</code>, messages are printed to the console to
indicate progress.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show_dots, dot_blocksize</code></td>
<td>
<p>If <code>TRUE</code>, dots are printed to the console to
indicate progress.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>file_encoding</code></td>
<td>
<p>File encoding that is assumed in the corpus files.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngram_size</code></td>
<td>
<p>Argument in support of ngrams/skipgrams (see also <code>max_skip</code>).
</p>
<p>If one wants to identify individual tokens, the value of <code>ngram_size</code>
should be <code>NULL</code> or <code>1</code>. If one wants to retrieve
token ngrams/skipgrams, <code>ngram_size</code> should be an integer indicating
the size of the ngrams/skipgrams. E.g. <code>2</code> for bigrams, or <code>3</code> for
trigrams, etc.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngram_sep</code></td>
<td>
<p>Character vector of length 1 containing the string that is used to
separate/link tokens in the representation of ngrams/skipgrams
in the output of this function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngram_n_open</code></td>
<td>
<p>If <code>ngram_size</code> is <code>2</code> or higher, and moreover
<code>ngram_n_open</code> is a number higher than <code>0</code>, then
ngrams with 'open slots' in them are retrieved. These
ngrams with 'open slots' are generalizations of fully lexically specific
ngrams (with the generalization being that one or more of the items
in the ngram are replaced by a notation that stands for 'any arbitrary token').
</p>
<p>For instance, if <code>ngram_size</code> is <code>4</code> and <code>ngram_n_open</code> is
<code>1</code>, and if moreover the input contains a
4-gram <code>"it_is_widely_accepted"</code>, then the output will contain
all modifications of <code>"it_is_widely_accepted"</code> in which one (since
<code>ngram_n_open</code> is <code>1</code>) of the items in this n-gram is
replaced by an open slot. The first and the last item inside
an ngram are never turned into an open slot; only the items in between
are candidates for being turned into open slots. Therefore, in the
example, the output will contain <code>"it_[]_widely_accepted"</code> and
<code>"it_is_[]_accepted"</code>.
</p>
<p>As a second example, if <code>ngram_size</code> is <code>5</code> and
<code>ngram_n_open</code> is <code>2</code>, and if moreover the input contains a
5-gram <code>"it_is_widely_accepted_that"</code>, then the output will contain
<code>"it_[]_[]_accepted_that"</code>, <code>"it_[]_widely_[]_that"</code>, and
<code>"it_is_[]_[]_that"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ngram_open</code></td>
<td>
<p>Character string used to represent open slots in ngrams in the
output of this function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>as_text</code></td>
<td>
<p>Logical.
Whether <code>x</code> is to be interpreted as a character vector containing the
actual contents of the corpus (if <code>as_text</code> is <code>TRUE</code>)
or as a character vector containing the names of the corpus files
(if <code>as_text</code> is <code>FALSE</code>).
If if <code>as_text</code> is <code>TRUE</code>, then the arguments
<code>blocksize</code>, <code>verbose</code>, <code>show_dots</code>, <code>dot_blocksize</code>,
and <code>file_encoding</code> are ignored.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The actual token identification is either based on the <code>re_token_splitter</code>
argument, a regular expression that identifies the areas between the tokens,
or on <code>re_token_extractor</code>, a regular expression that identifies the area
that are the tokens.
The first mechanism is the default mechanism: the argument <code>re_token_extractor</code>
is only used if <code>re_token_splitter</code> is <code>NULL</code>.
Currently the implementation of
<code>re_token_extractor</code> is a lot less time-efficient than that of <code>re_token_splitter</code>.
</p>


<h3>Value</h3>

<p>An object of the class <code>types</code>, which is based on a character vector.
It has additional attributes and methods such as:
</p>

<ul>
<li>
<p> base <code>print()</code>, <code>as_data_frame()</code>, <code>sort()</code> and
<code>base::summary()</code> (which returns the number of items and of unique items),
</p>
</li>
<li> <p><code>tibble::as_tibble()</code>,
</p>
</li>
<li>
<p> the <code>n_types()</code> getter and the <code>explore()</code> method,
</p>
</li>
<li>
<p> subsetting methods such as <code>keep_types()</code>, <code>keep_pos()</code>, etc. including <code style="white-space: pre;">⁠[]⁠</code>
subsetting (see brackets).
</p>
</li>
</ul>
<p>An object of class <code>types</code> can be merged with another by means of <code>types_merge()</code>,
written to file with <code>write_types()</code> and read from file with <code>write_types()</code>.
</p>


<h3>See Also</h3>

<p><code>as_types()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">toy_corpus &lt;- "Once upon a time there was a tiny toy corpus.
It consisted of three sentences. And it lived happily ever after."
(tps &lt;- types(toy_corpus, as_text = TRUE))
print(tps)

as.data.frame(tps)
as_tibble(tps)

sort(tps)
sort(tps, decreasing = TRUE)
</code></pre>


</div>