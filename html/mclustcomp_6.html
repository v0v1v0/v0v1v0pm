<div class="container">

<table style="width: 100%;"><tr>
<td>mclustcomp</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Measures for Comparing Clusterings</h2>

<h3>Description</h3>

<p>Given two partitions or clusterings <code class="reqn">C_1</code> and <code class="reqn">C_2</code>, it returns community comparison scores
corresponding with a set of designated methods. Note that two label vectors should be
of same length having either numeric or factor type. Currently we have 3 classes of methods
depending on methodological philosophy behind each. See below for the taxonomy.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mclustcomp(x, y, types = "all", tversky.param = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x, y</code></td>
<td>
<p>vectors of clustering labels</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>types</code></td>
<td>
<p><code>"all"</code> for returning scores for every available measure.
Either a single score name or a vector of score names can be supplied. See the section
for the list of the methods for details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tversky.param</code></td>
<td>
<p>a list of parameters for Tversky index; <code>alpha</code> and <code>beta</code> for
weight parameters, and <code>sym</code>, a logical where <code>FALSE</code> stands for original method, <code>TRUE</code>
for a revised variant to symmetrize the score. Default (alpha,beta)=(1,1).</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a data frame with columns <code>types</code> and corresponding <code>scores</code>.
</p>


<h3>Category 1. Counting Pairs</h3>


<table>
<tr>
<td style="text-align: center;">
TYPE </td>
<td style="text-align: left;"> FULL NAME </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'adjrand'</code>  </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Rand_index">Adjusted Rand index</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'chisq'</code>    </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Chi-squared_test">Chi-Squared Coefficient</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'fmi'</code>      </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Fowlkes-Mallows_index">Fowlkes-Mallows index</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'jaccard'</code>  </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard index</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'mirkin'</code>   </td>
<td style="text-align: left;"> Mirkin Metric, or Equivalence Mismatch Distance. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'overlap'</code>  </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Overlap_coefficient">Overlap Coefficient</a>, or Szymkiewicz-Simpson coefficient.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'pd'</code>       </td>
<td style="text-align: left;"> Partition Difference.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'rand'</code>     </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Rand_index">Rand Index</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'sdc'</code>      </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Sorensen-Dice_coefficient">Sørensen–Dice Coefficient</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'smc'</code>      </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Simple_matching_coefficient">Simple Matching Coefficient</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'tanimoto'</code> </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Jaccard_index">Tanimoto index</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'tversky'</code>  </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Tversky_index">Tversky index</a>.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'wallace1'</code> </td>
<td style="text-align: left;"> Wallace Criterion Type 1.</td>
</tr>
<tr>
<td style="text-align: center;">
<code>'wallace2'</code> </td>
<td style="text-align: left;"> Wallace Criterion Type 2.
</td>
</tr>
</table>
<p>Note that Tanimoto Coefficient and Dice's coefficient are special cases with (alpha,beta) = (1,1) and (0.5,0.5), respectively.
</p>


<h3>Category 2. Set Overlaps/Matching</h3>


<table>
<tr>
<td style="text-align: center;">
TYPE </td>
<td style="text-align: left;"> FULL NAME </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'f'</code>   </td>
<td style="text-align: left;"> F-Measure. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'mhm'</code> </td>
<td style="text-align: left;"> Meila-Heckerman Measure. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'mmm'</code> </td>
<td style="text-align: left;"> Maximum-Match Measure. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'vdm'</code> </td>
<td style="text-align: left;"> Van Dongen Measure.
</td>
</tr>
</table>
<h3>Category 3. Information Theory</h3>


<table>
<tr>
<td style="text-align: center;">
TYPE </td>
<td style="text-align: left;"> FULL NAME </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'jent'</code> </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Joint_entropy">Joint Entropy</a> </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'mi'</code>   </td>
<td style="text-align: left;"> Mutual Information. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'nmi1'</code> </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Mutual_information">Normalized Mutual Information</a> by Strehl and Ghosh. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'nmi2'</code> </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Mutual_information">Normalized Mutual Information</a> by Fred and Jain. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'nmi3'</code> </td>
<td style="text-align: left;"> Normalized Mutual Information by Danon et al. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'nvi'</code>  </td>
<td style="text-align: left;"> Normalized Variation of Information. </td>
</tr>
<tr>
<td style="text-align: center;">
<code>'vi'</code>   </td>
<td style="text-align: left;"> <a href="https://en.wikipedia.org/wiki/Variation_of_information">Variation of Information</a>.
</td>
</tr>
</table>
<h3>References</h3>

<p>Strehl A, Ghosh J (2003).
“Cluster Ensembles — a Knowledge Reuse Framework for Combining Multiple Partitions.”
<em>J. Mach. Learn. Res.</em>, <b>3</b>, 583–617.
ISSN 1532-4435.
</p>
<p>Meilă M (2007).
“Comparing clusterings—an information based distance.”
<em>Journal of Multivariate Analysis</em>, <b>98</b>(5), 873–895.
ISSN 0047259X.
</p>
<p>Meilă M (2003).
“Comparing Clusterings by the Variation of Information.”
In Goos G, Hartmanis J, van Leeuwen J, Schölkopf B, Warmuth MK (eds.), <em>Learning Theory and Kernel Machines</em>, volume 2777, 173–187.
Springer Berlin Heidelberg, Berlin, Heidelberg.
ISBN 978-3-540-40720-1 978-3-540-45167-9.
</p>
<p>Wagner S, Wagner D (2007).
“Comparing Clusterings – An Overview.”
Technical Report 2006-04, Department of Informatics.
</p>
<p>Albatineh AN, Niewiadomska-Bugaj M, Mihalko D (2006).
“On Similarity Indices and Correction for Chance Agreement.”
<em>Journal of Classification</em>, <b>23</b>(2), 301–313.
ISSN 0176-4268, 1432-1343.
</p>
<p>Mirkin B (2001).
“Eleven Ways to Look at the Chi-Squared Coefficient for Contingency Tables.”
<em>The American Statistician</em>, <b>55</b>(2), 111–120.
ISSN 0003-1305, 1537-2731.
</p>
<p>Rand WM (1971).
“Objective Criteria for the Evaluation of Clustering Methods.”
<em>Journal of the American Statistical Association</em>, <b>66</b>(336), 846.
ISSN 01621459.
</p>
<p>Kuncheva LI, Hadjitodorov ST (2004).
“Using diversity in cluster ensembles.”
In <em>Proceedings of the IEEE International Conference on Systems, Man and Cybernetics</em>, volume 2, 1214–1219.
ISBN 978-0-7803-8567-2.
</p>
<p>Fowlkes EB, Mallows CL (1983).
“A Method for Comparing Two Hierarchical Clusterings.”
<em>Journal of the American Statistical Association</em>, <b>78</b>(383), 553–569.
ISSN 0162-1459, 1537-274X.
</p>
<p>Dongen S (2000).
“Performance Criteria for Graph Clustering and Markov Cluster Experiments.”
CWI (Centre for Mathematics and Computer Science), Amsterdam, The Netherlands, The Netherlands.
</p>
<p>Jaccard P (1912).
“THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE.1.”
<em>New Phytologist</em>, <b>11</b>(2), 37–50.
ISSN 0028-646X, 1469-8137.
</p>
<p>Li T, Ogihara M, Ma S (2010).
“On combining multiple clusterings: an overview and a new perspective.”
<em>Applied Intelligence</em>, <b>33</b>(2), 207–219.
ISSN 0924-669X, 1573-7497.
</p>
<p>Larsen B, Aone C (1999).
“Fast and effective text mining using linear-time document clustering.”
In <em>Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining</em>, 16–22.
ISBN 978-1-58113-143-7.
</p>
<p>Meilă M, Heckerman D (2001).
“An Experimental Comparison of Model-Based Clustering Methods.”
<em>Machine Learning</em>, <b>42</b>(1), 9–29.
ISSN 1573-0565.
</p>
<p>Cover TM, Thomas JA (2006).
<em>Elements of information theory</em>, 2nd ed edition.
Wiley-Interscience, Hoboken, N.J.
ISBN 978-0-471-24195-9, OCLC: ocm59879802.
</p>
<p>Ana LNF, Jain AK (2003).
“Robust data clustering.”
In <em>2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, volume 2, II–128–II–133.
ISBN 978-0-7695-1900-5.
</p>
<p>Wallace DL (1983).
“Comment.”
<em>Journal of the American Statistical Association</em>, <b>78</b>(383), 569–576.
ISSN 0162-1459, 1537-274X.
</p>
<p>Simpson GG (1943).
“Mammals and the nature of continents.”
<em>American Journal of Science</em>, <b>241</b>, 1–31.
</p>
<p>Dice LR (1945).
“Measures of the Amount of Ecologic Association Between Species.”
<em>Ecology</em>, <b>26</b>(3), 297–302.
ISSN 00129658.
</p>
<p>Segaran T (2007).
<em>Programming collective intelligence: building smart web 2.0 applications</em>, 1st ed edition.
O'Reilly, Beijing ; Sebastapol [CA].
ISBN 978-0-596-52932-1, OCLC: ocn166886837.
</p>
<p>Tversky A (1977).
“Features of similarity.”
<em>Psychological Review</em>, <b>84</b>(4), 327–352.
ISSN 0033-295X.
</p>
<p>Danon L, Díaz-Guilera A, Duch J, Arenas A (2005).
“Comparing community structure identification.”
<em>Journal of Statistical Mechanics: Theory and Experiment</em>, <b>2005</b>(09), P09008–P09008.
ISSN 1742-5468.
</p>
<p>Lancichinetti A, Fortunato S, Kertész J (2009).
“Detecting the overlapping and hierarchical community structure in complex networks.”
<em>New Journal of Physics</em>, <b>11</b>(3), 033015.
ISSN 1367-2630.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## example 1. compare two identical clusterings
x = sample(1:5,20,replace=TRUE) # label from 1 to 5, 10 elements
y = x                           # set two labels x and y equal
mclustcomp(x,y)                 # show all results

## example 2. selection of a few methods
z = sample(1:4,20,replace=TRUE)           # generate a non-trivial clustering
cmethods = c("jaccard","tanimoto","rand") # select 3 methods
mclustcomp(x,z,types=cmethods)            # test with the selected scores

## example 3. tversky.param
tparam = list()                           # create an empty list
tparam$alpha = 2
tparam$beta  = 3
tparam$sym   = TRUE
mclustcomp(x,z,types="tversky")           # default set as Tanimoto case.
mclustcomp(x,z,types="tversky",tversky.param=tparam)


</code></pre>


</div>