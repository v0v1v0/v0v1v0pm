<div class="container">

<table style="width: 100%;"><tr>
<td>mface.sparse</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Fast covariance estimation for multivariate sparse functional data
</h2>

<h3>Description</h3>

<p>The function is to estimate the mean and covariance function
from a cluster of multivariate functions/longitudinal observations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mface.sparse(data, newdata = NULL, 
             center = TRUE, argvals.new = NULL, knots = 7, 
             knots.option = "equally-spaced", 
             p = 3, m = 2, 
             lambda = NULL, lambda_mean = NULL, lambda_bps = NULL, 
             search.length = 14, lower = -3, upper = 10, 
             calculate.scores = FALSE, pve = 0.99)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>a list containing all functional outcomes.
Each element is a data frame with three arguments:
(1) <code>argvals</code>: observation times;
(2) <code>subj</code>: subject indices;
(3) <code>y</code>: values of observations for each dimension.
Missing values not allowed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>newdata</code></td>
<td>

<p>of the same strucutre as <code>data</code>; defaults to NULL, then no prediction. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>

<p>logical. If TRUE, then Pspline smoothing of the population mean will be conducted and subtracted from the data before covariance smoothing; if FALSE, then the population mean will be just 0s.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>argvals.new</code></td>
<td>

<p>a vector of observation time points to evaluate mean function, covariance function, error variance and etc. If NULL, then 100 equidistant points in the range of data time points will be used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knots</code></td>
<td>

<p>the number of knots for B-spline basis functions to be used; defaults to 7.
The resulting number of basis functions is the number of interior knots plus the degree of B-splines.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>knots.option</code></td>
<td>

<p>if <code>knots</code> specifies the number of knots, then <code>knots.option</code> will be used.  Default "equally-spaced", then equally-spaced knots in the range of observed time points will be selected; alternatively, "quantile": quantiles of the observed time points will be selected; see details.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>

<p>the degrees of B-splines; defaults to 3.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>m</code></td>
<td>

<p>the order of differencing penalty; defaults to 2.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>

<p>the value of the smoothing parameter for auto-covariance smoothing; defaults to NULL.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda_mean</code></td>
<td>

<p>the value of the smoothing parameter for mean smoothing; defaults to NULL.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda_bps</code></td>
<td>

<p>the value of the smoothing parameter for cross-covariance smoothing; defaults to NULL.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>search.length</code></td>
<td>

<p>the number of equidistant (log scale) smoothing parameters to search; defaults to 14.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lower, upper</code></td>
<td>

<p>bounds for log smoothing parameter; defaults are -3 and 10, respectively.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>calculate.scores</code></td>
<td>

<p>if TRUE, scores will be calculated.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pve</code></td>
<td>

<p>Defaults 0.99. To select the number of eigenvalues by percentage of variance.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is a generalized version of bivariate P-splines (Eilers and Marx, 2003) for
covariance smoothing of multivariate sparse functional or longitudinal data.
It uses tensor product B-spline basis functions and
employs differencing penalties on the assosciated parameter matrix.
The smoothing parameters in the method are selected
by leave-one-subject-out cross validation and is implemented with a fast algorithm.
</p>
<p>If <code>center</code> is TRUE, then the population means will be calculated and are smoothed by
univariate P-spline smoothing: <code>pspline</code> (Eilers and Marx, 1996). This univariate
smoothing uses leave-one-subject-out cross validation to select the smoothing parameter.
</p>
<p>If knots.option is "equally-spaced", then the differencing penalty in Eilers and Marx (2003) is used; if knots.option is "quantile" then the integrated squared second order derivative penalty in Wood (2016) is used.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>fit</code></td>
<td>
<p>Univariate FPCA fit for each function</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.pred,mu.pred,Chat.diag.pred,se.pred,var.error.pred</code></td>
<td>
<p>Predicted/estimated objects at <code>newdata$argvals</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Theta</code></td>
<td>
<p>Estimated parameter matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>argvals.new</code></td>
<td>
<p>Vector of time points to evaluate population parameters</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Chat.new,Cor.new,Cor.raw.new,Chat.raw.diag.new,var.error.new</code></td>
<td>
<p>Estimated objects at <code>argvals.new</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eigenfunctions, eigenvalues</code></td>
<td>
<p>Estimated eigenfunctions (scaled eigenvector) and eigenvalues at <code>argvals.new</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>var.error.hat</code></td>
<td>
<p>Estimated objects for each outcome</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>calculate.scores,rand_eff</code></td>
<td>
<p>if <code>calculate.scores</code> is TRUE (default to FALSE), then predicted scores <code>rand_eff$scores</code> will be calculated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>...</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Cai Li &lt;cli9@ncsu.edu&gt; and Luo Xiao &lt;lxiao5@ncsu.edu&gt;
</p>


<h3>References</h3>

<p>Cai Li, Luo Xiao, and Sheng Luo, 2020. Fast covariance estimation for multivariate sparse functional data. Stat, 9(1), p.e245, 
doi: <a href="https://doi.org/10.1002/sta4.245">10.1002/sta4.245</a>.
</p>
<p>Luo Xiao, Cai Li, William Checkley and Ciprian Crainiceanu, Fast covariance estimation
for sparse functional data, Stat. Comput., doi: <a href="https://doi.org/10.1007/s11222-017-9744-8">10.1007/s11222-017-9744-8</a>.
</p>
<p>Paul Eilers and Brian  Marx, Multivariate calibration with temperature
interaction using two-dimensional penalized signal regression,
Chemometrics and Intelligent Laboratory Systems 66 (2003), 159-174.
</p>
<p>Paul Eilers and Brian Marx, Flexible smoothing with B-splines and penalties,
Statist. Sci., 11, 89-121, 1996.
</p>
<p>Simon N. Wood, P-splines with derivative based penalties and tensor product 
smoothing of unevenly distributed data, Stat. Comput., doi: <a href="https://doi.org/10.1007/s11222-016-9666-x">10.1007/s11222-016-9666-x</a>.
</p>


<h3>See Also</h3>

<p><code>face.sparse</code> in <code>face</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## a toy example
## settings
n &lt;- 25
sigma &lt;- 0.1
seed &lt;- 118

set.seed(seed)

## data generation
N1 &lt;- sample(3:7,n,replace=TRUE)
N2 &lt;- sample(3:7,n,replace=TRUE)
N3 &lt;- sample(3:7,n,replace=TRUE)


subj1 &lt;- c()
subj2 &lt;- c()
subj3 &lt;- c()
for(i in 1:n){
  subj1 &lt;- c(subj1,rep(i, N1[i]))
  subj2 &lt;- c(subj2,rep(i, N2[i]))
  subj3 &lt;- c(subj3,rep(i, N3[i]))
}
t1 &lt;- runif(sum(N1))
t2 &lt;- runif(sum(N2))
t3 &lt;- runif(sum(N3))
tnew &lt;- seq(0,1,length=100)
y1 &lt;- 5*sin(2*pi*t1)
y2 &lt;- 5*cos(2*pi*t2)
y3 &lt;- 5*(t3-1)^2

x1 &lt;- t(matrix(rep(5*sin(2*pi*tnew),n),length(tnew),n)) 
x2 &lt;- t(matrix(rep(5*cos(2*pi*tnew),n),length(tnew),n)) 
x3 &lt;- t(matrix(rep(5*(tnew-1)^2,n),length(tnew),n)) 

psi11 &lt;- function(x){sqrt(2/3)*sin(2*pi*x)}
psi12 &lt;- function(x){sqrt(2/3)*cos(4*pi*x)}
psi13 &lt;- function(x){sqrt(2/3)*sin(4*pi*x)}

psi21 &lt;- function(x){sqrt(2/3)*sin((1-1/2)*pi*x)}
psi22 &lt;- function(x){sqrt(2/3)*sin((2-1/2)*pi*x)}
psi23 &lt;- function(x){sqrt(2/3)*sin((3-1/2)*pi*x)}

psi31 &lt;- function(x){sqrt(2/3)*sin(1*pi*x)}
psi32 &lt;- function(x){sqrt(2/3)*sin(2*pi*x)}
psi33 &lt;- function(x){sqrt(2/3)*sin(3*pi*x)}

Lambda &lt;- c(2,1,0.5)*3

x &lt;- matrix(NA,nrow=n*length(tnew),ncol=3)
xi &lt;- matrix(NA,nrow=n,ncol=3)
for(k in 1:3){xi[,k] = rnorm(n)*sqrt(Lambda[k])}

for(i in 1:n){
  seq1 &lt;- (sum(N1[1:i])-N1[i]+1):(sum(N1[1:i]))
  seq2 &lt;- (sum(N2[1:i])-N2[i]+1):(sum(N2[1:i]))
  seq3 &lt;- (sum(N3[1:i])-N3[i]+1):(sum(N3[1:i]))
  
  Xt = xi[i,1]*c(psi11(t1[seq1]),psi21(t2[seq2]),psi31(t3[seq3])) + 
    xi[i,2]*c(psi12(t1[seq1]),psi22(t2[seq2]),psi32(t3[seq3])) + 
    xi[i,3]*c(psi13(t1[seq1]),psi23(t2[seq2]),psi33(t3[seq3]))
  
  y1[seq1] = y1[seq1] + Xt[1:N1[i]]
  y2[seq2] = y2[seq2] + Xt[N1[i]+1:N2[i]]
  y3[seq3] = y3[seq3] + Xt[N1[i]+N2[i]+1:N3[i]]
  
  x[((i-1)*length(tnew)+1) :(length(tnew)*i),] = c(x1[i,], x2[i,], x3[i,]) + 
    xi[i,1]*c(psi11(tnew),psi21(tnew),psi31(tnew)) + 
    xi[i,2]*c(psi12(tnew),psi22(tnew),psi32(tnew)) + 
    xi[i,3]*c(psi13(tnew),psi23(tnew),psi33(tnew))
}

True_C &lt;- Lambda[1]*c(psi11(tnew),psi21(tnew),psi31(tnew))%x%
  t(c(psi11(tnew), psi21(tnew), psi31(tnew))) +  
  Lambda[2]*c(psi12(tnew), psi22(tnew), psi32(tnew))%x%
  t(c(psi12(tnew), psi22(tnew), psi32(tnew))) + 
  Lambda[3]*c(psi13(tnew), psi23(tnew), psi33(tnew))%x%
  t(c(psi13(tnew), psi23(tnew), psi33(tnew)))


## observed data
y1 &lt;- y1 + rnorm(sum(N1))*sigma
y2 &lt;- y2 + rnorm(sum(N2))*sigma
y3 &lt;- y3 + rnorm(sum(N3))*sigma

# true trajectories
x1 &lt;- t(matrix(x[,1],length(tnew),n))
x2 &lt;- t(matrix(x[,2],length(tnew),n))
x3 &lt;- t(matrix(x[,3],length(tnew),n))

true_eigenfunctions &lt;- eigen(True_C)$vectors*sqrt(length(tnew))
true_eigenvalues &lt;- eigen(True_C)$values/length(tnew)

## organize data and apply mFACEs
data &lt;- list("y1" = data.frame("subj"= subj1, "argvals" = t1, "y" = y1),
             "y2" = data.frame("subj"= subj2, "argvals" = t2, "y" = y2),
             "y3" = data.frame("subj"= subj3, "argvals" = t3, "y" = y3)) 
fit &lt;- mface.sparse(data, argvals.new = tnew, knots = 5)


## set calculate.scores to TRUE if want to get scores
fit &lt;- mface.sparse(data, argvals.new = tnew, knots = 5, calculate.scores = TRUE)
scores &lt;- fit$rand_eff$scores


## prediction of several subjects
for(i in 1:2){
  sel &lt;- lapply(data, function(x){which(x$subj==i)})
  dat_i &lt;- mapply(function(data, sel){data[sel,]}, 
                  data = data, sel = sel, SIMPLIFY = FALSE)
  dat_i_pred &lt;- lapply(dat_i, function(x){
    data.frame(subj=rep(x$subj[1],nrow(x) + length(tnew)),
               argvals = c(rep(NA,nrow(x)),tnew),
               y = rep(NA,nrow(x) + length(tnew)))
  })
  for(j in 1:length(dat_i)){
    dat_i_pred[[j]][1:nrow(dat_i[[j]]), ] &lt;- dat_i[[j]]
  }
  pred &lt;- predict(fit, dat_i_pred)
  y_pred &lt;- mapply(function(pred_y.pred, dat_i){
    pred_y.pred[nrow(dat_i)+1:length(tnew)]}, pred_y.pred = pred$y.pred, 
    dat_i = dat_i, SIMPLIFY = TRUE)

  pre &lt;- pred
  
  Ylim = c(-12,12)
  Xlim = c(0,1)
  Ylab = bquote(y^(1))
  Xlab = "t"
  main = paste("Subject ", dat_i[[1]][1,1],sep="")
  idx = (nrow(dat_i[[1]])+1):(nrow(dat_i[[1]])+length(tnew))
  plot(dat_i[[1]][,"argvals"],dat_i[[1]][,"y"],ylim=Ylim,xlim=Xlim,ylab=Ylab,xlab=Xlab,
       main=main,cex.lab=2.0,cex.axis = 2.0,cex.main = 2.0,pch=1)
  lines(tnew,pre$y.pred$y1[idx],col="red",lwd=2)
  lines(tnew,pre$y.pred$y1[idx]-1.96*pre$se.pred$y1[idx],col="blue",lwd=2,lty=2)
  lines(tnew,pre$y.pred$y1[idx]+1.96*pre$se.pred$y1[idx],col="blue",lwd=2,lty=2)
  lines(tnew,x1[i,],col="purple",lwd=2)

  Ylab = bquote(y^(2))
  Xlab = "t"
  main = paste("Subject ", dat_i[[1]][1,1],sep="")
  idx = (nrow(dat_i[[2]])+1):(nrow(dat_i[[2]])+length(tnew))
  plot(dat_i[[2]][,"argvals"],dat_i[[2]][,"y"],ylim=Ylim,xlim=Xlim,ylab=Ylab,xlab=Xlab,
       main=main,cex.lab=2.0,cex.axis = 2.0,cex.main = 2.0,pch=1)
  lines(tnew,pre$y.pred$y2[idx],col="red",lwd=2)
  lines(tnew,pre$y.pred$y2[idx]-1.96*pre$se.pred$y2[idx],col="blue",lwd=2,lty=2)
  lines(tnew,pre$y.pred$y2[idx]+1.96*pre$se.pred$y2[idx],col="blue",lwd=2,lty=2)
  lines(tnew,x2[i,],col="purple",lwd=2)

  Ylab = bquote(y^(3))
  Xlab = "t"
  main = paste("Subject ", dat_i[[1]][1,1],sep="")
  idx = (nrow(dat_i[[3]])+1):(nrow(dat_i[[3]])+length(tnew))
  plot(dat_i[[3]][,"argvals"],dat_i[[3]][,"y"],ylim=Ylim,xlim=Xlim,ylab=Ylab,xlab=Xlab,
       main=main,cex.lab=2.0,cex.axis = 2.0,cex.main = 2.0,pch=1)
  lines(tnew,pre$y.pred$y3[idx],col="red",lwd=2)
  lines(tnew,pre$y.pred$y3[idx]-1.96*pre$se.pred$y3[idx],col="blue",lwd=2,lty=2)
  lines(tnew,pre$y.pred$y3[idx]+1.96*pre$se.pred$y3[idx],col="blue",lwd=2,lty=2)
  lines(tnew,x3[i,],col="purple",lwd=2)

}

</code></pre>


</div>