<div class="container">

<table style="width: 100%;"><tr>
<td>mglasso</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Inference of Multiscale Gaussian Graphical Model.</h2>

<h3>Description</h3>

<p>Cluster variables using L2 fusion penalty and simultaneously estimates a
gaussian graphical model structure with the addition of L1 sparsity penalty.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mglasso(
  x,
  lambda1 = 0,
  fuse_thresh = 0.001,
  maxit = NULL,
  distance = c("euclidean", "relative"),
  lambda2_start = 1e-04,
  lambda2_factor = 1.5,
  precision = 0.01,
  weights_ = NULL,
  type = c("initial"),
  compact = TRUE,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>Numeric matrix (<code class="reqn">n x p</code>). Multivariate normal sample with
<code class="reqn">n</code> independent observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda1</code></td>
<td>
<p>Positive numeric scalar. Lasso penalty.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fuse_thresh</code></td>
<td>
<p>Positive numeric scalar. Threshold for clusters fusion.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Integer scalar. Maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>distance</code></td>
<td>
<p>Character. Distance between regression vectors with
permutation on symmetric coefficients.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2_start</code></td>
<td>
<p>Numeric scalar. Starting value for fused-group Lasso
penalty (clustering penalty).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda2_factor</code></td>
<td>
<p>Numeric scalar. Step used to update fused-group Lasso
penalty in a multiplicative way..</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>precision</code></td>
<td>
<p>Tolerance for the stopping criterion (duality gap).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights_</code></td>
<td>
<p>Matrix of weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>If "initial" use classical version of <strong>MGLasso</strong> without
weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>compact</code></td>
<td>
<p>Logical scalar. If TRUE, only save results when previous
clusters are different from current.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Logical scalar. Print trace. Default value is FALSE.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Estimates a gaussian graphical model structure while hierarchically grouping
variables by optimizing a pseudo-likelihood function combining Lasso and
fuse-group Lasso penalties. The problem is solved via the <em>COntinuation
with NEsterov smoothing in a Shrinkage-Thresholding Algorithm</em> (Hadj-Selem et
al. 2018). Varying the fusion penalty <code class="reqn">\lambda_2</code> in a multiplicative
fashion allow to uncover a seemingly hierarchical clustering structure. For
<code class="reqn">\lambda_2 = 0</code>, the approach is theoretically equivalent to the
Meinshausen-BÃ¼hlmann (2006) <em>neighborhood selection</em> as resuming to the
optimization of <em>pseudo-likelihood</em> function with <code class="reqn">\ell_1</code> penalty
(Rocha et al., 2008). The algorithm stops when all the variables have merged
into one cluster. The criterion used to merge clusters is the
<code class="reqn">\ell_2</code>-norm distance between regression vectors.
</p>
<p>For each iteration of the algorithm, the following function is optimized :
</p>
<p style="text-align: center;"><code class="reqn">1/2 \sum_{i=1}^p || X^i - X^{\ i} \beta^i ||_2 ^2  + \lambda_1 \sum_{i
= 1}^p || \beta^i ||_1 + \lambda_2 \sum_{i &lt; j} || \beta^i -
\tau_{ij}(\beta^j) ||_2.</code>
</p>

<p>where <code class="reqn">\beta^i</code> is the vector of coefficients obtained after regression
<code class="reqn">X^i</code> on the others and <code class="reqn">\tau_{ij}</code> is a permutation exchanging
<code class="reqn">\beta_j^i</code> with <code class="reqn">\beta_i^j</code>.
</p>


<h3>Value</h3>

<p>A list-like object of class <code>mglasso</code> is returned.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>out</code></td>
<td>
<p>List of lists. Each element of the list corresponds to a
clustering level. An element named <code>levelk</code> contains the regression
matrix <code>beta</code> and clusters vector <code>clusters</code> for a clustering in
<code>k</code> clusters. When <code>compact = TRUE</code> <code>out</code> has as many
elements as the number of unique partitions. When set to <code>FALSE</code>, the
function returns as many items as the the range of values taken by
<code>lambda2</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>l1</code></td>
<td>
<p>the sparsity penalty <code>lambda1</code> used in the
problem solving.</p>
</td>
</tr>
</table>
<h3>See Also</h3>

<p><code>conesta()</code> for the problem solver,
<code>plot_mglasso()</code> for plotting the output of <code>mglasso</code>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
reticulate::use_condaenv("rmglasso", required = TRUE)
n = 50
K = 3
p = 9
rho = 0.85
blocs &lt;- list()
for (j in 1:K) {
  bloc &lt;- matrix(rho, nrow = p/K, ncol = p/K)
  for(i in 1:(p/K)) { bloc[i,i] &lt;- 1 }
  blocs[[j]] &lt;- bloc
}

mat.covariance &lt;- Matrix::bdiag(blocs)
mat.covariance

set.seed(11)
X &lt;- mvtnorm::rmvnorm(n, mean = rep(0,p), sigma = as.matrix(mat.covariance))
X &lt;- scale(X)

res &lt;- mglasso(X, 0.1, lambda2_start = 0.1)
res$out[[1]]$clusters
res$out[[1]]$beta

## End(Not run)
</code></pre>


</div>