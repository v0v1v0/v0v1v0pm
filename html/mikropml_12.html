<div class="container">

<table style="width: 100%;"><tr>
<td>calc_model_sensspec</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate and summarize performance for ROC and PRC plots</h2>

<h3>Description</h3>

<p>Use these functions to calculate cumulative sensitivity,
specificity, recall, etc. on single models, concatenate the results
together from multiple models, and compute mean ROC and PRC.
You can then plot mean ROC and PRC curves to visualize the results.
<strong>Note</strong>: These functions assume a binary outcome.
</p>


<h3>Usage</h3>

<pre><code class="language-R">calc_model_sensspec(trained_model, test_data, outcome_colname = NULL)

calc_mean_roc(sensspec_dat)

calc_mean_prc(sensspec_dat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>trained_model</code></td>
<td>
<p>Trained model from <code>caret::train()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sensspec_dat</code></td>
<td>
<p>data frame created by concatenating results of
<code>calc_model_sensspec()</code> for multiple models.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>data frame with summarized performance
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>calc_model_sensspec()</code>: Get sensitivity, specificity, and precision for a model.
</p>
</li>
<li> <p><code>calc_mean_roc()</code>: Calculate mean sensitivity over specificity for multiple models
</p>
</li>
<li> <p><code>calc_mean_prc()</code>: Calculate mean precision over recall for multiple models
</p>
</li>
</ul>
<h3>Author(s)</h3>

<p>Courtney Armour
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(dplyr)
# get cumulative performance for a single model
sensspec_1 &lt;- calc_model_sensspec(
  otu_mini_bin_results_glmnet$trained_model,
  otu_mini_bin_results_glmnet$test_data,
  "dx"
)
head(sensspec_1)

# get performance for multiple models
get_sensspec_seed &lt;- function(seed) {
  ml_result &lt;- run_ml(otu_mini_bin, "glmnet", seed = seed)
  sensspec &lt;- calc_model_sensspec(
    ml_result$trained_model,
    ml_result$test_data,
    "dx"
  ) %&gt;%
    dplyr::mutate(seed = seed)
  return(sensspec)
}
sensspec_dat &lt;- purrr::map_dfr(seq(100, 102), get_sensspec_seed)

# calculate mean sensitivity over specificity
roc_dat &lt;- calc_mean_roc(sensspec_dat)
head(roc_dat)

# calculate mean precision over recall
prc_dat &lt;- calc_mean_prc(sensspec_dat)
head(prc_dat)

# plot ROC &amp; PRC
roc_dat %&gt;% plot_mean_roc()
baseline_prec &lt;- calc_baseline_precision(otu_mini_bin, "dx", "cancer")
prc_dat %&gt;%
  plot_mean_prc(baseline_precision = baseline_prec)

# balanced precision
prior &lt;- calc_baseline_precision(otu_mini_bin,
  outcome_colname = "dx",
  pos_outcome = "cancer"
)
bprc_dat &lt;- sensspec_dat %&gt;%
  dplyr::mutate(balanced_precision = calc_balanced_precision(precision, prior)) %&gt;%
  dplyr::rename(recall = sensitivity) %&gt;%
  calc_mean_perf(group_var = recall, sum_var = balanced_precision)
bprc_dat %&gt;% plot_mean_prc(ycol = mean_balanced_precision) + ylab("Mean Bal. Precision")

## End(Not run)
</code></pre>


</div>