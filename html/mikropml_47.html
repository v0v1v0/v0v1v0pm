<div class="container">

<table style="width: 100%;"><tr>
<td>find_permuted_perf_metric</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Get permuted performance metric difference for a single feature
(or group of features)</h2>

<h3>Description</h3>

<p>Requires the <code>future.apply</code> package
</p>


<h3>Usage</h3>

<pre><code class="language-R">find_permuted_perf_metric(
  test_data,
  trained_model,
  outcome_colname,
  perf_metric_function,
  perf_metric_name,
  class_probs,
  feat,
  test_perf_value,
  nperms = 100,
  alpha = 0.05,
  progbar = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>trained_model</code></td>
<td>
<p>Trained model from <code>caret::train()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code>caret::defaultSummary()</code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>feat</code></td>
<td>
<p>feature or group of correlated features to permute.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_perf_value</code></td>
<td>
<p>value of the true performance metric on the held-out
test data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nperms</code></td>
<td>
<p>number of permutations to perform (default: <code>100</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>alpha level for the confidence interval
(default: <code>0.05</code> to obtain a 95% confidence interval)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>progbar</code></td>
<td>
<p>optional progress bar (default: <code>NULL</code>)</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>vector of mean permuted performance and mean difference between test
and permuted performance (test minus permuted performance)
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


</div>