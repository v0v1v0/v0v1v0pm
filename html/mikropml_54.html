<div class="container">

<table style="width: 100%;"><tr>
<td>get_feature_importance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Get feature importance using the permutation method</h2>

<h3>Description</h3>

<p>Calculates feature importance using a trained model and test data. Requires
the <code>future.apply</code> package.
</p>


<h3>Usage</h3>

<pre><code class="language-R">get_feature_importance(
  trained_model,
  test_data,
  outcome_colname,
  perf_metric_function,
  perf_metric_name,
  class_probs,
  method,
  seed = NA,
  corr_thresh = 1,
  groups = NULL,
  nperms = 100,
  corr_method = "spearman"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>trained_model</code></td>
<td>
<p>Trained model from <code>caret::train()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_data</code></td>
<td>
<p>Held out test data: dataframe of outcome and features.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>outcome_colname</code></td>
<td>
<p>Column name as a string of the outcome variable
(default <code>NULL</code>; the first column will be chosen automatically).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perf_metric_function</code></td>
<td>
<p>Function to calculate the performance metric to
be used for cross-validation and test performance. Some functions are
provided by caret (see <code>caret::defaultSummary()</code>).
Defaults: binary classification = <code>twoClassSummary</code>,
multi-class classification = <code>multiClassSummary</code>,
regression = <code>defaultSummary</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>perf_metric_name</code></td>
<td>
<p>The column name from the output of the function
provided to perf_metric_function that is to be used as the performance metric.
Defaults: binary classification = <code>"ROC"</code>,
multi-class classification = <code>"logLoss"</code>,
regression = <code>"RMSE"</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>class_probs</code></td>
<td>
<p>Whether to use class probabilities (TRUE for categorical outcomes, FALSE for numeric outcomes).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>ML method.
Options: <code>c("glmnet", "rf", "rpart2", "svmRadial", "xgbTree")</code>.
</p>

<ul>
<li>
<p> glmnet: linear, logistic, or multiclass regression
</p>
</li>
<li>
<p> rf: random forest
</p>
</li>
<li>
<p> rpart2: decision tree
</p>
</li>
<li>
<p> svmRadial: support vector machine
</p>
</li>
<li>
<p> xgbTree: xgboost
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>
<p>Random seed (default: <code>NA</code>).
Your results will only be reproducible if you set a seed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>corr_thresh</code></td>
<td>
<p>For feature importance, group correlations
above or equal to <code>corr_thresh</code> (range <code>0</code> to <code>1</code>; default: <code>1</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>groups</code></td>
<td>
<p>Vector of feature names to group together during permutation.
Each element should be a string with feature names separated by a pipe
character (<code>|</code>). If this is <code>NULL</code> (default), correlated features will be
grouped together based on <code>corr_thresh</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nperms</code></td>
<td>
<p>number of permutations to perform (default: <code>100</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>corr_method</code></td>
<td>
<p>correlation method. options or the same as those supported
by <code>stats::cor</code>: spearman, pearson, kendall. (default: spearman)</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>For permutation tests, the p-value is the number of permutation statistics
that are greater than the test statistic, divided by the number of
permutations. In our case, the permutation statistic is the model performance
(e.g. AUROC) after randomizing the order of observations for one feature, and
the test statistic is the actual performance on the test data. By default we
perform 100 permutations per feature; increasing this will increase the
precision of estimating the null distribution, but also increases runtime.
The p-value represents the probability of obtaining the actual performance in
the event that the null hypothesis is true, where the null hypothesis is that
the feature is not important for model performance.
</p>
<p>We strongly recommend providing multiple cores to speed up computation time.
See <a href="http://www.schlosslab.org/mikropml/articles/parallel.html">our vignette on parallel processing</a>
for more details.
</p>


<h3>Value</h3>

<p>Data frame with performance metrics for when each feature (or group
of correlated features; <code>feat</code>) is permuted (<code>perf_metric</code>), differences
between the actual test performance metric on and the permuted performance
metric (<code>perf_metric_diff</code>; test minus permuted performance), and the
p-value (<code>pvalue</code>: the probability of obtaining the actual performance
value under the null hypothesis). Features with a larger <code>perf_metric_diff</code>
are more important. The performance metric name (<code>perf_metric_name</code>) and
seed (<code>seed</code>) are also returned.
</p>


<h3>Author(s)</h3>

<p>Begüm Topçuoğlu, <a href="mailto:topcuoglu.begum@gmail.com">topcuoglu.begum@gmail.com</a>
</p>
<p>Zena Lapp, <a href="mailto:zenalapp@umich.edu">zenalapp@umich.edu</a>
</p>
<p>Kelly Sovacool, <a href="mailto:sovacool@umich.edu">sovacool@umich.edu</a>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
# If you called `run_ml()` with `feature_importance = FALSE` (the default),
# you can use `get_feature_importance()` later as long as you have the
# trained model and test data.
results &lt;- run_ml(otu_small, "glmnet", kfold = 2, cv_times = 2)
names(results$trained_model$trainingData)[1] &lt;- "dx"
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet"
)

# We strongly recommend providing multiple cores to speed up computation time.
# Do this before calling `get_feature_importance()`.
doFuture::registerDoFuture()
future::plan(future::multicore, workers = 2)

# Optionally, you can group features together with a custom grouping
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet",
  groups = c(
    "Otu00007", "Otu00008", "Otu00009", "Otu00011", "Otu00012",
    "Otu00015", "Otu00016", "Otu00018", "Otu00019", "Otu00020", "Otu00022",
    "Otu00023", "Otu00025", "Otu00028", "Otu00029", "Otu00030", "Otu00035",
    "Otu00036", "Otu00037", "Otu00038", "Otu00039", "Otu00040", "Otu00047",
    "Otu00050", "Otu00052", "Otu00054", "Otu00055", "Otu00056", "Otu00060",
    "Otu00003|Otu00002|Otu00005|Otu00024|Otu00032|Otu00041|Otu00053",
    "Otu00014|Otu00021|Otu00017|Otu00031|Otu00057",
    "Otu00013|Otu00006", "Otu00026|Otu00001|Otu00034|Otu00048",
    "Otu00033|Otu00010",
    "Otu00042|Otu00004", "Otu00043|Otu00027|Otu00049", "Otu00051|Otu00045",
    "Otu00058|Otu00044", "Otu00059|Otu00046"
  )
)

# the function can show a progress bar if you have the `progressr` package installed.
## optionally, specify the progress bar format:
progressr::handlers(progressr::handler_progress(
  format = ":message :bar :percent | elapsed: :elapsed | eta: :eta",
  clear = FALSE,
  show_after = 0
))
## tell progressr to always report progress
progressr::handlers(global = TRUE)
## run the function and watch the live progress udpates
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet"
)

# You can specify any correlation method supported by `stats::cor`:
feat_imp &lt;- get_feature_importance(results$trained_model,
  results$trained_model$trainingData,
  results$test_data,
  "dx",
  multiClassSummary,
  "AUC",
  class_probs = TRUE,
  method = "glmnet",
  corr_method = "pearson"
)

## End(Not run)

</code></pre>


</div>