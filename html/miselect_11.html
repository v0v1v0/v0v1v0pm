<div class="container">

<table style="width: 100%;"><tr>
<td>saenet</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multiple Imputation Stacked Adaptive Elastic Net</h2>

<h3>Description</h3>

<p>Fits an adaptive elastic net for multiply imputed data. The data is stacked
and is penalized that each imputation selects the same betas at each value
of lambda. "saenet" supports both continuous and binary responses.
</p>


<h3>Usage</h3>

<pre><code class="language-R">saenet(
  x,
  y,
  pf,
  adWeight,
  weights,
  family = c("gaussian", "binomial"),
  alpha = 1,
  nlambda = 100,
  lambda.min.ratio = ifelse(isTRUE(all.equal(adWeight, rep(1, p))), 0.001, 1e-06),
  lambda = NULL,
  maxit = 1000,
  eps = 1e-05
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A length <code>m</code> list of <code>n * p</code> numeric matrices. No matrix
should contain an intercept, or any missing values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>A length <code>m</code> list of length <code>n</code> numeric response vectors.
No vector should contain missing values</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pf</code></td>
<td>
<p>Penalty factor. Can be used to differentially penalize certain
variables</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>adWeight</code></td>
<td>
<p>Numeric vector of length p representing the adaptive weights
for the L1 penalty</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Numeric vector of length n containing the proportion observed
(non-missing) for each row in the un-imputed data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>The type of response. "gaussian" implies a continuous response
and "binomial" implies a binary response. Default is "gaussian".</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>Elastic net parameter. Can be a vector to cross validate over.
Default is 1</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nlambda</code></td>
<td>
<p>Length of automatically generated "lambda" sequence. If
"lambda" is non NULL, "nlambda" is ignored. Default is 100</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda.min.ratio</code></td>
<td>
<p>Ratio that determines the minimum value of "lambda"
when automatically generating a "lambda" sequence. If "lambda" is not
NULL, "lambda.min.ratio" is ignored. Default is 1e-3</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Optional numeric vector of lambdas to fit. If NULL,
<code>galasso</code> will automatically generate a lambda sequence based off
of <code>nlambda</code> and <code>lambda.min.ratio</code>. Default is NULL</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>Maximum number of iterations to run. Default is 1000</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>Tolerance for convergence. Default is 1e-5</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>saenet</code> works by stacking the multiply imputed data into a single
matrix and running a weighted adaptive elastic net on it. The objective
function is:
</p>
<p style="text-align: center;"><code class="reqn"> argmin_{\beta_j} -\frac{1}{n} \sum_{k=1}^{m} \sum_{i=1}^{n} o_i * L(\beta_j|Y_{ik},X_{ijk})</code>
</p>

<p style="text-align: center;"><code class="reqn"> + \lambda (\alpha \sum_{j=1}^{p} \hat{a}_j * pf_j |\beta_{j}|</code>
</p>

<p style="text-align: center;"><code class="reqn">+ (1 - \alpha)\sum_{j=1}^{p} pf_j * \beta_{j}^2)</code>
</p>

<p>Where L is the log likelihood, <code>o = w / m</code>, <code>a</code> is the
adaptive weights, and <code>pf</code> is the penalty factor. Simulations suggest
that the "stacked" objective function approach (i.e., <code>saenet</code>) tends
to be more computationally efficient and have better estimation and selection
properties. However, the advantage of <code>galasso</code> is that it allows one
to look at the differences between coefficient estimates across imputations.
</p>


<h3>Value</h3>

<p>An object with type saenet and subtype
saenet.gaussian or saenet.binomial, depending on which family was used.
Both subtypes have 4 elements:
</p>

<dl>
<dt>lambda</dt>
<dd>
<p>Sequence of lambda fit.</p>
</dd>
<dt>coef</dt>
<dd>
<p>nlambda x nalpha x p + 1 tensor representing the estimated betas
at each value of lambda and alpha.</p>
</dd>
<dt>df</dt>
<dd>
<p>Number of nonzero betas at each value of lambda and alpha.</p>
</dd>
</dl>
<h3>References</h3>

<p>Du, J., Boss, J., Han, P., Beesley, L. J., Kleinsasser, M., Goutman, S. A., ... 
&amp; Mukherjee, B. (2022). Variable selection with multiply-imputed datasets: 
choosing between stacked and grouped methods. Journal of Computational and 
Graphical Statistics, 31(4), 1063-1075. &lt;doi:10.1080/10618600.2022.2035739&gt;
</p>


<h3>Examples</h3>

<pre><code class="language-R">
library(miselect)
library(mice)

mids &lt;- mice(miselect.df, m = 5, printFlag = FALSE)
dfs &lt;- lapply(1:5, function(i) complete(mids, action = i))

# Generate list of imputed design matrices and imputed responses
x &lt;- list()
y &lt;- list()
for (i in 1:5) {
    x[[i]] &lt;- as.matrix(dfs[[i]][, paste0("X", 1:20)])
    y[[i]] &lt;- dfs[[i]]$Y
}

# Calculate observational weights
weights  &lt;- 1 - rowMeans(is.na(miselect.df))
pf       &lt;- rep(1, 20)
adWeight &lt;- rep(1, 20)

# Since 'Y' is a binary variable, we use 'family = "binomial"'
fit &lt;- saenet(x, y, pf, adWeight, weights, family = "binomial")

</code></pre>


</div>