<div class="container">

<table style="width: 100%;"><tr>
<td>robust.coef</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Unstandardized Coefficients with Heteroscedasticity-Consistent Standard Errors</h2>

<h3>Description</h3>

<p>This function computes heteroscedasticity-consistent standard errors and
significance values for linear models estimated by using the <code>lm()</code>
function and generalized linear models estimated by using the <code>glm()</code>
function. For linear models the heteroscedasticity-robust F-test is computed
as well. By default, the function uses the HC4 estimator.
</p>


<h3>Usage</h3>

<pre><code class="language-R">robust.coef(model, type = c("HC0", "HC1", "HC2", "HC3", "HC4", "HC4m", "HC5"),
            digits = 3, p.digits = 4, write = NULL, append = TRUE, check = TRUE,
            output = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>a fitted model of class <code>lm</code> or <code>glm</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>a character string specifying the estimation type, where
<code>"H0"</code> gives White's estimator and <code>"H1"</code> to
<code>"H5"</code> are refinement of this estimator. See help page
of the <code>vcovHC()</code> function in the R package <code>sandwich</code>
for more details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>digits</code></td>
<td>
<p>an integer value indicating the number of decimal places
to be used for displaying results. Note that information
criteria and chi-square test statistic are printed with
<code>digits</code> minus 1 decimal places.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p.digits</code></td>
<td>
<p>an integer value indicating the number of decimal places
to be used for displaying <em>p</em>-values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>write</code></td>
<td>
<p>a character string naming a file for writing the output into
either a text file with file extension <code>".txt"</code> (e.g.,
<code>"Output.txt"</code>) or Excel file with file extension
<code>".xlsx"</code>  (e.g., <code>"Output.xlsx"</code>). If the file
name does not contain any file extension, an Excel file will
be written.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>append</code></td>
<td>
<p>logical: if <code>TRUE</code> (default), output will be appended
to an existing text file with extension <code>.txt</code> specified
in <code>write</code>, if <code>FALSE</code> existing text file will be
overwritten.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check</code></td>
<td>
<p>logical: if <code>TRUE</code> (default), argument specification is checked.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>output</code></td>
<td>
<p>logical: if <code>TRUE</code> (default), output is shown.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The family of heteroscedasticity-consistent (HC) standard errors estimator for
the model parameters of a regression model is based on an HC covariance matrix
of the parameter estimates and does not require the assumption of homoscedasticity.
HC estimators approach the correct value with increasing sample size, even in
the presence of heteroscedasticity. On the other hand, the OLS standard error
estimator is biased and does not converge to the proper value when the assumption
of homoscedasticity is violated (Darlington &amp; Hayes, 2017).
</p>
<p>White (1980) introduced
the idea of HC covariance matrix to econometricians and derived the asymptotically
justified form of the HC covariance matrix known as HC0 (Long &amp; Ervin, 2000).
Simulation studies have shown that the HC0 estimator tends to underestimate the
true variance in small to moderately large samples (<code class="reqn">N \leq 250</code>) and in
the presence of leverage observations, which leads to an inflated
type I error risk (e.g., Cribari-Neto &amp; Lima, 2014). The alternative estimators
HC1 to HC5 are asymptotically equivalent to HC0 but include finite-sample corrections,
which results in superior small sample properties compared to the HC0 estimator.
Long and Ervin (2000) recommended routinely using the HC3 estimator regardless
of a heteroscedasticity test. However, the HC3 estimator can be unreliable when
the data contains leverage observations. The HC4 estimator, on
the other hand, performs well with small samples, in the presence of high leverage
observations, and when errors are not normally distributed (Cribari-Neto, 2004).
In summary, it appears that the HC4 estimator performs the best in terms of
controlling the type I and type II error risk (Rosopa, 2013). As opposed to the
findings of Cribari-Neto et al. (2007), the HC5 estimator did not show any
substantial advantages over HC4. Both HC5 and HC4 performed similarly across
all the simulation conditions considered in the study (Ng &amp; Wilcox, 2009).
</p>
<p>Note that the <em>F</em>-test of significance on the multiple correlation coefficient
<em>R</em> also assumes homoscedasticity of the errors. Violations of this assumption
can result in a hypothesis test that is either liberal or conservative, depending
on the form and severity of the heteroscedasticity.
</p>
<p>Hayes (2007) argued that using a HC estimator instead of assuming homoscedasticity
provides researchers with more confidence in the validity and statistical power
of inferential tests in regression analysis. Hence, the HC3 or HC4 estimator
should be used routinely when estimating regression models. If a HC estimator
is not used as the default method of standard error estimation, researchers are
advised to at least double-check the results by using an HC estimator to ensure
that conclusions are not compromised by heteroscedasticity. However, the presence
of heteroscedasticity suggests that the data is not adequately explained by
the statistical model of estimated conditional means. Unless heteroscedasticity
is believed to be solely caused by measurement error associated with the predictor
variable(s), it should serve as warning to the researcher regarding the adequacy
of the estimated model.
</p>


<h3>Value</h3>

<p>Returns an object of class <code>misty.object</code>, which is a list with following
entries:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>function call</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type</code></td>
<td>
<p>type of analysis</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>model specified in <code>model</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>args</code></td>
<td>
<p>specification of function arguments</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>result</code></td>
<td>
<p>list with results, i.e., <code>coef</code> for the unstandardized
regression coefficients with heteroscedasticity-consistent standard errors,
<code>F.test</code> for the heteroscedasticity-robust F-Test, and <code>sandwich</code>
for the sandwich covariance matrix</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>This function is based on the <code>vcovHC</code> function from the <code>sandwich</code>
package (Zeileis, Köll, &amp; Graham, 2020) and the functions <code>coeftest</code> and
<code>waldtest</code> from the <code>lmtest</code> package (Zeileis &amp; Hothorn, 2002).
</p>


<h3>Author(s)</h3>

<p>Takuya Yanagida <a href="mailto:takuya.yanagida@univie.ac.at">takuya.yanagida@univie.ac.at</a>
</p>


<h3>References</h3>

<p>Darlington, R. B., &amp; Hayes, A. F. (2017). <em>Regression analysis and linear
models: Concepts, applications, and implementation</em>. The Guilford Press.
</p>
<p>Cribari-Neto, F. (2004). Asymptotic inference under heteroskedasticity of unknown
form. <em>Computational Statistics &amp; Data Analysis, 45</em>, 215-233.
https://doi.org/10.1016/S0167-9473(02)00366-3
</p>
<p>Cribari-Neto, F., &amp; Lima, M. G. (2014). New heteroskedasticity-robust standard
errors for the linear regression model. <em>Brazilian Journal of Probability and Statistics, 28</em>,
83-95.
</p>
<p>Cribari-Neto, F., Souza, T., &amp; Vasconcellos, K. L. P. (2007). Inference under
heteroskedasticity and leveraged data. <em>Communications in Statistics - Theory and Methods, 36</em>,
1877-1888. https://doi.org/10.1080/03610920601126589
</p>
<p>Hayes, A.F, &amp; Cai, L. (2007). Using heteroscedasticity-consistent standard error
estimators in OLS regression: An introduction and software implementation.
<em>Behavior Research Methods, 39</em>, 709-722. https://doi.org/10.3758/BF03192961
</p>
<p>Long, J.S., &amp; Ervin, L.H. (2000). Using heteroscedasticity consistent standard
errors in the linear regression model. <em>The American Statistician, 54</em>,
217-224. https://doi.org/10.1080/00031305.2000.10474549
</p>
<p>Ng, M., &amp; Wilcoy, R. R. (2009). Level robust methods based on the least squares
regression estimator. <em>Journal of Modern Applied Statistical Methods, 8</em>,
284-395. https://doi.org/10.22237/jmasm/1257033840
</p>
<p>Rosopa, P. J., Schaffer, M. M., &amp; Schroeder, A. N. (2013). Managing heteroscedasticity
in general linear models. <em>Psychological Methods, 18</em>(3), 335-351.
https://doi.org/10.1037/a0032553
</p>
<p>White, H. (1980). A heteroskedastic-consistent covariance matrix estimator and
a direct test of heteroskedasticity. <em>Econometrica, 48</em>, 817-838.
https://doi.org/10.2307/1912934
</p>
<p>Zeileis, A., &amp; Hothorn, T. (2002). Diagnostic checking in regression relationships.
<em>R News, 2</em>(3), 7–10. http://CRAN.R-project.org/doc/Rnews/
</p>
<p>Zeileis A, Köll S, &amp; Graham N (2020). Various versatile variances: An
object-oriented implementation of clustered covariances in R.
<em>Journal of Statistical Software, 95</em>(1), 1-36.
https://doi.org/10.18637/jss.v095.i01
</p>


<h3>See Also</h3>

<p><code>std.coef</code>, <code>write.result</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">dat &lt;- data.frame(x1 = c(3, 2, 4, 9, 5, 3, 6, 4, 5, 6, 3, 5),
                  x2 = c(1, 4, 3, 1, 2, 4, 3, 5, 1, 7, 8, 7),
                  x3 = c(0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1),
                  y1 = c(2, 7, 4, 4, 7, 8, 4, 2, 5, 1, 3, 8),
                  y2 = c(0, 1, 0, 2, 0, 1, 0, 0, 1, 2, 1, 0))

#-------------------------------------------------------------------------------
# Example 1: Linear model

mod1 &lt;- lm(y1 ~ x1 + x2 + x3, data = dat)
robust.coef(mod1)

#-------------------------------------------------------------------------------
# Example 2: Generalized linear model

mod2 &lt;- glm(y2 ~ x1 + x2 + x3, data = dat, family = poisson())
robust.coef(mod2)

## Not run: 
#----------------------------------------------------------------------------
# Write Results

# Example 3a: Write Results into a text file
robust.coef(mod1, write = "Robust_Coef.txt", output = FALSE)

# Example 3b: Write Results into an Excel file
robust.coef(mod1, write = "Robust_Coef.xlsx", output = FALSE)

result &lt;- robust.coef(mod1, output = FALSE)
write.result(result, "Robust_Coef.xlsx")

## End(Not run)
</code></pre>


</div>