<div class="container">

<table style="width: 100%;"><tr>
<td>fitmssg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Computing the maximum likelihood estimator for the mixtures of skewed sub-Gaussian stable distributions using the EM algorithm.</h2>

<h3>Description</h3>

<p>Each <code class="reqn">d</code>-dimensional skewed sub-Gaussian stable (SSG) random vector <code class="reqn">\bf{Y}</code>, admits the representation given by Teimouri (2022):
</p>
<p style="text-align: center;"><code class="reqn">
{\bf{Y}} \mathop=\limits^d {\boldsymbol{\mu}}+\sqrt{P}{\boldsymbol{\lambda}}\vert{Z}_0\vert + \sqrt{P}{\Sigma}^{\frac{1}{2}}{\bf{Z}}_1,
</code>
</p>

<p>where <code class="reqn">\boldsymbol{\mu}</code> (location vector in <code class="reqn">{{{R}}}^{d}</code>, <code class="reqn">\boldsymbol{\lambda}</code> (skewness vector in <code class="reqn">{{{R}}}^{d}</code>), <code class="reqn">\Sigma</code> (positive definite symmetric dispersion matrix), and <code class="reqn">0&lt;\alpha \leq 2</code> (tail thickness) are model parameters. Furthermore, <code class="reqn">P</code> is a positive stable random variable, <code class="reqn">{Z}_0\sim N({0},1)</code>, and <code class="reqn">\bf{Z}_1\sim N_{d}\bigl({\bf{0}}, \Sigma\bigr)</code>. We note that <code class="reqn">Z</code>, <code class="reqn">Z_0</code>, and <code class="reqn">\boldsymbol{Z}_1</code> are mutually independent.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fitmssg(Y, K, eps = 0.15, initial = "FALSE", method = "moment", starts = starts)</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>an <code class="reqn">n\times d</code> matrix of observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>K</code></td>
<td>
<p>number of component.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eps</code></td>
<td>
<p>threshold value for stopping EM algorithm. It is 0.15 by default. The algorithm can be implemented faster if <code>eps</code> is larger.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>initial</code></td>
<td>
<p>logical statement. If <code>initial = TRUE</code>, then a list of the initial values must be given. Otherwise, it is determined by <code>method</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p>either <code>em</code> or <code>moment</code>. If <code>method = "moment"</code>, then the initial values are determined through the method of moment applied to each of <code class="reqn">K</code> clusters that are obtained through the k-means method of Hartigan and Wong (1979). Otherwise, the initial values for each cluster are determined through the EM algorithm (Teimouri et al., 2018) developed for sub-Gaussian stable distributions applied to each of <code class="reqn">K</code> clusters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>starts</code></td>
<td>
<p>a list of initial values if <code>initial="TRUE"</code>. The list contains a vector of length <code class="reqn">K</code> of mixing (weight) parameters, a vector of length <code class="reqn">K</code> of tail thickness parameters, <code class="reqn">K</code> vectors of length of <code class="reqn">d</code> of location parameters, <code class="reqn">K</code> dispersion matrices, <code class="reqn">K</code> vectors of length of <code class="reqn">d</code> of skewness parameters, respectively.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a list of estimated parameters corresponding to <code class="reqn">K</code> clusters, predicted labels for clusters, the log-likelihood value across iterations, the Bayesian information criterion (BIC), and the Akaike information criterion (AIC).
</p>


<h3>Author(s)</h3>

<p>Mahdi Teimouri</p>


<h3>References</h3>

<p>M. Teimouri, 2022. Finite mixture of skewed sub-Gaussian stable distributions, arxiv.org/abs/2205.14067.
</p>
<p>M. Teimouri, S. Rezakhah, and A. Mohammadpour, 2018. Parameter estimation using the EM algorithm for symmetric stable
random variables and sub-Gaussian random vectors, <em>Journal of Statistical Theory and Applications</em>, 17(3), 439-41.
</p>
<p>J. A. Hartigan, M. A. Wong, 1979. Algorithm as 136: A k-means clustering algorithm, <em>Journal of the Royal Statistical Society. Series c (Applied
Statistics)</em>, 28, 100-108.
</p>


<h3>Examples</h3>

<pre><code class="language-R">
data(bankruptcy)
out1&lt;-fitmssg(bankruptcy[,2:3], K=2, eps = 0.15, initial="FALSE", method="moment", starts=starts)
n1 &lt;- 100
n2 &lt;- 50
omega1 &lt;- n1/(n1 + n2)
omega2 &lt;- n2/(n1 + n2)
alpha1 &lt;- 1.6
alpha2 &lt;- 1.6
mu1 &lt;- c(-1, -1)
mu2 &lt;- c(6, 6)
sigma1 &lt;- matrix( c(2, 0.20, 0.20, 0.5), 2, 2 )
sigma2 &lt;- matrix( c(0.4, 0.10, 0.10, 0.2  ), 2, 2 )
lambda1 &lt;- c(5, 5)
lambda2 &lt;- c(-5, -5)
Sigma &lt;- array( NA, c(2, 2, 2) )
Sigma[, , 1] &lt;- sigma1
Sigma[, , 2] &lt;- sigma2
starts&lt;-list( c(omega1,omega2), c(alpha1,alpha2), rbind(mu1,mu2), Sigma, rbind(lambda1,lambda2) )
Y &lt;- rbind( rssg(n1 , alpha1, mu1, sigma1, lambda1),  rssg(n2, alpha2, mu2, sigma2, lambda2) )
out2&lt;-fitmssg(Y, K=2, eps=0.15, initial="TRUE", method="moment", starts=starts)

</code></pre>


</div>