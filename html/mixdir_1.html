<div class="container">

<table style="width: 100%;"><tr>
<td>find_defining_features</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Find the n defining features</h2>

<h3>Description</h3>

<p>Reduce the dimensionality of a dataset by calculating how important each feature is
for inferring the clustering.
</p>


<h3>Usage</h3>

<pre><code class="language-R">find_defining_features(mixdir_obj, X, n_features = Inf,
  measure = c("JS", "ARI"), subsample_size = Inf, step_size = Inf,
  exponential_decay = TRUE, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mixdir_obj</code></td>
<td>
<p>the result from a call to <code>mixdir()</code>. It needs to have the
fields category_prob. category_prob a list of a list of a named vector with probabilities
for each feature, latent class and possible category.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>the original dataset that was used for clustering.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n_features</code></td>
<td>
<p>the number of dimensions that should be selected. If it is
<code>Inf</code> (the default) all features are returned ordered by importance
(most important first).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measure</code></td>
<td>
<p>The measure used to assess the loss of clustering quality
if a variable is removed. Two measures are implemented: "JS" short for
Jensen-Shannon divergence comparing the original class probabilities
and the new predicted class probabilities (smaller is better),
"ARI" short for adjusted Rand index compares the overlap of the original
and the predicted classes (requires the <code>mcclust</code> package) (1 is perfect,
0 is as good as random).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subsample_size</code></td>
<td>
<p>Running this method on the full dataset can be slow,
but one can easily speed up the calculation by randomly selecting
a subset of rows from X without usually disproportionately hurting the
selection performance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>step_size</code></td>
<td>
<p>The method can either remove each feature individually
and return the n features that caused the greatest quality loss
(<code>step=Inf</code>) or iteratively remove the least important one until
the the size of the remaining features equal <code>n_features</code>
(<code>step=1</code>). Using a smaller step size increases the sensitivity
of the selection process, but takes longer to calculate.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exponential_decay</code></td>
<td>
<p>Boolean or number. Alternative way of
calculating how many features to remove each step. The default is
to always remove the least important 50% of the features
(<code>exponential_decay=2</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verbose</code></td>
<td>
<p>Boolean indicating if status messages should be printed.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Iteratively find the variable, whose removal least affects the
clustering compared with the original. If <code>n_features</code> is a finite number
the quality is a single number and reflects how good those n features maintain
the original clustering. If <code>n_features=Inf</code>, the method returns all features
ordered by decreasing importance. The accompanying quality vector contains the
"cumulative" loss if the corresponding variable would be removed.
Note that depending on the step size scheme the quality can differ. For example
if all variables are removed in one step (<code>step_size=Inf</code> and
<code>exponential_decay=FALSE</code>) the quality is not cumulative, but simply the
quality of the clustering excluding the corresponding feature. In that
sense the quality vector should not be used as a definitive answer, but
should only be used as a guidance to see where there are jumps in the quality.
</p>


<h3>See Also</h3>

<p><code>find_predictive_features</code> <code>find_typical_features</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">  
  data("mushroom")
  res &lt;- mixdir(mushroom[1:100, ], n_latent=20)
  find_defining_features(res, mushroom[1:100, ], n_features=3)
  find_defining_features(res, mushroom[1:100, ], n_features=Inf)
  
</code></pre>


</div>