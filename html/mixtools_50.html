<div class="container">

<table style="width: 100%;"><tr>
<td>normalmixEM</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>EM Algorithm for Mixtures of Univariate Normals</h2>

<h3>Description</h3>

<p>Return EM algorithm output for mixtures of normal distributions.
</p>


<h3>Usage</h3>

<pre><code class="language-R">normalmixEM(x, lambda = NULL, mu = NULL, sigma = NULL, k = 2, 
            mean.constr = NULL, sd.constr = NULL,
            epsilon = 1e-08, maxit = 1000, maxrestarts = 20, 
            verb = FALSE, fast = FALSE, ECM = FALSE,
            arbmean = TRUE, arbvar = TRUE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>A vector of length n consisting of the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>Initial value of mixing proportions.  Automatically 
repeated as necessary 
to produce a vector of length <code>k</code>, then normalized to sum to 1.
If <code>NULL</code>, then <code>lambda</code> is random from a uniform Dirichlet
distribution (i.e., its entries are uniform random and then it is 
normalized to sum to 1).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>Starting value of vector of component means.  If non-NULL and a
scalar, <code>arbmean</code> is set to <code>FALSE</code>.  If non-NULL and a vector,
<code>k</code> is set to <code>length(mu)</code>.  If NULL, then the initial value
is randomly generated from a normal distribution with center(s) determined
by binning the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>Starting value of vector of component standard deviations 
for algorithm.  If non-NULL
and a scalar, <code>arbvar</code> is set to <code>FALSE</code>.  If non-NULL and a vector,
<code>arbvar</code> is set to <code>TRUE</code> and <code>k</code> is set to <code>length(sigma)</code>.
If NULL, then the initial value is the reciprocal of the square root of
a vector of random exponential-distribution values whose means are determined
according to a binning method done on the data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>Number of components.  Initial value ignored unless <code>mu</code> 
and <code>sigma</code> are both NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mean.constr</code></td>
<td>
<p>Equality constraints on the mean parameters, given as
a vector of length <code>k</code>.  Each vector entry helps specify the constraints,
if any, on the corresponding mean parameter:  If <code>NA</code>, the corresponding
parameter is unconstrained.  If numeric, the corresponding
parameter is fixed at that value.  If a character string consisting of
a single character preceded by a coefficient, such as <code>"0.5a"</code>
or <code>"-b"</code>, all parameters using the same single character in their
constraints will fix these parameters equal to the coefficient times
some the same free parameter.  For instance, if 
<code>mean.constr = c(NA, 0, "a", "-a")</code>, then the first mean parameter
is unconstrained, the second is fixed at zero, and the third and forth
are constrained to be equal and opposite in sign.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sd.constr</code></td>
<td>
<p>Equality constraints on the standard deviation parameters.
See <code>mean.constr</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>epsilon</code></td>
<td>
<p>The convergence criterion.  Convergence is declared when the change in 
the observed data log-likelihood increases by less than epsilon.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>
<p>The maximum number of iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxrestarts</code></td>
<td>
<p>The maximum number of restarts allowed in case of a problem
with the particular starting values chosen due to one of the variance
estimates getting too small
(each restart uses randomly chosen
starting values).  It is well-known that when each component of a normal
mixture may have its own mean and variance, the likelihood has no maximizer;
in such cases, we hope to find a "nice" local maximum with this algorithm
instead, but occasionally the algorithm finds a "not nice" solution and
one of the variances goes to zero, driving the likelihood to infinity.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p>If TRUE, then various updates are printed during each 
iteration of the algorithm.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fast</code></td>
<td>
<p>If TRUE and k==2 and arbmean==TRUE, then use 
<code>normalmixEM2comp</code>, which is a much faster version of the EM 
algorithm for this case.
This version is less protected against certain kinds of underflow
that can cause numerical problems and it does not permit any restarts.  If
k&gt;2, <code>fast</code> is ignored.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ECM</code></td>
<td>
<p>logical:  Should this algorithm be an ECM algorithm in the sense
of Meng and Rubin (1993)?  If FALSE, the algorithm is a true EM algorithm;
if TRUE, then every half-iteration alternately updates the means conditional
on the variances or the variances conditional on the means, with an extra
E-step in between these updates.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>arbmean</code></td>
<td>
<p>If TRUE, then the component densities are allowed to have different <code>mu</code>s. If FALSE, then
a scale mixture will be fit.  Initial value ignored unless <code>mu</code> is NULL.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>arbvar</code></td>
<td>
<p>If TRUE, then the component densities are allowed to have different <code>sigma</code>s. If FALSE, then
a location mixture will be fit.  Initial value ignored unless <code>sigma</code> is NULL.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This is the standard EM algorithm for normal mixtures that maximizes
the conditional expected complete-data
log-likelihood at each M-step of the algorithm.
If desired, the
EM algorithm may be replaced by an ECM algorithm (see <code>ECM</code> argument)
that alternates between maximizing with respect to the <code>mu</code>
and <code>lambda</code> while holding <code>sigma</code> fixed, and maximizing with
respect to <code>sigma</code> and <code>lambda</code> while holding <code>mu</code>
fixed.  In the case where <code>arbmean</code> is <code>FALSE</code>
and <code>arbvar</code> is <code>TRUE</code>, there is no closed-form EM algorithm,
so the ECM option is forced in this case.
</p>


<h3>Value</h3>

<p><code>normalmixEM</code> returns a list of class <code>mixEM</code> with items:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>The raw data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The final mixing proportions.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>
<p>The final mean parameters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sigma</code></td>
<td>
<p>The final standard deviations. If <code>arbmean</code> = FALSE, then only the smallest standard
deviation is returned. See <code>scale</code> below.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scale</code></td>
<td>
<p>If <code>arbmean</code> = FALSE, then the scale factor for the component standard deviations is returned.
Otherwise, this is omitted from the output.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>loglik</code></td>
<td>
<p>The final log-likelihood.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>posterior</code></td>
<td>
<p>An nxk matrix of posterior probabilities for
observations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>all.loglik</code></td>
<td>
<p>A vector of each iteration's log-likelihood.  This vector
includes both the initial and the final values; thus, the number of iterations 
is one less than its length.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>restarts</code></td>
<td>
<p>The number of times the algorithm restarted due to unacceptable choice of initial values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ft</code></td>
<td>
<p>A character vector giving the name of the function.</p>
</td>
</tr>
</table>
<h3>References</h3>


<ul>
<li>
<p> McLachlan, G. J. and Peel, D. (2000) <em>Finite Mixture Models</em>, 
John Wiley and Sons, Inc.
</p>
</li>
<li>
<p> Meng, X.-L. and Rubin, D. B. (1993) Maximum Likelihood Estimation
Via the ECM Algorithm:  A General Framework, <em>Biometrika</em> 80(2):
267-278.
</p>
</li>
<li>
<p> Benaglia, T., Chauveau, D., Hunter, D. R., and Young, D.
mixtools: An R package for analyzing finite mixture models.
Journal of Statistical Software, 32(6):1-29, 2009.
</p>
</li>
</ul>
<h3>See Also</h3>

<p><code>mvnormalmixEM</code>, <code>normalmixEM2comp</code>,
<code>normalmixMMlc</code>, <code>spEMsymloc</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">##Analyzing the Old Faithful geyser data with a 2-component mixture of normals.

data(faithful)
attach(faithful)
set.seed(100)
system.time(out&lt;-normalmixEM(waiting, arbvar = FALSE, epsilon = 1e-03))
out
system.time(out2&lt;-normalmixEM(waiting, arbvar = FALSE, epsilon = 1e-03, fast=TRUE))
out2 # same thing but much faster
</code></pre>


</div>