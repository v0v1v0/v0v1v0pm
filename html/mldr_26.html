<div class="container">

<table style="width: 100%;"><tr>
<td>mldr_evaluate</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluate predictions made by a multilabel classifier</h2>

<h3>Description</h3>

<p>Taking as input an <code>mldr</code> object and a matrix with the predictions
given by a classifier, this function evaluates the classifier performance through
several multilabel metrics.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mldr_evaluate(mldr, predictions, threshold = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>mldr</code></td>
<td>
<p>Object of <code>"mldr"</code> class containing the instances to evaluate</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictions</code></td>
<td>
<p>Matrix with the labels predicted for each instance in the <code>mldr</code> parameter. Each element
should be a value into [0,1] range</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>threshold</code></td>
<td>
<p>Threshold to use to generate bipartition of labels. By default the value 0.5 is used</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A list with multilabel predictive performance measures. The items in the list will be </p>

<ul>
<li> <p><code>accuracy</code>
</p>
</li>
<li> <p><code>example_auc</code>
</p>
</li>
<li> <p><code>average_precision</code>
</p>
</li>
<li> <p><code>coverage</code>
</p>
</li>
<li> <p><code>fmeasure</code>
</p>
</li>
<li> <p><code>hamming_loss</code>
</p>
</li>
<li> <p><code>macro_auc</code>
</p>
</li>
<li> <p><code>macro_fmeasure</code>
</p>
</li>
<li> <p><code>macro_precision</code>
</p>
</li>
<li> <p><code>macro_recall</code>
</p>
</li>
<li> <p><code>micro_auc</code>
</p>
</li>
<li> <p><code>micro_fmeasure</code>
</p>
</li>
<li> <p><code>micro_precision</code>
</p>
</li>
<li> <p><code>micro_recall</code>
</p>
</li>
<li> <p><code>one_error</code>
</p>
</li>
<li> <p><code>precision</code>
</p>
</li>
<li> <p><code>ranking_loss</code>
</p>
</li>
<li> <p><code>recall</code>
</p>
</li>
<li> <p><code>subset_accuracy</code>
</p>
</li>
<li> <p><code>roc</code>
</p>
</li>
</ul>
<p>The <code>roc</code> element corresponds to a <code>roc</code> object associated to the <code>MicroAUC</code> value. This object can be given as input to <code>plot</code> for plotting the ROC curve
The <code>example_auc</code>, <code>macro_auc</code>, <code>micro_auc</code> and <code>roc</code> members will be <code>NULL</code> if the <code>pROC</code> package is not installed.
</p>


<h3>See Also</h3>

<p><code>mldr</code>, Basic metrics, Averaged metrics, Ranking-based metrics, <code>roc.mldr</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## Not run: 
library(mldr)

# Get the true labels in emotions
predictions &lt;- as.matrix(emotions$dataset[, emotions$labels$index])
# and introduce some noise (alternatively get the predictions from some classifier)
noised_labels &lt;- cbind(sample(1:593, 200, replace = TRUE), sample(1:6, 200, replace = TRUE))
predictions[noised_labels] &lt;- sample(0:1, 100, replace = TRUE)
# then evaluate predictive performance
res &lt;- mldr_evaluate(emotions, predictions)
str(res)
plot(res$roc, main = "ROC curve for emotions")

## End(Not run)
</code></pre>


</div>