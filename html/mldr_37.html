<div class="container">

<table style="width: 100%;"><tr>
<td>Averaged metrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multi-label averaged evaluation metrics</h2>

<h3>Description</h3>

<p>Evaluation metrics based on simple metrics for the confusion
matrix, averaged through several criteria.
</p>


<h3>Usage</h3>

<pre><code class="language-R">accuracy(true_labels, predicted_labels, undefined_value = "diagnose")

precision(true_labels, predicted_labels, undefined_value = "diagnose")

micro_precision(true_labels, predicted_labels, ...)

macro_precision(true_labels, predicted_labels,
  undefined_value = "diagnose")

recall(true_labels, predicted_labels, undefined_value = "diagnose")

micro_recall(true_labels, predicted_labels, ...)

macro_recall(true_labels, predicted_labels, undefined_value = "diagnose")

fmeasure(true_labels, predicted_labels, undefined_value = "diagnose")

micro_fmeasure(true_labels, predicted_labels, ...)

macro_fmeasure(true_labels, predicted_labels,
  undefined_value = "diagnose")
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true_labels</code></td>
<td>
<p>Matrix of true labels, columns corresponding to labels and
rows to instances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predicted_labels</code></td>
<td>
<p>Matrix of predicted labels, columns corresponding to
labels and rows to instances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>undefined_value</code></td>
<td>
<p>The value to be returned when a computation results in
an undefined value due to a division by zero. See details.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional parameters for precision, recall and Fmeasure.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><strong>Available metrics in this category</strong>
</p>

<ul>
<li> <p><code>accuracy</code>: Bipartition based accuracy
</p>
</li>
<li> <p><code>fmeasure</code>:  Example and binary partition F_1 measure (harmonic mean between precision and recall, averaged by instance)
</p>
</li>
<li> <p><code>macro_fmeasure</code>: Label and bipartition based F_1 measure (harmonic mean between precision and recall, macro-averaged by label)
</p>
</li>
<li> <p><code>macro_precision</code>: Label and bipartition based precision (macro-averaged by label)
</p>
</li>
<li> <p><code>macro_recall</code>: Label and bipartition based recall (macro-averaged by label)
</p>
</li>
<li> <p><code>micro_fmeasure</code>: Label and bipartition based F_1 measure (micro-averaged)
</p>
</li>
<li> <p><code>micro_precision</code>: Label and bipartition based precision (micro-averaged)
</p>
</li>
<li> <p><code>micro_recall</code>: Label and bipartition based recall (micro-averaged)
</p>
</li>
<li> <p><code>precision</code>: Example and bipartition based precision (averaged by instance)
</p>
</li>
<li> <p><code>recall</code>: Example and bipartition based recall (averaged by instance)
</p>
</li>
</ul>
<p><strong>Deciding a value when denominators are zero</strong>
</p>
<p>Parameter <code>undefined_value</code>: The value to be returned when a computation
results in an undefined value due to a division by zero. Can be a single
value (e.g. NA, 0), a function with the following signature:
</p>
<p><code>function(tp, fp, tn, fn)</code>
</p>
<p>or a string corresponding to one of the predefined strategies. These are:
</p>

<ul>
<li> <p><code>"diagnose"</code>: This strategy performs the following decision:
</p>

<ul>
<li>
<p> Returns 1 if there are no true labels and none were predicted
</p>
</li>
<li>
<p> Returns 0 otherwise
</p>
</li>
</ul>
<p>This is the default strategy, and the one followed by MULAN.
</p>
</li>
<li> <p><code>"ignore"</code>: Occurrences of undefined values will be ignored when
averaging (averages will be computed with potentially less values than
instances/labels). Undefined values in micro-averaged metrics cannot be
ignored (will return <code>NA</code>).
</p>
</li>
<li> <p><code>"na"</code>: Will return <code>NA</code> (with class <code>numeric</code>) and it
will be propagated when averaging (averaged metrics will potentially return
<code>NA</code>).
</p>
</li>
</ul>
<h3>Value</h3>

<p>Atomical numeric vector containing the resulting value in the range
[0, 1].
</p>


<h3>See Also</h3>

<p><code>mldr_evaluate</code>, <code>mldr_to_labels</code>
</p>
<p>Other evaluation metrics: <code>Basic metrics</code>,
<code>Ranking-based metrics</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">true_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,1,
0,0,0,
1,0,0
), ncol = 3, byrow = TRUE)
predicted_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,0,
1,0,0,
0,1,0
), ncol = 3, byrow = TRUE)

precision(true_labels, predicted_labels, undefined_value = "diagnose")
macro_recall(true_labels, predicted_labels, undefined_value = 0)
macro_fmeasure(
  true_labels, predicted_labels,
  undefined_value = function(tp, fp, tn, fn) as.numeric(fp == 0 &amp;&amp; fn == 0)
)
</code></pre>


</div>