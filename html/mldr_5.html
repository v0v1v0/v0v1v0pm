<div class="container">

<table style="width: 100%;"><tr>
<td>Ranking-based metrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multi-label ranking-based evaluation metrics</h2>

<h3>Description</h3>

<p>Functions that compute ranking-based metrics, given a matrix
of true labels and a matrix of predicted probabilities.
</p>


<h3>Usage</h3>

<pre><code class="language-R">average_precision(true_labels, predictions, ...)

one_error(true_labels, predictions)

coverage(true_labels, predictions, ...)

ranking_loss(true_labels, predictions)

macro_auc(true_labels, predictions, undefined_value = 0.5,
  na.rm = FALSE)

micro_auc(true_labels, predictions)

example_auc(true_labels, predictions, undefined_value = 0.5,
  na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true_labels</code></td>
<td>
<p>Matrix of true labels, columns corresponding to labels and
rows to instances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predictions</code></td>
<td>
<p>Matrix of probabilities predicted by a classifier.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Additional parameters to be passed to the ranking function.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>undefined_value</code></td>
<td>
<p>A default value for the cases when macro-averaged
and example-averaged AUC encounter undefined (not computable) values, e.g.
<code>0</code>, <code>0.5</code>, or <code>NA</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.rm</code></td>
<td>
<p>Logical specifying whether to ignore undefined values when
<code>undefined_value</code> is set to <code>NA</code>.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><strong>Available metrics in this category</strong>
</p>

<ul>
<li> <p><code>average_precision</code>: Example and ranking based average precision (how many steps have to be made in the ranking to reach a certain relevant label, averaged by instance)
</p>
</li>
<li> <p><code>coverage</code>: Example and ranking based coverage (how many steps have to be made in the ranking to cover all the relevant labels, averaged by instance)
</p>
</li>
<li> <p><code>example_auc</code>: Example based Area Under the Curve ROC (averaged by instance)
</p>
</li>
<li> <p><code>macro_auc</code>: Label and ranking based Area Under the Curve ROC (macro-averaged by label)
</p>
</li>
<li> <p><code>micro_auc</code>: Label and ranking based Area Under the Curve ROC (micro-averaged)
</p>
</li>
<li> <p><code>one_error</code>: Example and ranking based one-error (how many times the top-ranked label is not a relevant label, averaged by instance)
</p>
</li>
<li> <p><code>ranking_loss</code>: Example and ranking based ranking-loss (how many times a non-relevant label is ranked above a relevant one, evaluated for all label pairs and averaged by instance)
</p>
</li>
</ul>
<p><strong>Breaking ties in rankings</strong>
</p>
<p>The additional <code>ties_method</code> parameter for the ranking
function is passed to R's own <code>rank</code>. It accepts the following values:
</p>

<ul>
<li> <p><code>"average"</code>
</p>
</li>
<li> <p><code>"first"</code>
</p>
</li>
<li> <p><code>"last"</code>
</p>
</li>
<li> <p><code>"random"</code>
</p>
</li>
<li> <p><code>"max"</code>
</p>
</li>
<li> <p><code>"min"</code>
</p>
</li>
</ul>
<p>See <code>rank</code> for information on the effect of each
parameter.
The default behavior in mldr corresponds to value <code>"last"</code>, since this
is the behavior of the ranking method in MULAN, in order to facilitate fair
comparisons among classifiers over both platforms.
</p>


<h3>Value</h3>

<p>Atomical numeric vector specifying the resulting performance metric
value.
</p>


<h3>See Also</h3>

<p><code>mldr_evaluate</code>, <code>mldr_to_labels</code>
</p>
<p>Other evaluation metrics: <code>Averaged metrics</code>,
<code>Basic metrics</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">true_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,1,
0,0,0,
1,0,0
), ncol = 3, byrow = TRUE)
predicted_probs &lt;- matrix(c(
.6,.5,.9,
.0,.1,.2,
.8,.3,.2,
.7,.9,.1,
.7,.3,.2,
.1,.8,.3
), ncol = 3, byrow = TRUE)

# by default, labels with same ranking are assigned ascending rankings
# in the order they are encountered
coverage(true_labels, predicted_probs)
# in the following, labels with same ranking will receive the same,
# averaged ranking
average_precision(true_labels, predicted_probs, ties_method = "average")

# the following will treat all undefined values as 0 (counting them
# for the average)
example_auc(true_labels, predicted_probs, undefined_value = 0)
# the following will ignore undefined values (not counting them for
# the average)
example_auc(true_labels, predicted_probs, undefined_value = NA, na.rm = TRUE)
</code></pre>


</div>