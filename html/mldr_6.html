<div class="container">

<table style="width: 100%;"><tr>
<td>Basic metrics</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Multi-label evaluation metrics</h2>

<h3>Description</h3>

<p>Several evaluation metrics designed for multi-label problems.
</p>


<h3>Usage</h3>

<pre><code class="language-R">hamming_loss(true_labels, predicted_labels)

subset_accuracy(true_labels, predicted_labels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>true_labels</code></td>
<td>
<p>Matrix of true labels, columns corresponding to labels and
rows to instances.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>predicted_labels</code></td>
<td>
<p>Matrix of predicted labels, columns corresponding to
labels and rows to instances.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><strong>Available metrics in this category</strong>
</p>

<ul>
<li> <p><code>hamming_loss</code>: describes
the average absolute distance between a predicted label and its true value.
</p>
</li>
<li> <p><code>subset_accuracy</code>: the ratio of correctly predicted labelsets.
</p>
</li>
</ul>
<h3>Value</h3>

<p>Resulting value in the range [0, 1]
</p>


<h3>See Also</h3>

<p><code>mldr_evaluate</code>, <code>mldr_to_labels</code>
</p>
<p>Other evaluation metrics: <code>Averaged metrics</code>,
<code>Ranking-based metrics</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">true_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,1,
0,0,0,
1,0,0
), ncol = 3, byrow = TRUE)
predicted_labels &lt;- matrix(c(
1,1,1,
0,0,0,
1,0,0,
1,1,0,
1,0,0,
0,1,0
), ncol = 3, byrow = TRUE)

hamming_loss(true_labels, predicted_labels)
subset_accuracy(true_labels, predicted_labels)
</code></pre>


</div>