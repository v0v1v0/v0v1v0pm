<div class="container">

<table style="width: 100%;"><tr>
<td>mle.tools-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Overview of the “mle.tools” Package</h2>

<h3>Description</h3>

<p>The current version of the <span class="pkg">mle.tools</span> package has implemented three functions which are of great interest in maximum likelihood estimation. These functions calculates the expected /observed Fisher information and the bias-corrected maximum likelihood estimate(s) using the bias formula introduced by Cox and Snell (1968). They can be applied to any probability density function whose terms are available in the derivatives table of <code>D</code> function (see “deriv.c” source code for further details). Integrals, when required, are computed numerically via <code>integrate</code> function. Below are some mathematical details of how the returned values are calculated.
</p>
<p>Let <code class="reqn">X_{1},\ldots ,X_{n}</code> be <em>i.i.d.</em> random variables with
probability density functions <code class="reqn">f(x_{i}\mid \bold{\theta })</code> depending on a <code class="reqn">p</code>-dimensional parameter vector <code class="reqn">\bold{\theta } = (\theta_1,\ldots,\theta_p)</code>. The <em>(j,k)-th</em> element of the observed, <code class="reqn">H_{jk}</code>, and expected, <code class="reqn">I_{jk}</code>, Fisher information are calculated, respectively,  as
</p>
<p style="text-align: center;"><code class="reqn">H_{jk} =\left. {-\sum\limits_{i=1}^{n}\frac{%
\partial ^{2}}{\partial \theta _{j}\partial \theta _{k}}\log f\left(
x_{i}\mid {\bold{\theta} }\right) }\right\vert _{\bold{\theta }=\widehat{\bold{%
\theta }}}</code>
</p>

<p>and
</p>
<p style="text-align: center;"><code class="reqn">I_{jk}=-n\times E\left( \frac{\partial ^{2}}{\partial \theta _{j}\partial
\theta _{k}}\log f\left( x\mid \bold{\theta }\right) \right) =\left. -n\times
\int\limits_{\mathcal{X} }\frac{\partial ^{2}}{\partial \theta _{j}\partial
\theta _{k}}\log f\left( x\mid \bold{\theta }\right) \times f\left(
x\mid \bold{\theta }\right) dx\right\vert _{\bold{\theta }=\widehat{\bold{%
\theta }}}</code>
</p>

<p>where <code class="reqn">(j,k=1,\ldots,p)</code>, <code class="reqn">\bold{\widehat{\theta}}</code> is the maximum likelihood estimate of <code class="reqn">\bold{\theta}</code> and <code class="reqn">\mathcal{X}</code> denotes the support of the random variable <code class="reqn">X</code>.
</p>
<p>The <code>observed.varcov</code> function returns the inputted maximum likelihood estimate(s) and the inverse of <code class="reqn">\bold{H}</code> while the <code>expected.varcov</code> function returns the inputted maximum likelihood estimate(s) and the inverse of <code class="reqn">\bold{I}</code>. If <code class="reqn">\bold{H}</code> and/or <code class="reqn">\bold{I}</code> are singular an error message is returned.
</p>
<p>Furthermore, the bias corrected maximum likelihood estimate of <code class="reqn">\theta_s</code>   (<code class="reqn">s=1,\ldots,p)</code>, denoted by <code class="reqn">\widetilde{\theta_s}</code>, is
calculated as <code class="reqn">\widetilde{\theta_s} = \widehat{\theta} - \widehat{Bias}(\widehat{\theta}_s)</code>, where <code class="reqn">\widehat{\theta}_s</code> is the maximum likelihood estimate of <code class="reqn">{\theta}_s</code> and
</p>
<p style="text-align: center;"><code class="reqn">{\widehat{Bias}\left( {\widehat{\theta }}_{s}\right) =}\left. {%
\sum\limits_{j=1}^{p}\sum\limits_{k=1}^{p}\sum\limits_{l=1}^{p}\kappa
^{sj}\kappa ^{kl}\left[ 0.5\kappa _{{jkl}}+\kappa _{{jk,l}}\right] }%
\right\vert _{\bold{\theta }=\widehat{\bold{\theta }}}</code>
</p>
<p> where <code class="reqn">\kappa ^{jk}</code> is the <em>(j,k)-th</em> element of the inverse of the expected Fisher information, <code class="reqn">{\kappa_{jkl}=} n\times E\left( \frac{\partial ^{3}}{\partial \theta _{j}\partial {{\theta}}_{k}{\theta }_{l}}\log f\left( x\mid \bold{\theta }\right) \right)</code> and
<code class="reqn">\kappa_{jk,l}= n \times E\left( \frac{\partial ^{2}}{\partial \theta _{j}\partial \theta_{k}}\log f\left( x\mid\bold{\theta }\right) \times \frac{\partial }{{\theta }_{l}}\log f\left( x\mid\bold{\theta }\right) \right) </code>.
</p>
<p>The bias-corrected maximum likelihood estimate(s) and some other quantities are calculated via <code>coxsnell.bc</code> function. If the numerical integration fails
and/or <code class="reqn">\bold{I}</code> is singular an error message is returned.
</p>
<p>It is noteworthy that for a series of probability distributions it is possible, after extensive algebra, to obtain the analytical expressions for <code class="reqn">Bias({\widehat{\theta}_s)}</code>. In Stosic and Cordeiro (2009) are the analytic expressions for 22 two-parameter continuous probability distributions. They also present the <em>Maple</em> and <em>Mathematica</em> scripts used to obtain all analytic expressions (see Cordeiro and Cribari-Neto 2014 for further details).
</p>


<h3>Author(s)</h3>

<p>Josmar Mazucheli <a href="mailto:jmazucheli@gmail.com">jmazucheli@gmail.com</a>
</p>


<h3>References</h3>

<p>Azzalini, A. (1996). <em>Statistical Inference: Based on the Likelihood</em>. London: Chapman and Hall.
</p>
<p>Cordeiro, G. M. and Cribari-Neto, F., (2014). An introduction to Bartlett correction and bias reduction. SpringerBriefs in Statistics, New-York.
</p>
<p>Cordeiro, G. M. and McCullagh, P., (1991). Bias correction in generalized linear models. <em>Journal of the Royal Statistical Society, Series B</em>, <b>53</b>, 3, 629–643.
</p>
<p>Cox, D. R. and Hinkley, D. V. (1974). <em>Theoretical Statistics</em>. London: Chapman and Hall.
</p>
<p>Cox, D. R. and Snell, E. J., (1968). A general definition of residuals (with discussion). <em>Journal of the Royal Statistical Society, Series B</em>, <b>30</b>, 2, 24–275.
</p>
<p>Efron, B. and Hinkley, D. V. (1978). Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information. <em>Biometrika</em>, <b>65</b>, 3, 457–482.
</p>
<p>Pawitan, Y. (2001). <em>In All Likelihood: Statistical Modelling and Inference Using Likelihood</em>. Oxford: Oxford University Press.
</p>
<p>Stosic, B. D. and Cordeiro, G. M., (2009). Using Maple and Mathematica to derive bias corrections for two parameter distributions. <em>Journal of Statistical Computation and Simulation</em>, <b>79</b>, 6, 751–767.
</p>


</div>