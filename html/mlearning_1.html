<div class="container">

<table style="width: 100%;"><tr>
<td>mlearning-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Machine Learning Algorithms with Unified Interface and Confusion Matrices</h2>

<h3>Description</h3>

<p>This package provides wrappers around several existing machine learning
algorithms in R, under a unified user interface. Confusion matrices can also
be calculated and viewed as tables or plots. Key features are:
</p>

<ul>
<li>
<p> Unified, formula-based interface for all algorithms, similar to
<code>stats::lm()</code>.
</p>
</li>
<li>
<p> Optimized code when a simplified formula <code>y ~ .</code> is used, meaning all
variables in data are used (one of them (<code>y</code> here) is the class to be
predicted (classification problem, a factor variable), or the dependent
variable of the model (regression problem, a numeric variable).
</p>
</li>
<li>
<p> Similar way of dealing with missing data, both in the training set and in
predictions. Underlying algorithms deal differently with missing data. Some
accept them, other not.
</p>
</li>
<li>
<p> Unified way of dealing with factor levels that have no cases in the
training set. The training succeeds, but the classifier is, of course, unable
to classify items in the missing class.
</p>
</li>
<li>
<p> The <code>predict()</code> methods have similar arguments. They return the class,
membership to the classes, both, or something else (probabilities,
raw predictions, ...) depending on the algorithm or the problem
(classification or regression).
</p>
</li>
<li>
<p> The <code>cvpredict()</code> method is available for all algorithms and it performs
very easily a cross-validation, or even a leave_one_out validation (when
<code>cv.k</code> = number of cases). It operates transparently for the end-user.
</p>
</li>
<li>
<p> The <code>confusion()</code> method creates a confusion matrix and the object can be
printed, summarized, plotted. Various metrics are easily derived from the
confusion matrix. Also, it allows to adjust prior probabilities of the
classes in a classification problem, in order to obtain more representative
estimates of the metrics when priors are adjusted to values closes to real
proportions of classes in the data.
</p>
</li>
</ul>
<p>See <code>mlearning()</code> for further explanations and an example analysis. See
<code>mlLda()</code> for examples of the different forms of the formula that can be
used. See <code>plot.confusion()</code> for the different ways to explore the confusion
matrix.
</p>


<h3>Important functions</h3>


<ul>
<li> <p><code>ml_lda()</code>, <code>ml_qda()</code>, <code>ml_naive_bayes()</code>, <code>ml_knn()</code>, <code>ml_lvq()</code>,
<code>ml_nnet()</code>, <code>ml_rpart()</code>, <code>ml_rforest()</code> and <code>ml_svm()</code> to train classifiers
or regressors with the different algorithms that are supported in the
package,
</p>
</li>
<li> <p><code>predict()</code> and <code>cvpredict()</code> for predictions, including using
cross-validation,
</p>
</li>
<li> <p><code>confusion()</code> to calculate the confusion matrix (with various methods to
analyze it and to calculate derived metrics like recall, precision, F-score,
...)
</p>
</li>
<li> <p><code>prior()</code> to adjust prior probabilities,
</p>
</li>
<li> <p><code>response()</code> and <code>train()</code> to extract response and training variables from
an <strong>mlearning</strong> object.
</p>
</li>
</ul>
</div>