<div class="container">

<table style="width: 100%;"><tr>
<td>MLTuneParameters</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>R6 Class to perform hyperparameter tuning experiments</h2>

<h3>Description</h3>

<p>The <code>MLTuneParameters</code> class is used to construct a parameter tuner object
and to perform the tuning of a set of hyperparameters for a specified
machine learning algorithm using either a grid search or a Bayesian
optimization.
</p>


<h3>Details</h3>

<p>The hyperparameter tuning can be performed with a grid search or a Bayesian
optimization. In both cases, each hyperparameter setting is evaluated in a
k-fold cross-validation on the dataset specified.
</p>


<h3>Super classes</h3>

<p><code>mlexperiments::MLBase</code> -&gt; <code>mlexperiments::MLExperimentsBase</code> -&gt; <code>MLTuneParameters</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>parameter_bounds</code></dt>
<dd>
<p>A named list of tuples to define the parameter
bounds of the Bayesian hyperparameter optimization. For further details
please see the documentation of the <code>ParBayesianOptimization</code> package.</p>
</dd>
<dt><code>parameter_grid</code></dt>
<dd>
<p>A matrix with named columns in which each column
represents a parameter that should be optimized and each row represents
a specific hyperparameter setting that should be tested throughout the
procedure. For <code>strategy = "grid"</code>, each row of the <code>parameter_grid</code> is
considered as a setting that is evaluated. For <code>strategy = "bayesian"</code>,
the <code>parameter_grid</code> is passed further on to the <code>initGrid</code> argument of
the function <code>ParBayesianOptimization::bayesOpt()</code> in order to
initialize the Bayesian process. The maximum rows considered for
initializing the Bayesian process can be specified with the R option
<code>option("mlexperiments.bayesian.max_init")</code>, which is set to <code>50L</code> by
default.</p>
</dd>
<dt><code>optim_args</code></dt>
<dd>
<p>A named list of tuples to define the parameter
bounds of the Bayesian hyperparameter optimization. For further details
please see the documentation of the <code>ParBayesianOptimization</code> package.</p>
</dd>
<dt><code>split_type</code></dt>
<dd>
<p>A character. The splitting strategy to construct the
k cross-validation folds. This parameter is passed further on to the
function <code>splitTools::create_folds()</code> and defaults to <code>"stratified"</code>.</p>
</dd>
<dt><code>split_vector</code></dt>
<dd>
<p>A vector If another criteria than the provided <code>y</code>
should be considered for generating the cross-validation folds, it can
be defined here. It is important, that a vector of the same length as
<code>x</code> is provided here.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-MLTuneParameters-new"><code>MLTuneParameters$new()</code></a>
</p>
</li>
<li> <p><a href="#method-MLTuneParameters-execute"><code>MLTuneParameters$execute()</code></a>
</p>
</li>
<li> <p><a href="#method-MLTuneParameters-clone"><code>MLTuneParameters$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlexperiments" data-topic="MLExperimentsBase" data-id="set_data"><a href="../../mlexperiments/html/MLExperimentsBase.html#method-MLExperimentsBase-set_data"><code>mlexperiments::MLExperimentsBase$set_data()</code></a></span></li>
</ul></details><hr>
<a id="method-MLTuneParameters-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new <code>MLTuneParameters</code> object.
</p>


<h5>Usage</h5>

<div class="r"><pre>MLTuneParameters$new(
  learner,
  seed,
  strategy = c("grid", "bayesian"),
  ncores = -1L
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learner</code></dt>
<dd>
<p>An initialized learner object that inherits from class
<code>"MLLearnerBase"</code>.</p>
</dd>
<dt><code>seed</code></dt>
<dd>
<p>An integer. Needs to be set for reproducibility purposes.</p>
</dd>
<dt><code>strategy</code></dt>
<dd>
<p>A character. The strategy to optimize the hyperparameters
(either <code>"grid"</code> or <code>"bayesian"</code>).</p>
</dd>
<dt><code>ncores</code></dt>
<dd>
<p>An integer to specify the number of cores used for
parallelization (default: <code>-1L</code>).</p>
</dd>
</dl>
</div>



<h5>Details</h5>

<p>For <code>strategy = "bayesian"</code>, the number of starting iterations can be
set using the R option <code>"mlexperiments.bayesian.max_init"</code>, which
defaults to <code>50L</code>. This option reduces the provided initialization
grid to contain at most the specified number of rows. This
initialization grid is then further passed on to the <code>initGrid</code>
argument of ParBayesianOptimization::bayesOpt.
</p>



<h5>Returns</h5>

<p>A new <code>MLTuneParameters</code> R6 object.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>MLTuneParameters$new(
  learner = LearnerKnn$new(),
  seed = 123,
  strategy = "grid",
  ncores = 2
)

</pre>
</div>


<hr>
<a id="method-MLTuneParameters-execute"></a>



<h4>Method <code>execute()</code>
</h4>

<p>Execute the hyperparameter tuning.
</p>


<h5>Usage</h5>

<div class="r"><pre>MLTuneParameters$execute(k)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>k</code></dt>
<dd>
<p>An integer to define the number of cross-validation folds used
to tune the hyperparameters.</p>
</dd>
</dl>
</div>



<h5>Details</h5>

<p>All results of the hyperparameter tuning are saved in the field
<code style="white-space: pre;">⁠$results⁠</code> of the <code>MLTuneParameters</code> class. After successful execution
of the parameter tuning, <code style="white-space: pre;">⁠$results⁠</code> contains a list with the items
</p>

<dl>
<dt>"summary"</dt>
<dd>
<p>A data.table with the summarized results (same as
the returned value of the <code>execute</code> method).</p>
</dd>
<dt>"best.setting"</dt>
<dd>
<p>The best setting (according to the learner's
parameter <code>metric_optimization_higher_better</code>) identified during the
hyperparameter tuning.</p>
</dd>
<dt>"bayesOpt"</dt>
<dd>
<p>The returned value of
<code>ParBayesianOptimization::bayesOpt()</code> (only for <code>strategy = "bayesian"</code>).</p>
</dd>
</dl>
<h5>Returns</h5>

<p>A <code>data.table</code> with the results of the hyperparameter
optimization. The optimized metric, i.e. the cross-validated evaluation
metric is given in the column <code>metric_optim_mean</code>. More results are
accessible from the field <code style="white-space: pre;">⁠$results⁠</code> of the <code>MLTuneParameters</code> class.
</p>



<h5>Examples</h5>

<div class="r example copy">
<pre>dataset &lt;- do.call(
  cbind,
  c(sapply(paste0("col", 1:6), function(x) {
    rnorm(n = 500)
    },
    USE.NAMES = TRUE,
    simplify = FALSE
   ),
   list(target = sample(0:1, 500, TRUE))
))
tuner &lt;- MLTuneParameters$new(
  learner = LearnerKnn$new(),
  seed = 123,
  strategy = "grid",
  ncores = 2
)
tuner$parameter_bounds &lt;- list(k = c(2L, 80L))
tuner$parameter_grid &lt;- expand.grid(
  k = seq(4, 68, 8),
  l = 0,
  test = parse(text = "fold_test$x")
)
tuner$split_type &lt;- "stratified"
tuner$optim_args &lt;- list(
  iters.n = 4,
  kappa = 3.5,
  acq = "ucb"
)

# set data
tuner$set_data(
  x = data.matrix(dataset[, -7]),
  y = dataset[, 7]
)

tuner$execute(k = 3)

</pre>
</div>


<hr>
<a id="method-MLTuneParameters-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>MLTuneParameters$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>See Also</h3>

<p><code>ParBayesianOptimization::bayesOpt()</code>, <code>splitTools::create_folds()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">knn_tuner &lt;- MLTuneParameters$new(
  learner = LearnerKnn$new(),
  seed = 123,
  strategy = "grid",
  ncores = 2
)


## ------------------------------------------------
## Method `MLTuneParameters$new`
## ------------------------------------------------

MLTuneParameters$new(
  learner = LearnerKnn$new(),
  seed = 123,
  strategy = "grid",
  ncores = 2
)


## ------------------------------------------------
## Method `MLTuneParameters$execute`
## ------------------------------------------------

dataset &lt;- do.call(
  cbind,
  c(sapply(paste0("col", 1:6), function(x) {
    rnorm(n = 500)
    },
    USE.NAMES = TRUE,
    simplify = FALSE
   ),
   list(target = sample(0:1, 500, TRUE))
))
tuner &lt;- MLTuneParameters$new(
  learner = LearnerKnn$new(),
  seed = 123,
  strategy = "grid",
  ncores = 2
)
tuner$parameter_bounds &lt;- list(k = c(2L, 80L))
tuner$parameter_grid &lt;- expand.grid(
  k = seq(4, 68, 8),
  l = 0,
  test = parse(text = "fold_test$x")
)
tuner$split_type &lt;- "stratified"
tuner$optim_args &lt;- list(
  iters.n = 4,
  kappa = 3.5,
  acq = "ucb"
)

# set data
tuner$set_data(
  x = data.matrix(dataset[, -7]),
  y = dataset[, 7]
)

tuner$execute(k = 3)

</code></pre>


</div>