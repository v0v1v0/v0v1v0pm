<div class="container">

<table style="width: 100%;"><tr>
<td>HotstartStack</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Stack for Hot Start Learners</h2>

<h3>Description</h3>

<p>This class stores learners for hot starting training, i.e. resuming or
continuing from an already fitted model.
We assume that hot starting is only possible if a single hyperparameter
(also called the fidelity parameter, usually controlling the complexity or
expensiveness) is altered and all other hyperparameters are identical.
</p>
<p>The <code>HotstartStack</code> stores trained learners which can be potentially used to
hot start a learner. Learner automatically hot start while training if a
stack is attached to the <code style="white-space: pre;">⁠$hotstart_stack⁠</code> field and the stack contains a
suitable learner.
</p>
<p>For example, if you want to train a random forest learner with 1000 trees but
already have a random forest learner with 500 trees (hot start learner),
you can add the hot start learner to the <code>HotstartStack</code> of the expensive learner
with 1000 trees. If you now call the <code>train()</code> method (or <code>resample()</code> or
<code>benchmark()</code>), a random forest with 500 trees will be fitted and combined
with the 500 trees of the hotstart learner, effectively saving you to
fit 500 trees.
</p>
<p>Hot starting is only supported by learners which have the property
<code>"hotstart_forward"</code> or <code>"hotstart_backward"</code>. For example, an <code>xgboost</code> model
(in <a href="https://CRAN.R-project.org/package=mlr3learners"><span class="pkg">mlr3learners</span></a>) can hot start forward by adding more boosting
iterations, and a random forest can go backwards by removing trees.
The fidelity parameters are tagged with <code>"hotstart"</code> in learner's parameter set.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>stack</code></dt>
<dd>
<p><code>data.table::data.table()</code><br>
Stores hot start learners.</p>
</dd>
<dt><code>hotstart_threshold</code></dt>
<dd>
<p>(named <code>numeric(1)</code>)<br>
Threshold for storing learners in the stack.
If the value of the hotstart parameter is below this threshold, the learner is not added to the stack.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-HotstartStack-new"><code>HotstartStack$new()</code></a>
</p>
</li>
<li> <p><a href="#method-HotstartStack-add"><code>HotstartStack$add()</code></a>
</p>
</li>
<li> <p><a href="#method-HotstartStack-start_cost"><code>HotstartStack$start_cost()</code></a>
</p>
</li>
<li> <p><a href="#method-HotstartStack-format"><code>HotstartStack$format()</code></a>
</p>
</li>
<li> <p><a href="#method-HotstartStack-print"><code>HotstartStack$print()</code></a>
</p>
</li>
<li> <p><a href="#method-HotstartStack-clone"><code>HotstartStack$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-HotstartStack-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>HotstartStack$new(learners = NULL, hotstart_threshold = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learners</code></dt>
<dd>
<p>(List of Learners)<br>
Learners are added to the hotstart stack. If <code>NULL</code> (default), empty
stack is created.</p>
</dd>
<dt><code>hotstart_threshold</code></dt>
<dd>
<p>(named <code>numeric(1)</code>)<br>
Threshold for storing learners in the stack.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-HotstartStack-add"></a>



<h4>Method <code>add()</code>
</h4>

<p>Add learners to hot start stack.
</p>


<h5>Usage</h5>

<div class="r"><pre>HotstartStack$add(learners)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learners</code></dt>
<dd>
<p>(List of Learners).
Learners are added to the hotstart stack.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>self (invisibly).
</p>


<hr>
<a id="method-HotstartStack-start_cost"></a>



<h4>Method <code>start_cost()</code>
</h4>

<p>Calculates the cost for each learner of the stack to hot start the target <code>learner</code>.
</p>
<p>The following cost values can be returned:
</p>

<ul>
<li> <p><code>NA_real_</code>: Learner is unsuitable to hot start target <code>learner</code>.
</p>
</li>
<li> <p><code>-1</code>: Hotstart learner in the stack and target <code>learner</code> are identical.
</p>
</li>
<li> <p><code>0</code> Cost for hot starting backwards is always 0.
</p>
</li>
<li> <p><code style="white-space: pre;">⁠&gt; 0⁠</code> Cost for hot starting forward.
</p>
</li>
</ul>
<h5>Usage</h5>

<div class="r"><pre>HotstartStack$start_cost(learner, task_hash)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>learner</code></dt>
<dd>
<p>Learner<br>
Target learner.</p>
</dd>
<dt><code>task_hash</code></dt>
<dd>
<p>Task<br>
Hash of the task on which the target learner is trained.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-HotstartStack-format"></a>



<h4>Method <code>format()</code>
</h4>

<p>Helper for print outputs.
</p>


<h5>Usage</h5>

<div class="r"><pre>HotstartStack$format(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(ignored).</p>
</dd>
</dl>
</div>


<hr>
<a id="method-HotstartStack-print"></a>



<h4>Method <code>print()</code>
</h4>

<p>Printer.
</p>


<h5>Usage</h5>

<div class="r"><pre>HotstartStack$print(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(ignored).</p>
</dd>
</dl>
</div>


<hr>
<a id="method-HotstartStack-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>HotstartStack$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>Examples</h3>

<pre><code class="language-R"># train learner on pima task
task = tsk("pima")
learner = lrn("classif.debug", iter = 1)
learner$train(task)

# initialize stack with previously fitted learner
hot = HotstartStack$new(list(learner))

# retrieve learner with increased fidelity parameter
learner = lrn("classif.debug", iter = 2)

# calculate cost of hot starting
hot$start_cost(learner, task$hash)

# add stack with hot start learner
learner$hotstart_stack = hot

# train automatically uses hot start learner while fitting the model
learner$train(task)
</code></pre>


</div>