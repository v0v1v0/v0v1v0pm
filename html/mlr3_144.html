<div class="container">

<table style="width: 100%;"><tr>
<td>Measure</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Measure Class</h2>

<h3>Description</h3>

<p>This is the abstract base class for measures like MeasureClassif and MeasureRegr.
</p>
<p>Measures are classes tailored around two functions doing the work:
</p>

<ol>
<li>
<p> A function <code style="white-space: pre;">⁠$score()⁠</code> which quantifies the performance by comparing the truth and predictions.
</p>
</li>
<li>
<p> A function <code style="white-space: pre;">⁠$aggregator()⁠</code> which combines multiple performance scores returned by
<code style="white-space: pre;">⁠$score()⁠</code> to a single numeric value.
</p>
</li>
</ol>
<p>In addition to these two functions, meta-information about the performance measure is stored.
</p>
<p>Predefined measures are stored in the dictionary mlr_measures,
e.g. <code>classif.auc</code> or <code>time_train</code>.
Many of the measures in <span class="pkg">mlr3</span> are implemented in <a href="https://CRAN.R-project.org/package=mlr3measures"><span class="pkg">mlr3measures</span></a> as ordinary functions.
</p>
<p>A guide on how to extend <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> with custom measures can be found in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>


<h3>Inheriting</h3>

<p>For some measures (such as confidence intervals from <code>mlr3inferr</code>) it is necessary that a measure
returns more than one value.
In such cases it is necessary to overwrite the public methods <code style="white-space: pre;">⁠$aggregate()⁠</code> and/or <code style="white-space: pre;">⁠$score()⁠</code> to return a named <code>numeric()</code>
where at least one of its names corresponds to the <code>id</code> of the measure itself.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>id</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Identifier of the object.
Used in tables, plot and text output.</p>
</dd>
<dt><code>label</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>task_type</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Task type, e.g. <code>"classif"</code> or <code>"regr"</code>.
</p>
<p>For a complete list of possible task types (depending on the loaded packages),
see <code>mlr_reflections$task_types$type</code>.</p>
</dd>
<dt><code>param_set</code></dt>
<dd>
<p>(paradox::ParamSet)<br>
Set of hyperparameters.</p>
</dd>
<dt><code>obs_loss</code></dt>
<dd>
<p>(<code style="white-space: pre;">⁠function()⁠</code> | <code>NULL</code>)
Function to calculate the observation-wise loss.</p>
</dd>
<dt><code>trafo</code></dt>
<dd>
<p>(<code>list()</code> | <code>NULL</code>)
<code>NULL</code> or a list with two elements:
</p>

<ul>
<li> <p><code>trafo</code>: the transformation function applied after aggregating
observation-wise losses (e.g. <code>sqrt</code> for RMSE)
</p>
</li>
<li> <p><code>deriv</code>: The derivative of the <code>trafo</code>.
</p>
</li>
</ul>
</dd>
<dt><code>predict_type</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Required predict type of the Learner.</p>
</dd>
<dt><code>check_prerequisites</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
How to proceed if one of the following prerequisites is not met:
</p>

<ul>
<li>
<p> wrong predict type (e.g., probabilities required, but only labels available).
</p>
</li>
<li>
<p> wrong predict set (e.g., learner predicted on training set, but predictions of test set required).
</p>
</li>
<li>
<p> task properties not satisfied (e.g., binary classification measure on multiclass task).
</p>
</li>
</ul>
<p>Possible values are <code>"ignore"</code> (just return <code>NaN</code>) and <code>"warn"</code> (default, raise a warning before returning <code>NaN</code>).</p>
</dd>
<dt><code>task_properties</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Required properties of the Task.</p>
</dd>
<dt><code>range</code></dt>
<dd>
<p>(<code>numeric(2)</code>)<br>
Lower and upper bound of possible performance scores.</p>
</dd>
<dt><code>properties</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Properties of this measure.</p>
</dd>
<dt><code>minimize</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
If <code>TRUE</code>, good predictions correspond to small values of performance scores.</p>
</dd>
<dt><code>packages</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Set of required packages.
These packages are loaded, but not attached.</p>
</dd>
<dt><code>man</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
String in the format <code style="white-space: pre;">⁠[pkg]::[topic]⁠</code> pointing to a manual page for this object.
Defaults to <code>NA</code>, but can be set by child classes.</p>
</dd>
</dl>
</div>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>predict_sets</code></dt>
<dd>
<p>(<code>character()</code>)<br>
During <code>resample()</code>/<code>benchmark()</code>, a Learner can predict on multiple sets.
Per default, a learner only predicts observations in the test set (<code>predict_sets == "test"</code>).
To change this behavior, set <code>predict_sets</code> to a non-empty subset of <code style="white-space: pre;">⁠{"train", "test", "internal_valid"}⁠</code>.
The <code>"train"</code> predict set contains the train ids from the resampling. This means that if a learner does validation and
sets <code style="white-space: pre;">⁠$validate⁠</code> to a ratio (creating the validation data from the training data), the train predictions
will include the predictions for the validation data.
Each set yields a separate Prediction object.
Those can be combined via getters in ResampleResult/BenchmarkResult, or Measures can be configured
to operate on specific subsets of the calculated prediction sets.</p>
</dd>
<dt><code>hash</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Hash (unique identifier) for this object.</p>
</dd>
<dt><code>average</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Method for aggregation:
</p>

<ul>
<li> <p><code>"micro"</code>:
All predictions from multiple resampling iterations are first combined into a single Prediction object.
Next, the scoring function of the measure is applied on this combined object, yielding a single numeric score.
</p>
</li>
<li> <p><code>"macro"</code>:
The scoring function is applied on the Prediction object of each resampling iterations,
each yielding a single numeric score.
Next, the scores are combined with the <code>aggregator</code> function to a single numerical score.
</p>
</li>
<li> <p><code>"custom"</code>:
The measure comes with a custom aggregation method which directly operates on a ResampleResult.
</p>
</li>
</ul>
</dd>
<dt><code>aggregator</code></dt>
<dd>
<p>(<code style="white-space: pre;">⁠function()⁠</code>)<br>
Function to aggregate scores computed on different resampling iterations.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-Measure-new"><code>Measure$new()</code></a>
</p>
</li>
<li> <p><a href="#method-Measure-format"><code>Measure$format()</code></a>
</p>
</li>
<li> <p><a href="#method-Measure-print"><code>Measure$print()</code></a>
</p>
</li>
<li> <p><a href="#method-Measure-help"><code>Measure$help()</code></a>
</p>
</li>
<li> <p><a href="#method-Measure-score"><code>Measure$score()</code></a>
</p>
</li>
<li> <p><a href="#method-Measure-aggregate"><code>Measure$aggregate()</code></a>
</p>
</li>
<li> <p><a href="#method-Measure-clone"><code>Measure$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-Measure-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>
<p>Note that this object is typically constructed via a derived classes, e.g. MeasureClassif or MeasureRegr.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$new(
  id,
  task_type = NA,
  param_set = ps(),
  range = c(-Inf, Inf),
  minimize = NA,
  average = "macro",
  aggregator = NULL,
  obs_loss = NULL,
  properties = character(),
  predict_type = "response",
  predict_sets = "test",
  task_properties = character(),
  packages = character(),
  label = NA_character_,
  man = NA_character_,
  trafo = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Identifier for the new instance.</p>
</dd>
<dt><code>task_type</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Type of task, e.g. <code>"regr"</code> or <code>"classif"</code>.
Must be an element of mlr_reflections$task_types$type.</p>
</dd>
<dt><code>param_set</code></dt>
<dd>
<p>(paradox::ParamSet)<br>
Set of hyperparameters.</p>
</dd>
<dt><code>range</code></dt>
<dd>
<p>(<code>numeric(2)</code>)<br>
Feasible range for this measure as <code>c(lower_bound, upper_bound)</code>.
Both bounds may be infinite.</p>
</dd>
<dt><code>minimize</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Set to <code>TRUE</code> if good predictions correspond to small values,
and to <code>FALSE</code> if good predictions correspond to large values.
If set to <code>NA</code> (default), tuning this measure is not possible.</p>
</dd>
<dt><code>average</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
How to average multiple Predictions from a ResampleResult.
</p>
<p>The default, <code>"macro"</code>, calculates the individual performances scores for each Prediction and then uses the
function defined in <code style="white-space: pre;">⁠$aggregator⁠</code> to average them to a single number.
</p>
<p>If set to <code>"micro"</code>, the individual Prediction objects are first combined into a single new Prediction object which is then used to assess the performance.
The function in <code style="white-space: pre;">⁠$aggregator⁠</code> is not used in this case.</p>
</dd>
<dt><code>aggregator</code></dt>
<dd>
<p>(<code style="white-space: pre;">⁠function()⁠</code>)<br>
Function to aggregate over multiple iterations. The role of this function depends on
the value of field <code>"average"</code>:
</p>

<ul>
<li> <p><code>"macro"</code>: A numeric vector of scores (one per iteration) is passed.
The aggregate function defaults to <code>mean()</code> in this case.
</p>
</li>
<li> <p><code>"micro"</code>: The <code>aggregator</code> function is not used.
Instead, predictions from multiple iterations are first combined and then
scored in one go.
</p>
</li>
<li> <p><code>"custom"</code>: A ResampleResult is passed to the aggregate function.
</p>
</li>
</ul>
</dd>
<dt><code>obs_loss</code></dt>
<dd>
<p>(<code>function</code> or <code>NULL</code>)<br>
The observation-wise loss function, e.g. zero-one for classification error.</p>
</dd>
<dt><code>properties</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Properties of the measure.
Must be a subset of mlr_reflections$measure_properties.
Supported by <code>mlr3</code>:
</p>

<ul>
<li> <p><code>"requires_task"</code> (requires the complete Task),
</p>
</li>
<li> <p><code>"requires_learner"</code> (requires the trained Learner),
</p>
</li>
<li> <p><code>"requires_model"</code> (requires the trained Learner, including the fitted
model),
</p>
</li>
<li> <p><code>"requires_train_set"</code> (requires the training indices from the Resampling), and
</p>
</li>
<li> <p><code>"na_score"</code> (the measure is expected to occasionally return <code>NA</code> or <code>NaN</code>).
</p>
</li>
<li> <p><code>"primary_iters"</code> (the measure explictly handles resamplings that only use a subset
of their iterations for the point estimate).
</p>
</li>
<li> <p><code>"requires_no_prediction"</code> (No prediction is required; This usually means that the
measure extracts some information from the learner state.).
</p>
</li>
</ul>
</dd>
<dt><code>predict_type</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Required predict type of the Learner.
Possible values are stored in mlr_reflections$learner_predict_types.</p>
</dd>
<dt><code>predict_sets</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Prediction sets to operate on, used in <code>aggregate()</code> to extract the matching <code>predict_sets</code> from the ResampleResult.
Multiple predict sets are calculated by the respective Learner during <code>resample()</code>/<code>benchmark()</code>.
Must be a non-empty subset of <code style="white-space: pre;">⁠{"train", "test", "internal_valid"}⁠</code>.
If multiple sets are provided, these are first combined to a single prediction object.
Default is <code>"test"</code>.</p>
</dd>
<dt><code>task_properties</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Required task properties, see Task.</p>
</dd>
<dt><code>packages</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Set of required packages.
A warning is signaled by the constructor if at least one of the packages is not installed,
but loaded (not attached) later on-demand via <code>requireNamespace()</code>.</p>
</dd>
<dt><code>label</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Label for the new instance.</p>
</dd>
<dt><code>man</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
String in the format <code style="white-space: pre;">⁠[pkg]::[topic]⁠</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">⁠$help()⁠</code>.</p>
</dd>
<dt><code>trafo</code></dt>
<dd>
<p>(<code>list()</code> or <code>NULL</code>)<br>
An optional list with two elements, containing the transformation <code>"fn"</code> and its derivative <code>"deriv"</code>.
The transformation function is the function that is applied after aggregating the pointwise losses, i.e.
this requires an <code style="white-space: pre;">⁠$obs_loss⁠</code> to be present. An example is <code>sqrt</code> for RMSE.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-Measure-format"></a>



<h4>Method <code>format()</code>
</h4>

<p>Helper for print outputs.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$format(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(ignored).</p>
</dd>
</dl>
</div>


<hr>
<a id="method-Measure-print"></a>



<h4>Method <code>print()</code>
</h4>

<p>Printer.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$print(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(ignored).</p>
</dd>
</dl>
</div>


<hr>
<a id="method-Measure-help"></a>



<h4>Method <code>help()</code>
</h4>

<p>Opens the corresponding help page referenced by field <code style="white-space: pre;">⁠$man⁠</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$help()</pre></div>


<hr>
<a id="method-Measure-score"></a>



<h4>Method <code>score()</code>
</h4>

<p>Takes a Prediction (or a list of Prediction objects named with valid <code>predict_sets</code>)
and calculates a numeric score.
If the measure if flagged with the properties <code>"requires_task"</code>, <code>"requires_learner"</code>,
<code>"requires_model"</code> or <code>"requires_train_set"</code>, you must additionally
pass the respective Task, the (trained) Learner or the training set indices.
This is handled internally during <code>resample()</code>/<code>benchmark()</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$score(prediction, task = NULL, learner = NULL, train_set = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>prediction</code></dt>
<dd>
<p>(Prediction | named list of Prediction).</p>
</dd>
<dt><code>task</code></dt>
<dd>
<p>(Task).</p>
</dd>
<dt><code>learner</code></dt>
<dd>
<p>(Learner).</p>
</dd>
<dt><code>train_set</code></dt>
<dd>
<p>(<code>integer()</code>).</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>numeric(1)</code>.
</p>


<hr>
<a id="method-Measure-aggregate"></a>



<h4>Method <code>aggregate()</code>
</h4>

<p>Aggregates multiple performance scores into a single score, e.g. by using the <code>aggregator</code>
function of the measure.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$aggregate(rr)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>rr</code></dt>
<dd>
<p>ResampleResult.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>numeric(1)</code>.
</p>


<hr>
<a id="method-Measure-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>Measure$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>See Also</h3>


<ul>
<li>
<p> Chapter in the <a href="https://mlr3book.mlr-org.com/">mlr3book</a>:
<a href="https://mlr3book.mlr-org.com/chapters/chapter2/data_and_basic_modeling.html#sec-eval">https://mlr3book.mlr-org.com/chapters/chapter2/data_and_basic_modeling.html#sec-eval</a>
</p>
</li>
<li>
<p> Package <a href="https://CRAN.R-project.org/package=mlr3measures"><span class="pkg">mlr3measures</span></a> for the scoring functions.
Dictionary of Measures: mlr_measures
<code>as.data.table(mlr_measures)</code> for a table of available Measures in the running session (depending on the loaded packages).
</p>
</li>
<li>
<p> Extension packages for additional task types:
</p>

<ul>
<li> <p><a href="https://CRAN.R-project.org/package=mlr3proba"><span class="pkg">mlr3proba</span></a> for probabilistic supervised regression and survival analysis.
</p>
</li>
<li> <p><a href="https://CRAN.R-project.org/package=mlr3cluster"><span class="pkg">mlr3cluster</span></a> for unsupervised clustering.
</p>
</li>
</ul>
</li>
</ul>
<p>Other Measure: 
<code>MeasureClassif</code>,
<code>MeasureRegr</code>,
<code>MeasureSimilarity</code>,
<code>mlr_measures</code>,
<code>mlr_measures_aic</code>,
<code>mlr_measures_bic</code>,
<code>mlr_measures_classif.costs</code>,
<code>mlr_measures_debug_classif</code>,
<code>mlr_measures_elapsed_time</code>,
<code>mlr_measures_internal_valid_score</code>,
<code>mlr_measures_oob_error</code>,
<code>mlr_measures_regr.rsq</code>,
<code>mlr_measures_selected_features</code>
</p>


</div>