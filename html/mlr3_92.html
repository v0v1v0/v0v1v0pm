<div class="container">

<table style="width: 100%;"><tr>
<td>benchmark</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Benchmark Multiple Learners on Multiple Tasks</h2>

<h3>Description</h3>

<p>Runs a benchmark on arbitrary combinations of tasks (Task), learners (Learner), and resampling strategies (Resampling), possibly in parallel.
</p>
<p>For large-scale benchmarking we recommend to use the <a href="https://CRAN.R-project.org/package=mlr3batchmark"><span class="pkg">mlr3batchmark</span></a> package.
This package runs benchmark experiments on high-performance computing clusters and handles failed experiments.
</p>


<h3>Usage</h3>

<pre><code class="language-R">benchmark(
  design,
  store_models = FALSE,
  store_backends = TRUE,
  encapsulate = NA_character_,
  allow_hotstart = FALSE,
  clone = c("task", "learner", "resampling"),
  unmarshal = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>design</code></td>
<td>
<p>(<code>data.frame()</code>)<br>
Data frame (or <code>data.table::data.table()</code>) with three columns: "task", "learner", and "resampling".
Each row defines a resampling by providing a Task, Learner and an instantiated Resampling strategy.
The helper function <code>benchmark_grid()</code> can assist in generating an exhaustive design (see examples) and
instantiate the Resamplings per Task.
Additionally, you can set the additional column 'param_values', see <code>benchmark_grid()</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>store_models</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Store the fitted model in the resulting object=
Set to <code>TRUE</code> if you want to further analyse the models or want to
extract information like variable importance.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>store_backends</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Keep the DataBackend of the Task in the ResampleResult?
Set to <code>TRUE</code> if your performance measures require a Task,
or to analyse results more conveniently.
Set to <code>FALSE</code> to reduce the file size and memory footprint
after serialization.
The current default is <code>TRUE</code>, but this eventually will be changed
in a future release.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>encapsulate</code></td>
<td>
<p>(<code>character(1)</code>)<br>
If not <code>NA</code>, enables encapsulation by setting the field
<code>Learner$encapsulate</code> to one of the supported values:
<code>"none"</code> (disable encapsulation),
<code>"try"</code> (captures errors but output is printed to the console and not logged),
<code>"evaluate"</code> (execute via <a href="https://CRAN.R-project.org/package=evaluate"><span class="pkg">evaluate</span></a>) and
<code>"callr"</code> (start in external session via <a href="https://CRAN.R-project.org/package=callr"><span class="pkg">callr</span></a>).
If <code>NA</code>, encapsulation is not changed, i.e. the settings of the
individual learner are active.
Additionally, if encapsulation is set to <code>"evaluate"</code> or <code>"callr"</code>,
the fallback learner is set to the featureless learner if the learner
does not already have a fallback configured.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>allow_hotstart</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Determines if learner(s) are hot started with trained models in
<code style="white-space: pre;">⁠$hotstart_stack⁠</code>. See also HotstartStack.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>clone</code></td>
<td>
<p>(<code>character()</code>)<br>
Select the input objects to be cloned before proceeding by
providing a set with possible values <code>"task"</code>, <code>"learner"</code> and
<code>"resampling"</code> for Task, Learner and Resampling, respectively.
Per default, all input objects are cloned.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>unmarshal</code></td>
<td>
<p><code>Learner</code><br>
Whether to unmarshal learners that were marshaled during the execution.
If <code>TRUE</code> all models are stored in unmarshaled form.
If <code>FALSE</code>, all learners (that need marshaling) are stored in marshaled form.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>BenchmarkResult.
</p>


<h3>Predict Sets</h3>

<p>If you want to compare the performance of a learner on the training with the performance
on the test set, you have to configure the Learner to predict on multiple sets by
setting the field <code>predict_sets</code> to <code>c("train", "test")</code> (default is <code>"test"</code>).
Each set yields a separate Prediction object during resampling.
In the next step, you have to configure the measures to operate on the respective Prediction object:
</p>
<div class="sourceCode"><pre>m1 = msr("classif.ce", id = "ce.train", predict_sets = "train")
m2 = msr("classif.ce", id = "ce.test", predict_sets = "test")
</pre></div>
<p>The (list of) created measures can finally be passed to <code style="white-space: pre;">⁠$aggregate()⁠</code> or <code style="white-space: pre;">⁠$score()⁠</code>.
</p>


<h3>Parallelization</h3>

<p>This function can be parallelized with the <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> package.
One job is one resampling iteration, and all jobs are send to an apply function
from <a href="https://CRAN.R-project.org/package=future.apply"><span class="pkg">future.apply</span></a> in a single batch.
To select a parallel backend, use <code>future::plan()</code>.
More on parallelization can be found in the book:
<a href="https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html">https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html</a>
</p>


<h3>Progress Bars</h3>

<p>This function supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>.
Simply wrap the function call in <code>progressr::with_progress()</code> to enable them.
Alternatively, call <code>progressr::handlers()</code> with <code>global = TRUE</code> to enable progress bars
globally.
We recommend the <a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> package as backend which can be enabled with
<code>progressr::handlers("progress")</code>.
</p>


<h3>Logging</h3>

<p>The <a href="https://CRAN.R-project.org/package=mlr3"><span class="pkg">mlr3</span></a> uses the <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a> package for logging.
<a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a> supports multiple log levels which can be queried with
<code>getOption("lgr.log_levels")</code>.
</p>
<p>To suppress output and reduce verbosity, you can lower the log from the
default level <code>"info"</code> to <code>"warn"</code>:
</p>
<div class="sourceCode"><pre>lgr::get_logger("mlr3")$set_threshold("warn")
</pre></div>
<p>To get additional log output for debugging, increase the log level to <code>"debug"</code>
or <code>"trace"</code>:
</p>
<div class="sourceCode"><pre>lgr::get_logger("mlr3")$set_threshold("debug")
</pre></div>
<p>To log to a file or a data base, see the documentation of lgr::lgr-package.
</p>


<h3>Note</h3>

<p>The fitted models are discarded after the predictions have been scored in order to reduce memory consumption.
If you need access to the models for later analysis, set <code>store_models</code> to <code>TRUE</code>.
</p>


<h3>See Also</h3>


<ul>
<li>
<p> Chapter in the <a href="https://mlr3book.mlr-org.com/">mlr3book</a>:
<a href="https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking">https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking</a>
</p>
</li>
<li>
<p> Package <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> for some generic visualizations.
</p>
</li>
<li> <p><a href="https://CRAN.R-project.org/package=mlr3benchmark"><span class="pkg">mlr3benchmark</span></a> for post-hoc analysis of benchmark results.
</p>
</li>
</ul>
<p>Other benchmark: 
<code>BenchmarkResult</code>,
<code>benchmark_grid()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># benchmarking with benchmark_grid()
tasks = lapply(c("penguins", "sonar"), tsk)
learners = lapply(c("classif.featureless", "classif.rpart"), lrn)
resamplings = rsmp("cv", folds = 3)

design = benchmark_grid(tasks, learners, resamplings)
print(design)

set.seed(123)
bmr = benchmark(design)

## Data of all resamplings
head(as.data.table(bmr))

## Aggregated performance values
aggr = bmr$aggregate()
print(aggr)

## Extract predictions of first resampling result
rr = aggr$resample_result[[1]]
as.data.table(rr$prediction())

# Benchmarking with a custom design:
# - fit classif.featureless on penguins with a 3-fold CV
# - fit classif.rpart on sonar using a holdout
tasks = list(tsk("penguins"), tsk("sonar"))
learners = list(lrn("classif.featureless"), lrn("classif.rpart"))
resamplings = list(rsmp("cv", folds = 3), rsmp("holdout"))

design = data.table::data.table(
  task = tasks,
  learner = learners,
  resampling = resamplings
)

## Instantiate resamplings
design$resampling = Map(
  function(task, resampling) resampling$clone()$instantiate(task),
  task = design$task, resampling = design$resampling
)

## Run benchmark
bmr = benchmark(design)
print(bmr)

## Get the training set of the 2nd iteration of the featureless learner on penguins
rr = bmr$aggregate()[learner_id == "classif.featureless"]$resample_result[[1]]
rr$resampling$train_set(2)
</code></pre>


</div>