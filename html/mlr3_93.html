<div class="container">

<table style="width: 100%;"><tr>
<td>BenchmarkResult</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Container for Benchmarking Results</h2>

<h3>Description</h3>

<p>This is the result container object returned by <code>benchmark()</code>.
A BenchmarkResult consists of the data of multiple ResampleResults.
The contents of a <code>BenchmarkResult</code> and ResampleResult are almost identical and the stored ResampleResults can be extracted via the <code style="white-space: pre;">⁠$resample_result(i)⁠</code> method, where i is the index of the performed resample experiment.
This allows us to investigate the extracted ResampleResult and individual resampling iterations, as well as the predictions and models from each fold.
</p>
<p>BenchmarkResults can be visualized via <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a>'s <code>autoplot()</code> function.
</p>
<p>For statistical analysis of benchmark results and more advanced plots, see <a href="https://CRAN.R-project.org/package=mlr3benchmark"><span class="pkg">mlr3benchmark</span></a>.
</p>


<h3>S3 Methods</h3>


<ul>
<li> <p><code>as.data.table(rr, ..., reassemble_learners = TRUE, convert_predictions = TRUE, predict_sets = "test")</code><br>
BenchmarkResult -&gt; <code>data.table::data.table()</code><br>
Returns a tabular view of the internal data.
</p>
</li>
<li> <p><code>c(...)</code><br>
(BenchmarkResult, ...) -&gt; BenchmarkResult<br>
Combines multiple objects convertible to BenchmarkResult into a new BenchmarkResult.
</p>
</li>
</ul>
<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>task_type</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Task type of objects in the <code>BenchmarkResult</code>.
All stored objects (Task, Learner, Prediction) in a single <code>BenchmarkResult</code> are
required to have the same task type, e.g., <code>"classif"</code> or <code>"regr"</code>.
This is <code>NA</code> for empty BenchmarkResults.</p>
</dd>
<dt><code>tasks</code></dt>
<dd>
<p>(<code>data.table::data.table()</code>)<br>
Table of included Tasks with three columns:
</p>

<ul>
<li> <p><code>"task_hash"</code> (<code>character(1)</code>),
</p>
</li>
<li> <p><code>"task_id"</code> (<code>character(1)</code>), and
</p>
</li>
<li> <p><code>"task"</code> (Task).
</p>
</li>
</ul>
</dd>
<dt><code>learners</code></dt>
<dd>
<p>(<code>data.table::data.table()</code>)<br>
Table of included Learners with three columns:
</p>

<ul>
<li> <p><code>"learner_hash"</code> (<code>character(1)</code>),
</p>
</li>
<li> <p><code>"learner_id"</code> (<code>character(1)</code>), and
</p>
</li>
<li> <p><code>"learner"</code> (Learner).
</p>
</li>
</ul>
<p>Note that it is not feasible to access learned models via this field, as the training task would be ambiguous.
For this reason the returned learner are reset before they are returned.
Instead, select a row from the table returned by <code style="white-space: pre;">⁠$score()⁠</code>.</p>
</dd>
<dt><code>resamplings</code></dt>
<dd>
<p>(<code>data.table::data.table()</code>)<br>
Table of included Resamplings with three columns:
</p>

<ul>
<li> <p><code>"resampling_hash"</code> (<code>character(1)</code>),
</p>
</li>
<li> <p><code>"resampling_id"</code> (<code>character(1)</code>), and
</p>
</li>
<li> <p><code>"resampling"</code> (Resampling).
</p>
</li>
</ul>
</dd>
<dt><code>resample_results</code></dt>
<dd>
<p>(<code>data.table::data.table()</code>)<br>
Returns a table with three columns:
</p>

<ul>
<li> <p><code>uhash</code> (<code>character()</code>).
</p>
</li>
<li> <p><code>resample_result</code> (ResampleResult).
</p>
</li>
</ul>
</dd>
<dt><code>n_resample_results</code></dt>
<dd>
<p>(<code>integer(1)</code>)<br>
Returns the total number of stored ResampleResults.</p>
</dd>
<dt><code>uhashes</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Set of (unique) hashes of all included ResampleResults.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-BenchmarkResult-new"><code>BenchmarkResult$new()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-help"><code>BenchmarkResult$help()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-format"><code>BenchmarkResult$format()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-print"><code>BenchmarkResult$print()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-combine"><code>BenchmarkResult$combine()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-marshal"><code>BenchmarkResult$marshal()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-unmarshal"><code>BenchmarkResult$unmarshal()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-score"><code>BenchmarkResult$score()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-obs_loss"><code>BenchmarkResult$obs_loss()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-aggregate"><code>BenchmarkResult$aggregate()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-filter"><code>BenchmarkResult$filter()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-resample_result"><code>BenchmarkResult$resample_result()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-discard"><code>BenchmarkResult$discard()</code></a>
</p>
</li>
<li> <p><a href="#method-BenchmarkResult-clone"><code>BenchmarkResult$clone()</code></a>
</p>
</li>
</ul>
<hr>
<a id="method-BenchmarkResult-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$new(data = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data</code></dt>
<dd>
<p>(<code>ResultData</code>)<br>
An object of type <code>ResultData</code>, either extracted from another ResampleResult, another
BenchmarkResult, or manually constructed with <code>as_result_data()</code>.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-BenchmarkResult-help"></a>



<h4>Method <code>help()</code>
</h4>

<p>Opens the help page for this object.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$help()</pre></div>


<hr>
<a id="method-BenchmarkResult-format"></a>



<h4>Method <code>format()</code>
</h4>

<p>Helper for print outputs.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$format(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(ignored).</p>
</dd>
</dl>
</div>


<hr>
<a id="method-BenchmarkResult-print"></a>



<h4>Method <code>print()</code>
</h4>

<p>Printer.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$print()</pre></div>


<hr>
<a id="method-BenchmarkResult-combine"></a>



<h4>Method <code>combine()</code>
</h4>

<p>Fuses a second BenchmarkResult into itself, mutating the BenchmarkResult in-place.
If the second BenchmarkResult <code>bmr</code> is <code>NULL</code>, simply returns <code>self</code>.
Note that you can alternatively use the combine function <code>c()</code> which calls this method internally.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$combine(bmr)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>bmr</code></dt>
<dd>
<p>(BenchmarkResult)<br>
A second BenchmarkResult object.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Returns the object itself, but modified <strong>by reference</strong>.
You need to explicitly <code style="white-space: pre;">⁠$clone()⁠</code> the object beforehand if you want to keep
the object in its previous state.
</p>


<hr>
<a id="method-BenchmarkResult-marshal"></a>



<h4>Method <code>marshal()</code>
</h4>

<p>Marshals all stored models.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$marshal(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(any)<br>
Additional arguments passed to <code>marshal_model()</code>.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-BenchmarkResult-unmarshal"></a>



<h4>Method <code>unmarshal()</code>
</h4>

<p>Unmarshals all stored models.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$unmarshal(...)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>...</code></dt>
<dd>
<p>(any)<br>
Additional arguments passed to <code>unmarshal_model()</code>.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-BenchmarkResult-score"></a>



<h4>Method <code>score()</code>
</h4>

<p>Returns a table with one row for each resampling iteration, including
all involved objects: Task, Learner, Resampling, iteration number
(<code>integer(1)</code>), and Prediction. If <code>ids</code> is set to <code>TRUE</code>, character
column of extracted ids are added to the table for convenient
filtering: <code>"task_id"</code>, <code>"learner_id"</code>, and <code>"resampling_id"</code>.
</p>
<p>Additionally calculates the provided performance measures and binds the
performance scores as extra columns. These columns are named using the id of
the respective Measure.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$score(
  measures = NULL,
  ids = TRUE,
  conditions = FALSE,
  predictions = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>measures</code></dt>
<dd>
<p>(Measure | list of Measure)<br>
Measure(s) to calculate.</p>
</dd>
<dt><code>ids</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Adds object ids (<code>"task_id"</code>, <code>"learner_id"</code>, <code>"resampling_id"</code>) as
extra character columns to the returned table.</p>
</dd>
<dt><code>conditions</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Adds condition messages (<code>"warnings"</code>, <code>"errors"</code>) as extra
list columns of character vectors to the returned table</p>
</dd>
<dt><code>predictions</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Additionally return prediction objects, one column for each <code>predict_set</code> of all learners combined.
Columns are named <code>"prediction_train"</code>, <code>"prediction_test"</code> and <code>"prediction_internal_valid"</code>,
if present.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>data.table::data.table()</code>.
</p>


<hr>
<a id="method-BenchmarkResult-obs_loss"></a>



<h4>Method <code>obs_loss()</code>
</h4>

<p>Calculates the observation-wise loss via the loss function set in the
Measure's field <code>obs_loss</code>.
Returns a <code>data.table()</code> with the columns <code>row_ids</code>, <code>truth</code>, <code>response</code> and
one additional numeric column for each measure, named with the respective measure id.
If there is no observation-wise loss function for the measure, the column is filled with
<code>NA</code> values.
Note that some measures such as RMSE, do have an <code style="white-space: pre;">⁠$obs_loss⁠</code>, but they require an
additional transformation after aggregation, in this example taking the square-root.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$obs_loss(measures = NULL, predict_sets = "test")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>measures</code></dt>
<dd>
<p>(Measure | list of Measure)<br>
Measure(s) to calculate.</p>
</dd>
<dt><code>predict_sets</code></dt>
<dd>
<p>(<code>character()</code>)<br>
The predict sets.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-BenchmarkResult-aggregate"></a>



<h4>Method <code>aggregate()</code>
</h4>

<p>Returns a result table where resampling iterations are combined into
ResampleResults. A column with the aggregated performance score is
added for each Measure, named with the id of the respective measure.
</p>
<p>The method for aggregation is controlled by the Measure, e.g. micro
aggregation, macro aggregation or custom aggregation. Most measures
default to macro aggregation.
</p>
<p>Note that the aggregated performances just give a quick impression which
approaches work well and which approaches are probably underperforming.
However, the aggregates do not account for variance and cannot replace
a statistical test.
See <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> to get a better impression via boxplots or
<a href="https://CRAN.R-project.org/package=mlr3benchmark"><span class="pkg">mlr3benchmark</span></a> for critical difference plots and
significance tests.
</p>
<p>For convenience, different flags can be set to extract more
information from the returned ResampleResult.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$aggregate(
  measures = NULL,
  ids = TRUE,
  uhashes = FALSE,
  params = FALSE,
  conditions = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>measures</code></dt>
<dd>
<p>(Measure | list of Measure)<br>
Measure(s) to calculate.</p>
</dd>
<dt><code>ids</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Adds object ids (<code>"task_id"</code>, <code>"learner_id"</code>, <code>"resampling_id"</code>) as
extra character columns for convenient subsetting.</p>
</dd>
<dt><code>uhashes</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Adds the uhash values of the ResampleResult as extra character
column <code>"uhash"</code>.</p>
</dd>
<dt><code>params</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Adds the hyperparameter values as extra list column <code>"params"</code>. You
can unnest them with <code>mlr3misc::unnest()</code>.</p>
</dd>
<dt><code>conditions</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Adds the number of resampling iterations with at least one warning as
extra integer column <code>"warnings"</code>, and the number of resampling
iterations with errors as extra integer column <code>"errors"</code>.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>data.table::data.table()</code>.
</p>


<hr>
<a id="method-BenchmarkResult-filter"></a>



<h4>Method <code>filter()</code>
</h4>

<p>Subsets the benchmark result. If <code>task_ids</code> is not <code>NULL</code>, keeps all
tasks with provided task ids and discards all others tasks.
Same procedure for <code>learner_ids</code> and <code>resampling_ids</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$filter(
  task_ids = NULL,
  task_hashes = NULL,
  learner_ids = NULL,
  learner_hashes = NULL,
  resampling_ids = NULL,
  resampling_hashes = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>task_ids</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Ids of Tasks to keep.</p>
</dd>
<dt><code>task_hashes</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Hashes of Tasks to keep.</p>
</dd>
<dt><code>learner_ids</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Ids of Learners to keep.</p>
</dd>
<dt><code>learner_hashes</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Hashes of Learners to keep.</p>
</dd>
<dt><code>resampling_ids</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Ids of Resamplings to keep.</p>
</dd>
<dt><code>resampling_hashes</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Hashes of Resamplings to keep.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Returns the object itself, but modified <strong>by reference</strong>.
You need to explicitly <code style="white-space: pre;">⁠$clone()⁠</code> the object beforehand if you want to keeps
the object in its previous state.
</p>


<hr>
<a id="method-BenchmarkResult-resample_result"></a>



<h4>Method <code>resample_result()</code>
</h4>

<p>Retrieve the i-th ResampleResult, by position or by unique hash <code>uhash</code>.
<code>i</code> and <code>uhash</code> are mutually exclusive.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$resample_result(i = NULL, uhash = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>i</code></dt>
<dd>
<p>(<code>integer(1)</code>)<br>
The iteration value to filter for.</p>
</dd>
<dt><code>uhash</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
The <code>ushash</code> value to filter for.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>ResampleResult.
</p>


<hr>
<a id="method-BenchmarkResult-discard"></a>



<h4>Method <code>discard()</code>
</h4>

<p>Shrinks the BenchmarkResult by discarding parts of the internally stored data.
Note that certain operations might stop work, e.g. extracting
importance values from learners or calculating measures requiring the task's data.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$discard(backends = FALSE, models = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>backends</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
If <code>TRUE</code>, the DataBackend is removed from all stored Tasks.</p>
</dd>
<dt><code>models</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
If <code>TRUE</code>, the stored model is removed from all Learners.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>Returns the object itself, but modified <strong>by reference</strong>.
You need to explicitly <code style="white-space: pre;">⁠$clone()⁠</code> the object beforehand if you want to keeps
the object in its previous state.
</p>


<hr>
<a id="method-BenchmarkResult-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>BenchmarkResult$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>Note</h3>

<p>All stored objects are accessed by reference.
Do not modify any extracted object without cloning it first.
</p>


<h3>See Also</h3>


<ul>
<li>
<p> Chapter in the <a href="https://mlr3book.mlr-org.com/">mlr3book</a>:
<a href="https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking">https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html#sec-benchmarking</a>
</p>
</li>
<li>
<p> Package <a href="https://CRAN.R-project.org/package=mlr3viz"><span class="pkg">mlr3viz</span></a> for some generic visualizations.
</p>
</li>
<li> <p><a href="https://CRAN.R-project.org/package=mlr3benchmark"><span class="pkg">mlr3benchmark</span></a> for post-hoc analysis of benchmark results.
</p>
</li>
</ul>
<p>Other benchmark: 
<code>benchmark()</code>,
<code>benchmark_grid()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(123)
learners = list(
  lrn("classif.featureless", predict_type = "prob"),
  lrn("classif.rpart", predict_type = "prob")
)

design = benchmark_grid(
  tasks = list(tsk("sonar"), tsk("penguins")),
  learners = learners,
  resamplings = rsmp("cv", folds = 3)
)
print(design)

bmr = benchmark(design)
print(bmr)

bmr$tasks
bmr$learners

# first 5 resampling iterations
head(as.data.table(bmr, measures = c("classif.acc", "classif.auc")), 5)

# aggregate results
bmr$aggregate()

# aggregate results with hyperparameters as separate columns
mlr3misc::unnest(bmr$aggregate(params = TRUE), "params")

# extract resample result for classif.rpart
rr = bmr$aggregate()[learner_id == "classif.rpart", resample_result][[1]]
print(rr)

# access the confusion matrix of the first resampling iteration
rr$predictions()[[1]]$confusion

# reduce to subset with task id "sonar"
bmr$filter(task_ids = "sonar")
print(bmr)
</code></pre>


</div>