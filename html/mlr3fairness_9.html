<div class="container">

<table style="width: 100%;"><tr>
<td>fairness_accuracy_tradeoff</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Plot Fairness Accuracy Trade-offs</h2>

<h3>Description</h3>

<p>Provides visualization wrt. trade-offs between fairness and accuracy metrics across learners and
resampling iterations.
This can assist in gauging the optimal model from a set of options along with estimates of variance
(through individual resampling iterations).
</p>


<h3>Usage</h3>

<pre><code class="language-R">fairness_accuracy_tradeoff(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>
<p>(PredictionClassif | BenchmarkResult | ResampleResult)<br>
The binary class prediction object that will be evaluated.
</p>

<ul>
<li>
<p> If provided a PredictionClassif.
Then only one point will indicate the accuracy and fairness metrics for the current predictions.
Requires also passing a Task.
</p>
</li>
<li>
<p> If provided a ResampleResult.
Then the plot will compare the accuracy and fairness metrics for the same model,
but different resampling iterations  as well as the aggregate indicated by a cross.
</p>
</li>
<li>
<p> If provided a BenchmarkResult.
Then the plot will compare the accuracy and fairness metrics for all models and all resampling iterations.
Points are colored according to the learner_id and faceted by task_id.
The aggregated score is indicated by a cross.
</p>
</li>
</ul>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Arguments to be passed to methods. Such as:
</p>

<ul>
<li> <p><code>fairness_measure</code> (Measure)<br>
The fairness measures that will evaluated.
Default measure set to be <code>msr("fairness.fpr")</code>
</p>
</li>
<li> <p><code>accuracy_measure</code> (Measure)<br>
The accuracy measure that will evaluated.
Default measure set to be msr("classif.acc").
</p>
</li>
<li> <p><code>task</code> (TaskClassif)<br>
The data task that contains the protected column, only required when the class of object is (PredictionClassif)
</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Value</h3>

<p>A 'ggplot2' object.
</p>


<h3>Protected Attributes</h3>

<p>The protected attribute is specified as a <code>col_role</code> in the corresponding <code>Task()</code>:<br><code style="white-space: pre;">⁠&lt;Task&gt;$col_roles$pta = "name_of_attribute"⁠</code> <br>
This also allows specifying more than one protected attribute,
in which case fairness will be considered on the level of intersecting groups defined by all columns
selected as a predicted attribute.
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("mlr3")
library("mlr3learners")
library("ggplot2")

# Setup the Fairness measure and tasks
task = tsk("adult_train")$filter(1:500)
learner = lrn("classif.ranger", predict_type = "prob")
fairness_measure = msr("fairness.tpr")

# Example 1 - A single prediction
learner$train(task)
predictions = learner$predict(task)
fairness_accuracy_tradeoff(predictions, fairness_measure, task = task)

# Example2 - A benchmark
design = benchmark_grid(
  tasks = task,
  learners = lrns(c("classif.featureless", "classif.rpart"),
    predict_type = "prob", predict_sets = c("train", "test")),
  resamplings = rsmps("cv", folds = 2)
)
bmr = benchmark(design)
fairness_accuracy_tradeoff(bmr, fairness_measure)
</code></pre>


</div>