<div class="container">

<table style="width: 100%;"><tr>
<td>mlr_tuners_hyperband</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Tuner Using the Hyperband Algorithm</h2>

<h3>Description</h3>

<p>Optimizer using the Hyperband (HB) algorithm.
HB runs the Successive Halving Algorithm (SHA) with different numbers of stating configurations.
The algorithm is initialized with the same parameters as Successive Halving but without <code>n</code>.
Each run of Successive Halving is called a bracket and starts with a different budget <code>r_0</code>.
A smaller starting budget means that more configurations can be tried out.
The most explorative bracket allocated the minimum budget <code>r_min</code>.
The next bracket increases the starting budget by a factor of <code>eta</code>.
In each bracket, the starting budget increases further until the last bracket <code>s = 0</code> essentially performs a random search with the full budget <code>r_max</code>.
The number of brackets <code>s_max + 1</code> is calculated with <code>s_max = log(r_min / r_max)(eta)</code>.
Under the condition that <code>r_0</code> increases by <code>eta</code> with each bracket, <code>r_min</code> sometimes has to be adjusted slightly in order not to use more than <code>r_max</code> resources in the last bracket.
The number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.
The following table shows a full run of HB with <code>eta = 2</code>, <code>r_min = 1</code> and <code>r_max = 8</code>.</p>

<table>
<tr>
<td style="text-align: right;">
   <code>s</code> </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;"> 3 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;"> 2 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;"> 1 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;"> 0 </td>
</tr>
<tr>
<td style="text-align: right;">
   <code>i</code> </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> <code>n_i</code> </td>
<td style="text-align: right;"> <code>r_i</code> </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> <code>n_i</code> </td>
<td style="text-align: right;"> <code>r_i</code> </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> <code>n_i</code> </td>
<td style="text-align: right;"> <code>r_i</code> </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> <code>n_i</code> </td>
<td style="text-align: right;"> <code>r_i</code> </td>
</tr>
<tr>
<td style="text-align: right;">
   0 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 8 </td>
<td style="text-align: right;"> 1 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 6 </td>
<td style="text-align: right;"> 2 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 4 </td>
<td style="text-align: right;"> 4 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 8 </td>
<td style="text-align: right;"> 4 </td>
</tr>
<tr>
<td style="text-align: right;">
   1 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 4 </td>
<td style="text-align: right;"> 2 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 3 </td>
<td style="text-align: right;"> 4 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 2 </td>
<td style="text-align: right;"> 8 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;">  </td>
</tr>
<tr>
<td style="text-align: right;">
   2 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 2 </td>
<td style="text-align: right;"> 4 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 1 </td>
<td style="text-align: right;"> 8 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;">  </td>
</tr>
<tr>
<td style="text-align: right;">
   3 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;"> 1 </td>
<td style="text-align: right;"> 8 </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: left;">  </td>
<td style="text-align: right;">  </td>
<td style="text-align: right;">  </td>
</tr>
<tr>
<td style="text-align: right;">
</td>
</tr>
</table>
<p><code>s</code> is the bracket number, <code>i</code> is the stage number, <code>n_i</code> is the number of configurations and <code>r_i</code> is the budget allocated to a single configuration.
</p>
<p>The budget hyperparameter must be tagged with <code>"budget"</code> in the search space.
The minimum budget (<code>r_min</code>) which is allocated in the base stage of the most explorative bracket, is set by the lower bound of the budget parameter.
The upper bound defines the maximum budget (<code>r_max</code>) which is allocated to the candidates in the last stages.
</p>


<h3>Dictionary</h3>

<p>This mlr3tuning::Tuner can be instantiated via the dictionary
mlr3tuning::mlr_tuners or with the associated sugar function <code>mlr3tuning::tnr()</code>:
</p>
<div class="sourceCode"><pre>TunerBatchHyperband$new()
mlr_tuners$get("hyperband")
tnr("hyperband")
</pre></div>


<h3>Subsample Budget</h3>

<p>If the learner lacks a natural budget parameter,
mlr3pipelines::PipeOpSubsample can be applied to use the subsampling rate
as budget parameter. The resulting mlr3pipelines::GraphLearner is fitted on
small proportions of the mlr3::Task in the first stage, and on the complete
task in last stage.
</p>


<h3>Custom Sampler</h3>

<p>Hyperband supports custom paradox::Sampler object for initial
configurations in each bracket.
A custom sampler may look like this (the full example is given in the
<em>examples</em> section):
</p>
<div class="sourceCode"><pre># - beta distribution with alpha = 2 and beta = 5
# - categorical distribution with custom probabilities
sampler = SamplerJointIndep$new(list(
  Sampler1DRfun$new(params[[2]], function(n) rbeta(n, 2, 5)),
  Sampler1DCateg$new(params[[3]], prob = c(0.2, 0.3, 0.5))
))
</pre></div>


<h3>Progress Bars</h3>

<p><code style="white-space: pre;">⁠$optimize()⁠</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a bbotk::Terminator. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Parallelization</h3>

<p>This hyperband implementation evaluates hyperparameter configurations of equal budget across brackets in one batch.
For example, all configurations in stage 1 of bracket 3 and stage 0 of bracket 2 in one batch.
To select a parallel backend, use the <code>plan()</code> function of the <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> package.
</p>


<h3>Logging</h3>

<p>Hyperband uses a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Resources</h3>

<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul>
<li> <p><a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Tune</a> the hyperparameters of XGBoost with Hyperband.
</p>
</li>
<li>
<p> Use data <a href="https://mlr-org.com/gallery/series/2023-01-16-hyperband-subsampling/">subsampling</a> and Hyperband to optimize a support vector machine.
</p>
</li>
</ul>
<h3>Parameters</h3>


<dl>
<dt><code>eta</code></dt>
<dd>
<p><code>numeric(1)</code><br>
With every stage, the budget is increased by a factor of <code>eta</code> and only the best <code>1 / eta</code> points are promoted to the next stage.
Non-integer values are supported, but <code>eta</code> is not allowed to be less or equal to 1.</p>
</dd>
<dt><code>sampler</code></dt>
<dd>
<p>paradox::Sampler<br>
Object defining how the samples of the parameter space should be drawn in the base stage of each bracket.
The default is uniform sampling.</p>
</dd>
<dt><code>repetitions</code></dt>
<dd>
<p><code>integer(1)</code><br>
If <code>1</code> (default), optimization is stopped once all brackets are evaluated.
Otherwise, optimization is stopped after <code>repetitions</code> runs of HB.
The bbotk::Terminator might stop the optimization before all repetitions are executed.</p>
</dd>
</dl>
<h3>Archive</h3>

<p>The bbotk::Archive holds the following additional columns that are specific to HB:
</p>

<ul>
<li> <p><code>bracket</code> (<code>integer(1)</code>)<br>
The bracket index. Counts down to 0.
</p>
</li>
<li> <p><code>stage</code> (<code style="white-space: pre;">⁠integer(1))⁠</code><br>
The stages of each bracket. Starts counting at 0.
</p>
</li>
<li> <p><code>repetition</code> (<code style="white-space: pre;">⁠integer(1))⁠</code><br>
Repetition index. Start counting at 1.
</p>
</li>
</ul>
<h3>Super classes</h3>

<p><code>mlr3tuning::Tuner</code> -&gt; <code>mlr3tuning::TunerBatch</code> -&gt; <code>mlr3tuning::TunerBatchFromOptimizerBatch</code> -&gt; <code>TunerBatchHyperband</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchHyperband-new"><code>TunerBatchHyperband$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchHyperband-clone"><code>TunerBatchHyperband$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-format"><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-help"><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-print"><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href="../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize"><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul></details><hr>
<a id="method-TunerBatchHyperband-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchHyperband$new()</pre></div>


<hr>
<a id="method-TunerBatchHyperband-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchHyperband$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>Source</h3>

<p>Li L, Jamieson K, DeSalvo G, Rostamizadeh A, Talwalkar A (2018).
“Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.”
<em>Journal of Machine Learning Research</em>, <b>18</b>(185), 1-52.
<a href="https://jmlr.org/papers/v18/16-558.html">https://jmlr.org/papers/v18/16-558.html</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R">if(requireNamespace("xgboost")) {
  library(mlr3learners)

  # define hyperparameter and budget parameter
  search_space = ps(
    nrounds = p_int(lower = 1, upper = 16, tags = "budget"),
    eta = p_dbl(lower = 0, upper = 1),
    booster = p_fct(levels = c("gbtree", "gblinear", "dart"))
  )

  
  # hyperparameter tuning on the pima indians diabetes data set
  instance = tune(
    tnr("hyperband"),
    task = tsk("pima"),
    learner = lrn("classif.xgboost", eval_metric = "logloss"),
    resampling = rsmp("cv", folds = 3),
    measures = msr("classif.ce"),
    search_space = search_space,
    term_evals = 100
  )

  # best performing hyperparameter configuration
  instance$result
  
}
</code></pre>


</div>