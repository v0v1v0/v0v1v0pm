<div class="container">

<table style="width: 100%;"><tr>
<td>fbeta</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>F-beta Score</h2>

<h3>Description</h3>

<p>Measure to compare true observed labels with predicted
labels
in binary classification tasks.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fbeta(truth, response, positive, beta = 1, na_value = NaN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>truth</code></td>
<td>
<p>(<code>factor()</code>)<br>
True (observed) labels.
Must have the exactly same two levels and the same length as <code>response</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>response</code></td>
<td>
<p>(<code>factor()</code>)<br>
Predicted response labels.
Must have the exactly same two levels and the same length as <code>truth</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>positive</code></td>
<td>
<p>(<code style="white-space: pre;">⁠character(1))⁠</code><br>
Name of the positive class.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>beta</code></td>
<td>
<p>(<code>numeric(1)</code>)<br>
Parameter to give either precision or recall more weight.
Default is <code>1</code>, resulting in balanced weights.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na_value</code></td>
<td>
<p>(<code>numeric(1)</code>)<br>
Value that should be returned if the measure is not defined for the input
(as described in the note). Default is <code>NaN</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>(<code>any</code>)<br>
Additional arguments. Currently ignored.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>With <code class="reqn">P</code> as <code>precision()</code> and <code class="reqn">R</code> as <code>recall()</code>, the F-beta Score is defined as </p>
<p style="text-align: center;"><code class="reqn">
   (1 + \beta^2) \frac{P \cdot R}{(\beta^2 P) + R}.
</code>
</p>

<p>It measures the effectiveness of retrieval with respect to a user who attaches <code class="reqn">\beta</code> times
as much importance to recall as precision.
For <code class="reqn">\beta = 1</code>, this measure is called "F1" score.
</p>
<p>This measure is undefined if precision or recall is undefined, i.e. TP + FP = 0 or TP + FN = 0.
</p>


<h3>Value</h3>

<p>Performance value as <code>numeric(1)</code>.
</p>


<h3>Meta Information</h3>


<ul>
<li>
<p> Type: <code>"binary"</code>
</p>
</li>
<li>
<p> Range: <code class="reqn">[0, 1]</code>
</p>
</li>
<li>
<p> Minimize: <code>FALSE</code>
</p>
</li>
<li>
<p> Required prediction: <code>response</code>
</p>
</li>
</ul>
<h3>References</h3>

<p>Rijsbergen, Van CJ (1979).
<em>Information Retrieval</em>, 2nd edition.
Butterworth-Heinemann, Newton, MA, USA.
ISBN 408709294.
</p>
<p>Goutte C, Gaussier E (2005).
“A Probabilistic Interpretation of Precision,  Recall and F-Score,  with Implication for Evaluation.”
In <em>Lecture Notes in Computer Science</em>, 345–359.
<a href="https://doi.org/10.1007/978-3-540-31865-1_25">doi:10.1007/978-3-540-31865-1_25</a>.
</p>


<h3>See Also</h3>

<p>Other Binary Classification Measures: 
<code>auc()</code>,
<code>bbrier()</code>,
<code>dor()</code>,
<code>fdr()</code>,
<code>fn()</code>,
<code>fnr()</code>,
<code>fomr()</code>,
<code>fp()</code>,
<code>fpr()</code>,
<code>gmean()</code>,
<code>gpr()</code>,
<code>npv()</code>,
<code>ppv()</code>,
<code>prauc()</code>,
<code>tn()</code>,
<code>tnr()</code>,
<code>tp()</code>,
<code>tpr()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(1)
lvls = c("a", "b")
truth = factor(sample(lvls, 10, replace = TRUE), levels = lvls)
response = factor(sample(lvls, 10, replace = TRUE), levels = lvls)
fbeta(truth, response, positive = "a")
</code></pre>


</div>