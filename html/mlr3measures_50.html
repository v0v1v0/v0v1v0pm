<div class="container">

<table style="width: 100%;"><tr>
<td>prauc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Area Under the Precision-Recall Curve</h2>

<h3>Description</h3>

<p>Measure to compare true observed labels with predicted
probabilities
in binary classification tasks.
</p>


<h3>Usage</h3>

<pre><code class="language-R">prauc(truth, prob, positive, na_value = NaN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>truth</code></td>
<td>
<p>(<code>factor()</code>)<br>
True (observed) labels.
Must have the exactly same two levels and the same length as <code>response</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prob</code></td>
<td>
<p>(<code>numeric()</code>)<br>
Predicted probability for positive class.
Must have exactly same length as <code>truth</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>positive</code></td>
<td>
<p>(<code style="white-space: pre;">⁠character(1))⁠</code><br>
Name of the positive class.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na_value</code></td>
<td>
<p>(<code>numeric(1)</code>)<br>
Value that should be returned if the measure is not defined for the input
(as described in the note). Default is <code>NaN</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>(<code>any</code>)<br>
Additional arguments. Currently ignored.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Computes the area under the Precision-Recall curve (PRC).
The PRC can be interpreted as the relationship between precision and recall (sensitivity),
and is considered to be a more appropriate measure for unbalanced datasets than the ROC curve.
The AUC-PRC is computed by integration of the piecewise function.
</p>
<p>This measure is undefined if the true values are either all positive or
all negative.
</p>


<h3>Value</h3>

<p>Performance value as <code>numeric(1)</code>.
</p>


<h3>Meta Information</h3>


<ul>
<li>
<p> Type: <code>"binary"</code>
</p>
</li>
<li>
<p> Range: <code class="reqn">[0, 1]</code>
</p>
</li>
<li>
<p> Minimize: <code>FALSE</code>
</p>
</li>
<li>
<p> Required prediction: <code>prob</code>
</p>
</li>
</ul>
<h3>References</h3>

<p>Davis J, Goadrich M (2006).
“The relationship between precision-recall and ROC curves.”
In <em>Proceedings of the 23rd International Conference on Machine Learning</em>.
ISBN 9781595933836.
</p>


<h3>See Also</h3>

<p>Other Binary Classification Measures: 
<code>auc()</code>,
<code>bbrier()</code>,
<code>dor()</code>,
<code>fbeta()</code>,
<code>fdr()</code>,
<code>fn()</code>,
<code>fnr()</code>,
<code>fomr()</code>,
<code>fp()</code>,
<code>fpr()</code>,
<code>gmean()</code>,
<code>gpr()</code>,
<code>npv()</code>,
<code>ppv()</code>,
<code>tn()</code>,
<code>tnr()</code>,
<code>tp()</code>,
<code>tpr()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">truth = factor(c("a", "a", "a", "b"))
prob = c(.6, .7, .1, .4)
prauc(truth, prob, "a")
</code></pre>


</div>