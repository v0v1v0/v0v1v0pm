<div class="container">

<table style="width: 100%;"><tr>
<td>mlr_pipeops_imputeoor</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Out of Range Imputation</h2>

<h3>Description</h3>

<p>Impute factorial features by adding a new level <code>".MISSING"</code>.
</p>
<p>Impute numerical features by constant values shifted below the minimum or above the maximum by
using <code class="reqn">min(x) - offset - multiplier * diff(range(x))</code> or
<code class="reqn">max(x) + offset + multiplier * diff(range(x))</code>.
</p>
<p>This type of imputation is especially sensible in the context of tree-based methods, see also
Ding &amp; Simonoff (2010).
</p>
<p>If a factor is missing during prediction, but not during training, this adds an unseen level
<code>".MISSING"</code>, which would be a problem for most models. This is why it is recommended to use
<code>po("fixfactors")</code> and
<code>po("imputesample", affect_columns = selector_type(types = c("factor", "ordered")))</code>
(or some other imputation method) after this imputation method, if missing values are expected during prediction
in factor columns that had no missing values during training.
</p>


<h3>Format</h3>

<p><code>R6Class</code> object inheriting from <code>PipeOpImpute</code>/<code>PipeOp</code>.
</p>


<h3>Construction</h3>

<div class="sourceCode"><pre>PipeOpImputeOOR$new(id = "imputeoor", param_vals = list())
</pre></div>

<ul>
<li> <p><code>id</code> :: <code>character(1)</code><br>
Identifier of resulting object, default <code>"imputeoor"</code>.
</p>
</li>
<li> <p><code>param_vals</code> :: named <code>list</code><br>
List of hyperparameter settings, overwriting the hyperparameter settings that would otherwise be set during construction. Default <code>list()</code>.
</p>
</li>
</ul>
<h3>Input and Output Channels</h3>

<p>Input and output channels are inherited from <code>PipeOpImpute</code>.
</p>
<p>The output is the input <code>Task</code> with all affected features having missing values imputed as described above.
</p>


<h3>State</h3>

<p>The <code style="white-space: pre;">⁠$state⁠</code> is a named <code>list</code> with the <code style="white-space: pre;">⁠$state⁠</code> elements inherited from <code>PipeOpImpute</code>.
</p>
<p>The <code style="white-space: pre;">⁠$state$model⁠</code> contains either <code>".MISSING"</code> used for <code>character</code> and <code>factor</code> (also
<code>ordered</code>) features or <code>numeric(1)</code> indicating the constant value used for imputation of
<code>integer</code> and <code>numeric</code> features.
</p>


<h3>Parameters</h3>

<p>The parameters are the parameters inherited from <code>PipeOpImpute</code>, as well as:
</p>

<ul>
<li> <p><code>min</code> :: <code>logical(1)</code> <br>
Should <code>integer</code> and <code>numeric</code> features be shifted below the minimum? Initialized to TRUE. If FALSE
they are shifted above the maximum. See also the description above.
</p>
</li>
<li> <p><code>offset</code> :: <code>numeric(1)</code> <br>
Numerical non-negative offset as used in the description above for <code>integer</code> and <code>numeric</code>
features. Initialized to 1.
</p>
</li>
<li> <p><code>multiplier</code> :: <code>numeric(1)</code> <br>
Numerical non-negative multiplier as used in the description above for <code>integer</code> and <code>numeric</code>
features. Initialized to 1.
</p>
</li>
</ul>
<h3>Internals</h3>

<p>Adds an explicit new <code>level()</code> to <code>factor</code> and <code>ordered</code> features, but not to <code>character</code> features.
For <code>integer</code> and <code>numeric</code> features uses the <code>min</code>, <code>max</code>, <code>diff</code> and <code>range</code> functions.
<code>integer</code> and <code>numeric</code> features that are entirely <code>NA</code> are imputed as <code>0</code>.
</p>


<h3>Methods</h3>

<p>Only methods inherited from <code>PipeOpImpute</code>/<code>PipeOp</code>.
</p>


<h3>References</h3>

<p>Ding Y, Simonoff JS (2010).
“An Investigation of Missing Data Methods for Classification Trees Applied to Binary Response Data.”
<em>Journal of Machine Learning Research</em>, <b>11</b>(6), 131-170.
<a href="https://jmlr.org/papers/v11/ding10a.html">https://jmlr.org/papers/v11/ding10a.html</a>.
</p>


<h3>See Also</h3>

<p>https://mlr-org.com/pipeops.html
</p>
<p>Other PipeOps: 
<code>PipeOp</code>,
<code>PipeOpEnsemble</code>,
<code>PipeOpImpute</code>,
<code>PipeOpTargetTrafo</code>,
<code>PipeOpTaskPreproc</code>,
<code>PipeOpTaskPreprocSimple</code>,
<code>mlr_pipeops</code>,
<code>mlr_pipeops_adas</code>,
<code>mlr_pipeops_blsmote</code>,
<code>mlr_pipeops_boxcox</code>,
<code>mlr_pipeops_branch</code>,
<code>mlr_pipeops_chunk</code>,
<code>mlr_pipeops_classbalancing</code>,
<code>mlr_pipeops_classifavg</code>,
<code>mlr_pipeops_classweights</code>,
<code>mlr_pipeops_colapply</code>,
<code>mlr_pipeops_collapsefactors</code>,
<code>mlr_pipeops_colroles</code>,
<code>mlr_pipeops_copy</code>,
<code>mlr_pipeops_datefeatures</code>,
<code>mlr_pipeops_encode</code>,
<code>mlr_pipeops_encodeimpact</code>,
<code>mlr_pipeops_encodelmer</code>,
<code>mlr_pipeops_featureunion</code>,
<code>mlr_pipeops_filter</code>,
<code>mlr_pipeops_fixfactors</code>,
<code>mlr_pipeops_histbin</code>,
<code>mlr_pipeops_ica</code>,
<code>mlr_pipeops_imputeconstant</code>,
<code>mlr_pipeops_imputehist</code>,
<code>mlr_pipeops_imputelearner</code>,
<code>mlr_pipeops_imputemean</code>,
<code>mlr_pipeops_imputemedian</code>,
<code>mlr_pipeops_imputemode</code>,
<code>mlr_pipeops_imputesample</code>,
<code>mlr_pipeops_kernelpca</code>,
<code>mlr_pipeops_learner</code>,
<code>mlr_pipeops_missind</code>,
<code>mlr_pipeops_modelmatrix</code>,
<code>mlr_pipeops_multiplicityexply</code>,
<code>mlr_pipeops_multiplicityimply</code>,
<code>mlr_pipeops_mutate</code>,
<code>mlr_pipeops_nmf</code>,
<code>mlr_pipeops_nop</code>,
<code>mlr_pipeops_ovrsplit</code>,
<code>mlr_pipeops_ovrunite</code>,
<code>mlr_pipeops_pca</code>,
<code>mlr_pipeops_proxy</code>,
<code>mlr_pipeops_quantilebin</code>,
<code>mlr_pipeops_randomprojection</code>,
<code>mlr_pipeops_randomresponse</code>,
<code>mlr_pipeops_regravg</code>,
<code>mlr_pipeops_removeconstants</code>,
<code>mlr_pipeops_renamecolumns</code>,
<code>mlr_pipeops_replicate</code>,
<code>mlr_pipeops_rowapply</code>,
<code>mlr_pipeops_scale</code>,
<code>mlr_pipeops_scalemaxabs</code>,
<code>mlr_pipeops_scalerange</code>,
<code>mlr_pipeops_select</code>,
<code>mlr_pipeops_smote</code>,
<code>mlr_pipeops_smotenc</code>,
<code>mlr_pipeops_spatialsign</code>,
<code>mlr_pipeops_subsample</code>,
<code>mlr_pipeops_targetinvert</code>,
<code>mlr_pipeops_targetmutate</code>,
<code>mlr_pipeops_targettrafoscalerange</code>,
<code>mlr_pipeops_textvectorizer</code>,
<code>mlr_pipeops_threshold</code>,
<code>mlr_pipeops_tunethreshold</code>,
<code>mlr_pipeops_unbranch</code>,
<code>mlr_pipeops_updatetarget</code>,
<code>mlr_pipeops_vtreat</code>,
<code>mlr_pipeops_yeojohnson</code>
</p>
<p>Other Imputation PipeOps: 
<code>PipeOpImpute</code>,
<code>mlr_pipeops_imputeconstant</code>,
<code>mlr_pipeops_imputehist</code>,
<code>mlr_pipeops_imputelearner</code>,
<code>mlr_pipeops_imputemean</code>,
<code>mlr_pipeops_imputemedian</code>,
<code>mlr_pipeops_imputemode</code>,
<code>mlr_pipeops_imputesample</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library("mlr3")
set.seed(2409)
data = tsk("pima")$data()
data$y = factor(c(NA, sample(letters, size = 766, replace = TRUE), NA))
data$z = ordered(c(NA, sample(1:10, size = 767, replace = TRUE)))
task = TaskClassif$new("task", backend = data, target = "diabetes")
task$missings()
po = po("imputeoor")
new_task = po$train(list(task = task))[[1]]
new_task$missings()
new_task$data()

# recommended use when missing values are expected during prediction on
# factor columns that had no missing values during training
gr = po("imputeoor") %&gt;&gt;%
  po("fixfactors") %&gt;&gt;%
  po("imputesample", affect_columns = selector_type(types = c("factor", "ordered")))
t1 = as_task_classif(data.frame(l = as.ordered(letters[1:3]), t = letters[1:3]), target = "t")
t2 = as_task_classif(data.frame(l = as.ordered(c("a", NA, NA)), t = letters[1:3]), target = "t")
gr$train(t1)[[1]]$data()

# missing values during prediction are sampled randomly
gr$predict(t2)[[1]]$data()
</code></pre>


</div>