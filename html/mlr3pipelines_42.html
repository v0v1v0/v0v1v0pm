<div class="container">

<table style="width: 100%;"><tr>
<td>mlr_learners_avg</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Optimized Weighted Average of Features for Classification and Regression</h2>

<h3>Description</h3>

<p>Computes a weighted average of inputs.
Used in the context of computing weighted averages of predictions.
</p>
<p>Predictions are averaged using <code>weights</code> (in order of appearance in the data) which are optimized using
nonlinear optimization from the package <a href="https://CRAN.R-project.org/package=nloptr"><span class="pkg">nloptr</span></a> for a measure provided in
<code>measure</code>. (defaults to <code>classif.ce</code> for <code>LearnerClassifAvg</code> and <code>regr.mse</code> for <code>LearnerRegrAvg</code>).
Learned weights can be obtained from <code style="white-space: pre;">⁠$model⁠</code>.
This Learner implements and generalizes an approach proposed in LeDell (2015) that uses non-linear
optimization in order to learn base-learner weights that optimize a given performance metric (e.g <code>AUC</code>).
The approach is similar but not exactly the same as the one implemented as <code>AUC</code> in the <a href="https://CRAN.R-project.org/package=SuperLearner"><span class="pkg">SuperLearner</span></a>
R package (when <code>metric</code> is <code>"classif.auc"</code>).
For a more detailed analysis and the general idea, the reader is referred to LeDell (2015).
</p>
<p>Note, that weights always sum to 1 by division by <code>sum(weights)</code> before weighting
incoming features.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mlr_learners_classif.avg

mlr_learners_regr.avg
</code></pre>


<h3>Format</h3>

<p><code>R6Class</code> object inheriting from <code>mlr3::LearnerClassif</code>/<code>mlr3::Learner</code>.
</p>


<h3>Parameters</h3>

<p>The parameters are the parameters inherited from <code>LearnerClassif</code>, as well as:
</p>

<ul>
<li> <p><code>measure</code> :: <code>Measure</code> | <code>character</code> <br><code>Measure</code> to optimize for.
Will be converted to a <code>Measure</code> in case it is <code>character</code>.
Initialized to <code>"classif.ce"</code>, i.e. misclassification error for classification
and <code>"regr.mse"</code>, i.e. mean squared error for regression.
</p>
</li>
<li> <p><code>optimizer</code> :: <code>Optimizer</code> | <code>character(1)</code><br><code>Optimizer</code> used to find optimal thresholds.
If <code>character</code>, converts to <code>Optimizer</code>
via <code>opt</code>. Initialized to <code>OptimizerNLoptr</code>.
Nloptr hyperparameters are initialized to <code>xtol_rel = 1e-8</code>, <code>algorithm = "NLOPT_LN_COBYLA"</code>
and equal initial weights for each learner.
For more fine-grained control, it is recommended to supply a instantiated <code>Optimizer</code>.
</p>
</li>
<li> <p><code>log_level</code> :: <code>character(1)</code> | <code>integer(1)</code><br>
Set a temporary log-level for <code>lgr::get_logger("bbotk")</code>. Initialized to: "warn".
</p>
</li>
</ul>
<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">⁠LearnerClassifAvg$new(), id = "classif.avg")⁠</code> <br>
(<code>chr</code>) -&gt; <code>self</code> <br>
Constructor.
</p>
</li>
<li> <p><code style="white-space: pre;">⁠LearnerRegrAvg$new(), id = "regr.avg")⁠</code> <br>
(<code>chr</code>) -&gt; <code>self</code> <br>
Constructor.
</p>
</li>
</ul>
<h3>References</h3>

<p>LeDell, Erin (2015).
<em>Scalable Ensemble Learning and Computationally Efficient Variance Estimation</em>.
Ph.D. thesis, UC Berkeley.
</p>


<h3>See Also</h3>

<p>Other Learners: 
<code>mlr_learners_graph</code>
</p>
<p>Other Ensembles: 
<code>PipeOpEnsemble</code>,
<code>mlr_pipeops_classifavg</code>,
<code>mlr_pipeops_ovrunite</code>,
<code>mlr_pipeops_regravg</code>
</p>


</div>