<div class="container">

<table style="width: 100%;"><tr>
<td>mlr_pipeops_preproc_torch</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Base Class for Lazy Tensor Preprocessing</h2>

<h3>Description</h3>

<p>This <code>PipeOp</code> can be used to preprocess (one or more) <code>lazy_tensor</code> columns contained in an <code>mlr3::Task</code>.
The preprocessing function is specified as construction argument <code>fn</code> and additional arguments to this
function can be defined through the <code>PipeOp</code>'s parameter set.
The preprocessing is done per column, i.e. the number of lazy tensor output columns is equal
to the number of lazy tensor input columns.
</p>
<p>To create custom preprocessing <code>PipeOp</code>s you can use <code>pipeop_preproc_torch</code>.
</p>


<h3>Inheriting</h3>

<p>In addition to specifying the construction arguments, you can overwrite the private <code>.shapes_out()</code> method.
If you don't overwrite it, the output shapes are assumed to be unknown (<code>NULL</code>).
</p>

<ul><li> <p><code>.shapes_out(shapes_in, param_vals, task)</code><br>
(<code>list()</code>, <code style="white-space: pre;">⁠list(), ⁠</code>Task<code>or</code>NULL<code style="white-space: pre;">⁠) -&gt; ⁠</code>list()<code style="white-space: pre;">⁠\cr This private method calculates the output shapes of the lazy tensor columns that are created from applying the preprocessing function with the provided parameter values (⁠</code>param_vals<code style="white-space: pre;">⁠). The ⁠</code>task<code style="white-space: pre;">⁠is very rarely needed, but if it is it should be checked that it is not⁠</code>NULL'.
</p>
<p>This private method only has the responsibility to calculate the output shapes for one input column, i.e. the
input <code>shapes_in</code> can be assumed to have exactly one shape vector for which it must calculate the output shapes
and return it as a <code>list()</code> of length 1.
It can also be assumed that the shape is not <code>NULL</code> (i.e. unknown).
Also, the first dimension can be <code>NA</code>, i.e. is unknown (as for the batch dimension).
</p>
</li></ul>
<h3>Input and Output Channels</h3>

<p>See <code>PipeOpTaskPreproc</code>.
</p>


<h3>State</h3>

<p>In addition to state elements from <code>PipeOpTaskPreprocSimple</code>,
the state also contains the <code style="white-space: pre;">⁠$param_vals⁠</code> that were set during training.
</p>


<h3>Parameters</h3>

<p>In addition to the parameters inherited from <code>PipeOpTaskPreproc</code> as well as those specified during construction
as the argument <code>param_set</code> there are the following parameters:
</p>

<ul><li> <p><code>stages</code> :: <code>character(1)</code><br>
The stages during which to apply the preprocessing.
Can be one of <code>"train"</code>, <code>"predict"</code> or <code>"both"</code>.
The initial value of this parameter is set to <code>"train"</code> when the <code>PipeOp</code>'s id starts with <code>"augment_"</code> and
to <code>"both"</code> otherwise.
Note that the preprocessing that is applied during <code style="white-space: pre;">⁠$predict()⁠</code> uses the parameters that were set during
<code style="white-space: pre;">⁠$train()⁠</code> and not those that are set when performing the prediction.
</p>
</li></ul>
<h3>Internals</h3>

<p>During <code style="white-space: pre;">⁠$train()⁠</code> / <code style="white-space: pre;">⁠$predict()⁠</code>, a <code>PipeOpModule</code> with one input and one output channel is created.
The pipeop applies the function <code>fn</code> to the input tensor while additionally
passing the parameter values (minus <code>stages</code> and <code>affect_columns</code>) to <code>fn</code>.
The preprocessing graph of the lazy tensor columns is shallowly cloned and the <code>PipeOpModule</code> is added.
This is done to avoid modifying user input and means that identical <code>PipeOpModule</code>s can be part of different
preprocessing graphs. This is only possible, because the created <code>PipeOpModule</code> is stateless.
</p>
<p>At a later point in the graph, preprocessing graphs will be merged if possible to avoid unnecessary computation.
This is best illustrated by example:
One lazy tensor column's preprocessing graph is <code>A -&gt; B</code>.
Then, two branches are created <code>B -&gt; C</code> and <code>B -&gt; D</code>, creating two preprocessing graphs
<code>A -&gt; B -&gt; C</code> and <code>A -&gt; B -&gt; D</code>. When loading the data, we want to run the preprocessing only once, i.e. we don't
want to run the <code>A -&gt; B</code> part twice. For this reason, <code>task_dataset()</code> will try to merge graphs and cache
results from graphs. However, only graphs using the same dataset can currently be merged.
</p>
<p>Also, the shapes created during <code style="white-space: pre;">⁠$train()⁠</code> and <code style="white-space: pre;">⁠$predict()⁠</code> might differ.
To avoid the creation of graphs where the predict shapes are incompatible with the train shapes,
the hypothetical predict shapes are already calculated during <code style="white-space: pre;">⁠$train()⁠</code> (this is why the parameters that are set
during train are also used during predict) and the <code>PipeOpTorchModel</code> will check the train and predict shapes for
compatibility before starting the training.
</p>
<p>Otherwise, this mechanism is very similar to the <code>ModelDescriptor</code> construct.
</p>


<h3>Super classes</h3>

<p><code>mlr3pipelines::PipeOp</code> -&gt; <code>mlr3pipelines::PipeOpTaskPreproc</code> -&gt; <code>PipeOpTaskPreprocTorch</code>
</p>


<h3>Active bindings</h3>

<div class="r6-active-bindings">

<dl>
<dt><code>fn</code></dt>
<dd>
<p>The preprocessing function.</p>
</dd>
<dt><code>rowwise</code></dt>
<dd>
<p>Whether the preprocessing is applied rowwise.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-PipeOpTaskPreprocTorch-new"><code>PipeOpTaskPreprocTorch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-PipeOpTaskPreprocTorch-shapes_out"><code>PipeOpTaskPreprocTorch$shapes_out()</code></a>
</p>
</li>
<li> <p><a href="#method-PipeOpTaskPreprocTorch-clone"><code>PipeOpTaskPreprocTorch$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="help"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-help"><code>mlr3pipelines::PipeOp$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="predict"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-predict"><code>mlr3pipelines::PipeOp$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="print"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-print"><code>mlr3pipelines::PipeOp$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="train"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-train"><code>mlr3pipelines::PipeOp$train()</code></a></span></li>
</ul></details><hr>
<a id="method-PipeOpTaskPreprocTorch-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this <code>R6</code> class.
</p>


<h5>Usage</h5>

<div class="r"><pre>PipeOpTaskPreprocTorch$new(
  fn,
  id = "preproc_torch",
  param_vals = list(),
  param_set = ps(),
  packages = character(0),
  rowwise = FALSE,
  stages_init = NULL,
  tags = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fn</code></dt>
<dd>
<p>(<code>function</code> or <code>character(2)</code>)<br>
The preprocessing function. Must not modify its input in-place.
If it is a <code>character(2)</code>, the first element should be the namespace and the second element the name.
When the preprocessing function is applied to the tensor, the tensor will be passed by position as the first argument.
If the <code>param_set</code> is inferred (left as <code>NULL</code>) it is assumed that the first argument is the <code>torch_tensor</code>.</p>
</dd>
<dt><code>id</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The id for of the new object.</p>
</dd>
<dt><code>param_vals</code></dt>
<dd>
<p>(named <code>list()</code>)<br>
Parameter values to be set after construction.</p>
</dd>
<dt><code>param_set</code></dt>
<dd>
<p>(<code>ParamSet</code>)<br>
In case the function <code>fn</code> takes additional parameter besides a <code>torch_tensor</code> they can be
specfied as parameters. None of the parameters can have the <code>"predict"</code> tag.
All tags should include <code>"train"</code>.</p>
</dd>
<dt><code>packages</code></dt>
<dd>
<p>(<code>character()</code>)<br>
The packages the preprocessing function depends on.</p>
</dd>
<dt><code>rowwise</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Whether the preprocessing function is applied rowwise (and then concatenated by row) or directly to the whole
tensor. In the first case there is no batch dimension.</p>
</dd>
<dt><code>stages_init</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Initial value for the <code>stages</code> parameter.</p>
</dd>
<dt><code>tags</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Tags for the pipeop.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-PipeOpTaskPreprocTorch-shapes_out"></a>



<h4>Method <code>shapes_out()</code>
</h4>

<p>Calculates the output shapes that would result in applying the preprocessing to one or more
lazy tensor columns with the provided shape.
Names are ignored and only order matters.
It uses the parameter values that are currently set.
</p>


<h5>Usage</h5>

<div class="r"><pre>PipeOpTaskPreprocTorch$shapes_out(shapes_in, stage = NULL, task = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>shapes_in</code></dt>
<dd>
<p>(<code>list()</code> of (<code>integer()</code> or <code>NULL</code>))<br>
The input input shapes of the lazy tensors.
<code>NULL</code> indicates that the shape is unknown.
First dimension must be <code>NA</code> (if it is not <code>NULL</code>).</p>
</dd>
<dt><code>stage</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The stage: either <code>"train"</code> or <code>"predict"</code>.</p>
</dd>
<dt><code>task</code></dt>
<dd>
<p>(<code>Task</code> or <code>NULL</code>)<br>
The task, which is very rarely needed.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>list()</code> of (<code>integer()</code> or <code>NULL</code>)
</p>


<hr>
<a id="method-PipeOpTaskPreprocTorch-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>PipeOpTaskPreprocTorch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>Examples</h3>

<pre><code class="language-R">
# Creating a simple task
d = data.table(
  x1 = as_lazy_tensor(rnorm(10)),
  x2 = as_lazy_tensor(rnorm(10)),
  x3 = as_lazy_tensor(as.double(1:10)),
  y = rnorm(10)
)

taskin = as_task_regr(d, target = "y")

# Creating a simple preprocessing pipeop
po_simple = po("preproc_torch",
  # get rid of environment baggage
  fn = mlr3misc::crate(function(x, a) x + a),
  param_set = paradox::ps(a = paradox::p_int(tags = c("train", "required")))
)

po_simple$param_set$set_values(
  a = 100,
  affect_columns = selector_name(c("x1", "x2")),
  stages = "both" # use during train and predict
)

taskout_train = po_simple$train(list(taskin))[[1L]]
materialize(taskout_train$data(cols = c("x1", "x2")), rbind = TRUE)

taskout_predict_noaug = po_simple$predict(list(taskin))[[1L]]
materialize(taskout_predict_noaug$data(cols = c("x1", "x2")), rbind = TRUE)

po_simple$param_set$set_values(
  stages = "train"
)

# transformation is not applied
taskout_predict_aug = po_simple$predict(list(taskin))[[1L]]
materialize(taskout_predict_aug$data(cols = c("x1", "x2")), rbind = TRUE)

# Creating a more complex preprocessing PipeOp
PipeOpPreprocTorchPoly = R6::R6Class("PipeOpPreprocTorchPoly",
 inherit = PipeOpTaskPreprocTorch,
 public = list(
   initialize = function(id = "preproc_poly", param_vals = list()) {
     param_set = paradox::ps(
       n_degree = paradox::p_int(lower = 1L, tags = c("train", "required"))
     )
     param_set$set_values(
       n_degree = 1L
     )
     fn = mlr3misc::crate(function(x, n_degree) {
       torch::torch_cat(
         lapply(seq_len(n_degree), function(d) torch::torch_pow(x, d)),
         dim = 2L
       )
     })

     super$initialize(
       fn = fn,
       id = id,
       packages = character(0),
       param_vals = param_vals,
       param_set = param_set,
       stages_init = "both"
     )
   }
 ),
 private = list(
   .shapes_out = function(shapes_in, param_vals, task) {
     # shapes_in is a list of length 1 containing the shapes
     checkmate::assert_true(length(shapes_in[[1L]]) == 2L)
     if (shapes_in[[1L]][2L] != 1L) {
       stop("Input shape must be (NA, 1)")
     }
     list(c(NA, param_vals$n_degree))
   }
 )
)

po_poly = PipeOpPreprocTorchPoly$new(
  param_vals = list(n_degree = 3L, affect_columns = selector_name("x3"))
)

po_poly$shapes_out(list(c(NA, 1L)), stage = "train")

taskout = po_poly$train(list(taskin))[[1L]]
materialize(taskout$data(cols = "x3"), rbind = TRUE)

</code></pre>


</div>