<div class="container">

<table style="width: 100%;"><tr>
<td>mlr_pipeops_module</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Class for Torch Module Wrappers</h2>

<h3>Description</h3>

<p><code>PipeOpModule</code> wraps an <code>nn_module</code> or <code>function</code> that is being called during the <code>train</code> phase of this
<code>mlr3pipelines::PipeOp</code>. By doing so, this allows to assemble <code>PipeOpModule</code>s in a computational
<code>mlr3pipelines::Graph</code> that represents either a neural network or a preprocessing graph of a <code>lazy_tensor</code>.
In most cases it is easier to create such a network by creating a graph that generates this graph.
</p>
<p>In most cases it is easier to create such a network by creating a structurally related graph consisting
of nodes of class <code>PipeOpTorchIngress</code> and <code>PipeOpTorch</code>. This graph will then generate the graph consisting
of <code>PipeOpModule</code>s as part of the <code>ModelDescriptor</code>.
</p>


<h3>Input and Output Channels</h3>

<p>The number and names of the input and output channels can be set during construction. They input and output
<code>"torch_tensor"</code> during training, and <code>NULL</code> during prediction as the prediction phase currently serves no
meaningful purpose.
</p>


<h3>State</h3>

<p>The state is the value calculated by the public method <code>shapes_out()</code>.
</p>


<h3>Parameters</h3>

<p>No parameters.
</p>


<h3>Internals</h3>

<p>During training, the wrapped <code>nn_module</code> / <code>function</code> is called with the provided inputs in the order in which
the channels are defined. Arguments are <strong>not</strong> matched by name.
</p>


<h3>Super class</h3>

<p><code>mlr3pipelines::PipeOp</code> -&gt; <code>PipeOpModule</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>module</code></dt>
<dd>
<p>(<code>nn_module</code>)<br>
The torch module that is called during the training phase.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-PipeOpModule-new"><code>PipeOpModule$new()</code></a>
</p>
</li>
<li> <p><a href="#method-PipeOpModule-clone"><code>PipeOpModule$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="help"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-help"><code>mlr3pipelines::PipeOp$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="predict"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-predict"><code>mlr3pipelines::PipeOp$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="print"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-print"><code>mlr3pipelines::PipeOp$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3pipelines" data-topic="PipeOp" data-id="train"><a href="../../mlr3pipelines/html/PipeOp.html#method-PipeOp-train"><code>mlr3pipelines::PipeOp$train()</code></a></span></li>
</ul></details><hr>
<a id="method-PipeOpModule-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>PipeOpModule$new(
  id = "module",
  module = nn_identity(),
  inname = "input",
  outname = "output",
  param_vals = list(),
  packages = character(0)
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The id for of the new object.</p>
</dd>
<dt><code>module</code></dt>
<dd>
<p>(<code>nn_module</code> or <code style="white-space: pre;">⁠function()⁠</code>)<br>
The torch module or function that is being wrapped.</p>
</dd>
<dt><code>inname</code></dt>
<dd>
<p>(<code>character()</code>)<br>
The names of the input channels.</p>
</dd>
<dt><code>outname</code></dt>
<dd>
<p>(<code>character()</code>)<br>
The names of the output channels. If this parameter has length 1, the parameter module must
return a tensor. Otherwise it must return a <code>list()</code> of tensors of corresponding length.</p>
</dd>
<dt><code>param_vals</code></dt>
<dd>
<p>(named <code>list()</code>)<br>
Parameter values to be set after construction.</p>
</dd>
<dt><code>packages</code></dt>
<dd>
<p>(<code>character()</code>)<br>
The R packages this object depends on.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-PipeOpModule-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>PipeOpModule$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>See Also</h3>

<p>Other Graph Network: 
<code>ModelDescriptor()</code>,
<code>TorchIngressToken()</code>,
<code>mlr_learners_torch_model</code>,
<code>mlr_pipeops_torch</code>,
<code>mlr_pipeops_torch_ingress</code>,
<code>mlr_pipeops_torch_ingress_categ</code>,
<code>mlr_pipeops_torch_ingress_ltnsr</code>,
<code>mlr_pipeops_torch_ingress_num</code>,
<code>model_descriptor_to_learner()</code>,
<code>model_descriptor_to_module()</code>,
<code>model_descriptor_union()</code>,
<code>nn_graph()</code>
</p>
<p>Other PipeOp: 
<code>mlr_pipeops_torch_callbacks</code>,
<code>mlr_pipeops_torch_optimizer</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
## creating an PipeOpModule manually

# one input and output channel
po_module = po("module",
  id = "linear",
  module = torch::nn_linear(10, 20),
  inname = "input",
  outname = "output"
)
x = torch::torch_randn(16, 10)
# This calls the forward function of the wrapped module.
y = po_module$train(list(input = x))
str(y)

# multiple input and output channels
nn_custom = torch::nn_module("nn_custom",
  initialize = function(in_features, out_features) {
    self$lin1 = torch::nn_linear(in_features, out_features)
    self$lin2 = torch::nn_linear(in_features, out_features)
  },
  forward = function(x, z) {
    list(out1 = self$lin1(x), out2 = torch::nnf_relu(self$lin2(z)))
  }
)

module = nn_custom(3, 2)
po_module = po("module",
  id = "custom",
  module = module,
  inname = c("x", "z"),
  outname = c("out1", "out2")
)
x = torch::torch_randn(1, 3)
z = torch::torch_randn(1, 3)
out = po_module$train(list(x = x, z = z))
str(out)

# How such a PipeOpModule is usually generated
graph = po("torch_ingress_num") %&gt;&gt;% po("nn_linear", out_features = 10L)
result = graph$train(tsk("iris"))
# The PipeOpTorchLinear generates a PipeOpModule and adds it to a new (module) graph
result[[1]]$graph
linear_module = result[[1L]]$graph$pipeops$nn_linear
linear_module
formalArgs(linear_module$module)
linear_module$input$name

# Constructing a PipeOpModule using a simple function
po_add1 = po("module",
  id = "add_one",
  module = function(x) x + 1
)
input = list(torch_tensor(1))
po_add1$train(input)$output

</code></pre>


</div>