<div class="container">

<table style="width: 100%;"><tr>
<td>TorchOptimizer</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Torch Optimizer</h2>

<h3>Description</h3>

<p>This wraps a <code>torch::torch_optimizer_generator</code>a and annotates it with metadata, most importantly a <code>ParamSet</code>.
The optimizer is created for the given parameter values by calling the <code style="white-space: pre;">⁠$generate()⁠</code> method.
</p>
<p>This class is usually used to configure the optimizer of a torch learner, e.g.
when construcing a learner or in a <code>ModelDescriptor</code>.
</p>
<p>For a list of available optimizers, see <code>mlr3torch_optimizers</code>.
Items from this dictionary can be retrieved using <code>t_opt()</code>.
</p>


<h3>Parameters</h3>

<p>Defined by the constructor argument <code>param_set</code>.
If no parameter set is provided during construction, the parameter set is constructed by creating a parameter
for each argument of the wrapped loss function, where the parametes are then of type <code>ParamUty</code>.
</p>


<h3>Super class</h3>

<p><code>mlr3torch::TorchDescriptor</code> -&gt; <code>TorchOptimizer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TorchOptimizer-new"><code>TorchOptimizer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TorchOptimizer-generate"><code>TorchOptimizer$generate()</code></a>
</p>
</li>
<li> <p><a href="#method-TorchOptimizer-clone"><code>TorchOptimizer$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlr3torch" data-topic="TorchDescriptor" data-id="help"><a href="../../mlr3torch/html/TorchDescriptor.html#method-TorchDescriptor-help"><code>mlr3torch::TorchDescriptor$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3torch" data-topic="TorchDescriptor" data-id="print"><a href="../../mlr3torch/html/TorchDescriptor.html#method-TorchDescriptor-print"><code>mlr3torch::TorchDescriptor$print()</code></a></span></li>
</ul></details><hr>
<a id="method-TorchOptimizer-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TorchOptimizer$new(
  torch_optimizer,
  param_set = NULL,
  id = NULL,
  label = NULL,
  packages = NULL,
  man = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>torch_optimizer</code></dt>
<dd>
<p>(<code>torch_optimizer_generator</code>)<br>
The torch optimizer.</p>
</dd>
<dt><code>param_set</code></dt>
<dd>
<p>(<code>ParamSet</code> or <code>NULL</code>)<br>
The parameter set. If <code>NULL</code> (default) it is inferred from <code>torch_optimizer</code>.</p>
</dd>
<dt><code>id</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The id for of the new object.</p>
</dd>
<dt><code>label</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Label for the new instance.</p>
</dd>
<dt><code>packages</code></dt>
<dd>
<p>(<code>character()</code>)<br>
The R packages this object depends on.</p>
</dd>
<dt><code>man</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
String in the format <code style="white-space: pre;">⁠[pkg]::[topic]⁠</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">⁠$help()⁠</code>.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-TorchOptimizer-generate"></a>



<h4>Method <code>generate()</code>
</h4>

<p>Instantiates the optimizer.
</p>


<h5>Usage</h5>

<div class="r"><pre>TorchOptimizer$generate(params)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>params</code></dt>
<dd>
<p>(named <code>list()</code> of <code>torch_tensor</code>s)<br>
The parameters of the network.</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>torch_optimizer</code>
</p>


<hr>
<a id="method-TorchOptimizer-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TorchOptimizer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>See Also</h3>

<p>Other Torch Descriptor: 
<code>TorchCallback</code>,
<code>TorchDescriptor</code>,
<code>TorchLoss</code>,
<code>as_torch_callbacks()</code>,
<code>as_torch_loss()</code>,
<code>as_torch_optimizer()</code>,
<code>mlr3torch_losses</code>,
<code>mlr3torch_optimizers</code>,
<code>t_clbk()</code>,
<code>t_loss()</code>,
<code>t_opt()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
# Create a new torch loss
torch_opt = TorchOptimizer$new(optim_adam, label = "adam")
torch_opt
# If the param set is not specified, parameters are inferred but are of class ParamUty
torch_opt$param_set

# open the help page of the wrapped optimizer
# torch_opt$help()

# Retrieve an optimizer from the dictionary
torch_opt = t_opt("sgd", lr = 0.1)
torch_opt
torch_opt$param_set
torch_opt$label
torch_opt$id

# Create the optimizer for a network
net = nn_linear(10, 1)
opt = torch_opt$generate(net$parameters)

# is the same as
optim_sgd(net$parameters, lr = 0.1)

# Use in a learner
learner = lrn("regr.mlp", optimizer = t_opt("sgd"))
# The parameters of the optimizer are added to the learner's parameter set
learner$param_set

</code></pre>


</div>