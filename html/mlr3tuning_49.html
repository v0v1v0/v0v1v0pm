<div class="container">

<table style="width: 100%;"><tr>
<td>mlr_tuners_gensa</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Hyperparameter Tuning with Generalized Simulated Annealing</h2>

<h3>Description</h3>

<p>Subclass for generalized simulated annealing tuning.
Calls <code>GenSA::GenSA()</code> from package <a href="https://CRAN.R-project.org/package=GenSA"><span class="pkg">GenSA</span></a>.
</p>


<h3>Details</h3>

<p>In contrast to the <code>GenSA::GenSA()</code> defaults, we set <code>smooth = FALSE</code> as a default.
</p>


<h3>Dictionary</h3>

<p>This Tuner can be instantiated with the associated sugar function <code>tnr()</code>:
</p>
<div class="sourceCode"><pre>tnr("gensa")
</pre></div>


<h3>Parallelization</h3>

<p>In order to support general termination criteria and parallelization, we
evaluate points in a batch-fashion of size <code>batch_size</code>. Larger batches mean
we can parallelize more, smaller batches imply a more fine-grained checking
of termination criteria. A batch contains of <code>batch_size</code> times <code>resampling$iters</code> jobs.
E.g., if you set a batch size of 10 points and do a 5-fold cross validation, you can
utilize up to 50 cores.
</p>
<p>Parallelization is supported via package <a href="https://CRAN.R-project.org/package=future"><span class="pkg">future</span></a> (see <code>mlr3::benchmark()</code>'s
section on parallelization for more details).
</p>


<h3>Logging</h3>

<p>All Tuners use a logger (as implemented in <a href="https://CRAN.R-project.org/package=lgr"><span class="pkg">lgr</span></a>) from package
<a href="https://CRAN.R-project.org/package=bbotk"><span class="pkg">bbotk</span></a>.
Use <code>lgr::get_logger("bbotk")</code> to access and control the logger.
</p>


<h3>Optimizer</h3>

<p>This Tuner is based on bbotk::OptimizerBatchGenSA which can be applied on any black box optimization problem.
See also the documentation of <a href="https://bbotk.mlr-org.com/">bbotk</a>.
</p>


<h3>Parameters</h3>


<dl>
<dt><code>smooth</code></dt>
<dd>
<p><code>logical(1)</code></p>
</dd>
<dt><code>temperature</code></dt>
<dd>
<p><code>numeric(1)</code></p>
</dd>
<dt><code>acceptance.param</code></dt>
<dd>
<p><code>numeric(1)</code></p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p><code>logical(1)</code></p>
</dd>
<dt><code>trace.mat</code></dt>
<dd>
<p><code>logical(1)</code></p>
</dd>
</dl>
<p>For the meaning of the control parameters, see <code>GenSA::GenSA()</code>. Note that we
have removed all control parameters which refer to the termination of the
algorithm and where our terminators allow to obtain the same behavior.
</p>
<p>In contrast to the <code>GenSA::GenSA()</code> defaults, we set <code>trace.mat = FALSE</code>.
Note that <code>GenSA::GenSA()</code> uses <code>smooth = TRUE</code> as a default.
In the case of using this optimizer for Hyperparameter Optimization you may
want to set <code>smooth = FALSE</code>.
</p>


<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li>
<p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li>
<p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li>
</ul>
<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul><li>
<p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>
<h3>Progress Bars</h3>

<p><code style="white-space: pre;">⁠$optimize()⁠</code> supports progress bars via the package <a href="https://CRAN.R-project.org/package=progressr"><span class="pkg">progressr</span></a>
combined with a Terminator. Simply wrap the function in
<code>progressr::with_progress()</code> to enable them. We recommend to use package
<a href="https://CRAN.R-project.org/package=progress"><span class="pkg">progress</span></a> as backend; enable with <code>progressr::handlers("progress")</code>.
</p>


<h3>Super classes</h3>

<p><code>mlr3tuning::Tuner</code> -&gt; <code>mlr3tuning::TunerBatch</code> -&gt; <code>mlr3tuning::TunerBatchFromOptimizerBatch</code> -&gt; <code>TunerBatchGenSA</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatchGenSA-new"><code>TunerBatchGenSA$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatchGenSA-clone"><code>TunerBatchGenSA$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-format"><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-help"><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-print"><code>mlr3tuning::Tuner$print()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="TunerBatchFromOptimizerBatch" data-id="optimize"><a href="../../mlr3tuning/html/TunerBatchFromOptimizerBatch.html#method-TunerBatchFromOptimizerBatch-optimize"><code>mlr3tuning::TunerBatchFromOptimizerBatch$optimize()</code></a></span></li>
</ul></details><hr>
<a id="method-TunerBatchGenSA-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchGenSA$new()</pre></div>


<hr>
<a id="method-TunerBatchGenSA-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatchGenSA$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>Source</h3>

<p>Tsallis C, Stariolo DA (1996).
“Generalized simulated annealing.”
<em>Physica A: Statistical Mechanics and its Applications</em>, <b>233</b>(1-2), 395–406.
<a href="https://doi.org/10.1016/s0378-4371%2896%2900271-3">doi:10.1016/s0378-4371(96)00271-3</a>.
</p>
<p>Xiang Y, Gubian S, Suomela B, Hoeng J (2013).
“Generalized Simulated Annealing for Global Optimization: The GenSA Package.”
<em>The R Journal</em>, <b>5</b>(1), 13.
<a href="https://doi.org/10.32614/rj-2013-002">doi:10.32614/rj-2013-002</a>.
</p>


<h3>See Also</h3>

<p>Other Tuner: 
<code>Tuner</code>,
<code>mlr_tuners</code>,
<code>mlr_tuners_cmaes</code>,
<code>mlr_tuners_design_points</code>,
<code>mlr_tuners_grid_search</code>,
<code>mlr_tuners_internal</code>,
<code>mlr_tuners_irace</code>,
<code>mlr_tuners_nloptr</code>,
<code>mlr_tuners_random_search</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Hyperparameter Optimization

# load learner and set search space
learner = lrn("classif.rpart",
  cp = to_tune(1e-04, 1e-1, logscale = TRUE)
)

# run hyperparameter tuning on the Palmer Penguins data set
instance = tune(
  tuner = tnr("gensa"),
  task = tsk("penguins"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  term_evals = 10
)

# best performing hyperparameter configuration
instance$result

# all evaluated hyperparameter configuration
as.data.table(instance$archive)

# fit final model on complete data set
learner$param_set$values = instance$result_learner_param_vals
learner$train(tsk("penguins"))
</code></pre>


</div>