<div class="container">

<table style="width: 100%;"><tr>
<td>TunerBatch</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Class for Batch Tuning Algorithms</h2>

<h3>Description</h3>

<p>The TunerBatch implements the optimization algorithm.
</p>


<h3>Details</h3>

<p>TunerBatch is an abstract base class that implements the base functionality each tuner must provide.
A subclass is implemented in the following way:
</p>

<ul>
<li>
<p> Inherit from Tuner.
</p>
</li>
<li>
<p> Specify the private abstract method <code style="white-space: pre;">⁠$.optimize()⁠</code> and use it to call into your optimizer.
</p>
</li>
<li>
<p> You need to call <code>instance$eval_batch()</code> to evaluate design points.
</p>
</li>
<li>
<p> The batch evaluation is requested at the TuningInstanceBatchSingleCrit/TuningInstanceBatchMultiCrit object <code>instance</code>, so each batch is possibly executed in parallel via <code>mlr3::benchmark()</code>, and all evaluations are stored inside of <code>instance$archive</code>.
</p>
</li>
<li>
<p> Before the batch evaluation, the bbotk::Terminator is checked, and if it is positive, an exception of class <code>"terminated_error"</code> is generated.
In the  later case the current batch of evaluations is still stored in <code>instance</code>, but the numeric scores are not sent back to the handling optimizer as it has lost execution control.
</p>
</li>
<li>
<p> After such an exception was caught we select the best configuration from <code>instance$archive</code> and return it.
</p>
</li>
<li>
<p> Note that therefore more points than specified by the bbotk::Terminator may be evaluated, as the Terminator is only checked before a batch evaluation, and not in-between evaluation in a batch.
How many more depends on the setting of the batch size.
</p>
</li>
<li>
<p> Overwrite the private super-method <code>.assign_result()</code> if you want to decide yourself how to estimate the final configuration in the instance and its estimated performance.
The default behavior is: We pick the best resample-experiment, regarding the given measure, then assign its configuration and aggregated performance to the instance.
</p>
</li>
</ul>
<h3>Private Methods</h3>


<ul>
<li> <p><code>.optimize(instance)</code> -&gt; <code>NULL</code><br>
Abstract base method. Implement to specify tuning of your subclass.
See details sections.
</p>
</li>
<li> <p><code>.assign_result(instance)</code> -&gt; <code>NULL</code><br>
Abstract base method. Implement to specify how the final configuration is selected.
See details sections.
</p>
</li>
</ul>
<h3>Resources</h3>

<p>There are several sections about hyperparameter optimization in the <a href="https://mlr3book.mlr-org.com">mlr3book</a>.
</p>

<ul>
<li>
<p> An overview of all tuners can be found on our <a href="https://mlr-org.com/tuners.html">website</a>.
</p>
</li>
<li>
<p> Learn more about <a href="https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-tuner">tuners</a>.
</p>
</li>
</ul>
<p>The <a href="https://mlr-org.com/gallery-all-optimization.html">gallery</a> features a collection of case studies and demos about optimization.
</p>

<ul><li>
<p> Use the <a href="https://mlr-org.com/gallery/series/2023-01-15-hyperband-xgboost/">Hyperband</a> optimizer with different budget parameters.
</p>
</li></ul>
<h3>Super class</h3>

<p><code>mlr3tuning::Tuner</code> -&gt; <code>TunerBatch</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TunerBatch-new"><code>TunerBatch$new()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatch-optimize"><code>TunerBatch$optimize()</code></a>
</p>
</li>
<li> <p><a href="#method-TunerBatch-clone"><code>TunerBatch$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="format"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-format"><code>mlr3tuning::Tuner$format()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="help"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-help"><code>mlr3tuning::Tuner$help()</code></a></span></li>
<li><span class="pkg-link" data-pkg="mlr3tuning" data-topic="Tuner" data-id="print"><a href="../../mlr3tuning/html/Tuner.html#method-Tuner-print"><code>mlr3tuning::Tuner$print()</code></a></span></li>
</ul></details><hr>
<a id="method-TunerBatch-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Creates a new instance of this R6 class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatch$new(
  id = "tuner_batch",
  param_set,
  param_classes,
  properties,
  packages = character(),
  label = NA_character_,
  man = NA_character_
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>id</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Identifier for the new instance.</p>
</dd>
<dt><code>param_set</code></dt>
<dd>
<p>(paradox::ParamSet)<br>
Set of control parameters.</p>
</dd>
<dt><code>param_classes</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Supported parameter classes for learner hyperparameters that the tuner can optimize, as given in the paradox::ParamSet <code style="white-space: pre;">⁠$class⁠</code> field.</p>
</dd>
<dt><code>properties</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Set of properties of the tuner.
Must be a subset of <code>mlr_reflections$tuner_properties</code>.</p>
</dd>
<dt><code>packages</code></dt>
<dd>
<p>(<code>character()</code>)<br>
Set of required packages.
Note that these packages will be loaded via <code>requireNamespace()</code>, and are not attached.</p>
</dd>
<dt><code>label</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Label for this object.
Can be used in tables, plot and text output instead of the ID.</p>
</dd>
<dt><code>man</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
String in the format <code style="white-space: pre;">⁠[pkg]::[topic]⁠</code> pointing to a manual page for this object.
The referenced help package can be opened via method <code style="white-space: pre;">⁠$help()⁠</code>.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-TunerBatch-optimize"></a>



<h4>Method <code>optimize()</code>
</h4>

<p>Performs the tuning on a TuningInstanceBatchSingleCrit or TuningInstanceBatchMultiCrit until termination.
The single evaluations will be written into the ArchiveBatchTuning that resides in the TuningInstanceBatchSingleCrit/TuningInstanceBatchMultiCrit.
The result will be written into the instance object.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatch$optimize(inst)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inst</code></dt>
<dd>
<p>(TuningInstanceBatchSingleCrit | TuningInstanceBatchMultiCrit).</p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p><code>data.table::data.table()</code>
</p>


<hr>
<a id="method-TunerBatch-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TunerBatch$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




</div>