<div class="container">

<table style="width: 100%;"><tr>
<td>benchmark</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Benchmark experiment for multiple learners and tasks.</h2>

<h3>Description</h3>

<p>Complete benchmark experiment to compare different learning algorithms across one or more tasks
w.r.t. a given resampling strategy. Experiments are paired, meaning always the same
training / test sets are used for the different learners.
Furthermore, you can of course pass “enhanced” learners via wrappers, e.g., a
learner can be automatically tuned using makeTuneWrapper.
</p>


<h3>Usage</h3>

<pre><code class="language-R">benchmark(
  learners,
  tasks,
  resamplings,
  measures,
  keep.pred = TRUE,
  keep.extract = FALSE,
  models = FALSE,
  show.info = getMlrOption("show.info")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>learners</code></td>
<td>
<p>(list of Learner | character)<br>
Learning algorithms which should be compared, can also be a single learner.
If you pass strings the learners will be created via makeLearner.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tasks</code></td>
<td>
<p>list of Task<br>
Tasks that learners should be run on.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>resamplings</code></td>
<td>
<p>(list of ResampleDesc | ResampleInstance)<br>
Resampling strategy for each tasks.
If only one is provided, it will be replicated to match the number of tasks.
If missing, a 10-fold cross validation is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measures</code></td>
<td>
<p>(list of Measure)<br>
Performance measures for all tasks.
If missing, the default measure of the first task is used.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.pred</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Keep the prediction data in the <code>pred</code> slot of the result object.
If you do many experiments (on larger data sets) these objects might unnecessarily increase
object size / mem usage, if you do not really need them.
The default is set to <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.extract</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Keep the <code>extract</code> slot of the result object. When creating a lot of
benchmark results with extensive tuning, the resulting R objects can become
very large in size. That is why the tuning results stored in the <code>extract</code>
slot are removed by default (<code>keep.extract = FALSE</code>). Note that when
<code>keep.extract = FALSE</code> you will not be able to conduct analysis in the
tuning results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>models</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Should all fitted models be stored in the ResampleResult?
Default is <code>FALSE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show.info</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Print verbose output on console?
Default is set via configureMlr.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>BenchmarkResult.
</p>


<h3>See Also</h3>

<p>Other benchmark: 
<code>BenchmarkResult</code>,
<code>batchmark()</code>,
<code>convertBMRToRankMatrix()</code>,
<code>friedmanPostHocTestBMR()</code>,
<code>friedmanTestBMR()</code>,
<code>generateCritDifferencesData()</code>,
<code>getBMRAggrPerformances()</code>,
<code>getBMRFeatSelResults()</code>,
<code>getBMRFilteredFeatures()</code>,
<code>getBMRLearnerIds()</code>,
<code>getBMRLearnerShortNames()</code>,
<code>getBMRLearners()</code>,
<code>getBMRMeasureIds()</code>,
<code>getBMRMeasures()</code>,
<code>getBMRModels()</code>,
<code>getBMRPerformances()</code>,
<code>getBMRPredictions()</code>,
<code>getBMRTaskDescs()</code>,
<code>getBMRTaskIds()</code>,
<code>getBMRTuneResults()</code>,
<code>plotBMRBoxplots()</code>,
<code>plotBMRRanksAsBarChart()</code>,
<code>plotBMRSummary()</code>,
<code>plotCritDifferences()</code>,
<code>reduceBatchmarkResults()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">



lrns = list(makeLearner("classif.lda"), makeLearner("classif.rpart"))
tasks = list(iris.task, sonar.task)
rdesc = makeResampleDesc("CV", iters = 2L)
meas = list(acc, ber)
bmr = benchmark(lrns, tasks, rdesc, measures = meas)
rmat = convertBMRToRankMatrix(bmr)
print(rmat)
plotBMRSummary(bmr)
plotBMRBoxplots(bmr, ber, style = "violin")
plotBMRRanksAsBarChart(bmr, pos = "stack")
friedmanTestBMR(bmr)
friedmanPostHocTestBMR(bmr, p.value = 0.05)




</code></pre>


</div>