<div class="container">

<table style="width: 100%;"><tr>
<td>generateLearningCurveData</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Generates a learning curve.</h2>

<h3>Description</h3>

<p>Observe how the performance changes with an increasing number of observations.
</p>


<h3>Usage</h3>

<pre><code class="language-R">generateLearningCurveData(
  learners,
  task,
  resampling = NULL,
  percs = seq(0.1, 1, by = 0.1),
  measures,
  stratify = FALSE,
  show.info = getMlrOption("show.info")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>learners</code></td>
<td>
<p>[(list of) Learner)<br>
Learning algorithms which should be compared.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>task</code></td>
<td>
<p>(Task)<br>
The task.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>resampling</code></td>
<td>
<p>(ResampleDesc | ResampleInstance)<br>
Resampling strategy to evaluate the performance measure.
If no strategy is given a default "Holdout" will be performed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>percs</code></td>
<td>
<p>(numeric)<br>
Vector of percentages to be drawn from the training split.
These values represent the x-axis.
Internally makeDownsampleWrapper is used in combination with benchmark.
Thus for each percentage a different set of observations is drawn resulting in noisy performance measures as the quality of the sample can differ.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>measures</code></td>
<td>
<p>[(list of) Measure)<br>
Performance measures to generate learning curves for, representing the y-axis.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stratify</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Only for classification:
Should the downsampled data be stratified according to the target classes?</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show.info</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Print verbose output on console?
Default is set via configureMlr.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>(LearningCurveData). A <code>list</code> containing:
</p>

<ul>
<li>
<p> The Task
</p>
</li>
<li>
<p> List of Measure)<br>
Performance measures
</p>
</li>
<li>
<p> data (data.frame) with columns:
</p>

<ul>
<li> <p><code>learner</code> Names of learners.
</p>
</li>
<li> <p><code>percentage</code> Percentages drawn from the training split.
</p>
</li>
<li>
<p> One column for each Measure passed to generateLearningCurveData.
</p>
</li>
</ul>
</li>
</ul>
<h3>See Also</h3>

<p>Other generate_plot_data: 
<code>generateCalibrationData()</code>,
<code>generateCritDifferencesData()</code>,
<code>generateFeatureImportanceData()</code>,
<code>generateFilterValuesData()</code>,
<code>generatePartialDependenceData()</code>,
<code>generateThreshVsPerfData()</code>,
<code>plotFilterValues()</code>
</p>
<p>Other learning_curve: 
<code>plotLearningCurve()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">

r = generateLearningCurveData(list("classif.rpart", "classif.knn"),
  task = sonar.task, percs = seq(0.2, 1, by = 0.2),
  measures = list(tp, fp, tn, fn),
  resampling = makeResampleDesc(method = "Subsample", iters = 5),
  show.info = FALSE)
plotLearningCurve(r)


</code></pre>


</div>