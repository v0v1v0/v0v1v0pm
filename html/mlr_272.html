<div class="container">

<table style="width: 100%;"><tr>
<td>makeMultilabelBinaryRelevanceWrapper</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Use binary relevance method to create a multilabel learner.</h2>

<h3>Description</h3>

<p>Every learner which is implemented in mlr and which supports binary
classification can be converted to a wrapped binary relevance multilabel learner.
The multilabel classification problem is converted into simple binary classifications
for each label/target on which the binary learner is applied.
</p>
<p>Models can easily be accessed via getLearnerModel.
</p>
<p>Note that it does not make sense to set a threshold in the used base <code>learner</code>
when you predict probabilities.
On the other hand, it can make a lot of sense, to call setThreshold
on the <code>MultilabelBinaryRelevanceWrapper</code> for each label indvidually;
Or to tune these thresholds with tuneThreshold; especially when you face very
unabalanced class distributions for each binary label.
</p>


<h3>Usage</h3>

<pre><code class="language-R">makeMultilabelBinaryRelevanceWrapper(learner)
</code></pre>


<h3>Arguments</h3>

<table><tr style="vertical-align: top;">
<td><code>learner</code></td>
<td>
<p>(Learner | <code>character(1)</code>)<br>
The learner.
If you pass a string the learner will be created via makeLearner.</p>
</td>
</tr></table>
<h3>Value</h3>

<p>Learner.
</p>


<h3>References</h3>

<p>Tsoumakas, G., &amp; Katakis, I. (2006)
<em>Multi-label classification: An overview.</em>
Dept. of Informatics, Aristotle University of Thessaloniki, Greece.
</p>


<h3>See Also</h3>

<p>Other wrapper: 
<code>makeBaggingWrapper()</code>,
<code>makeClassificationViaRegressionWrapper()</code>,
<code>makeConstantClassWrapper()</code>,
<code>makeCostSensClassifWrapper()</code>,
<code>makeCostSensRegrWrapper()</code>,
<code>makeDownsampleWrapper()</code>,
<code>makeDummyFeaturesWrapper()</code>,
<code>makeExtractFDAFeatsWrapper()</code>,
<code>makeFeatSelWrapper()</code>,
<code>makeFilterWrapper()</code>,
<code>makeImputeWrapper()</code>,
<code>makeMulticlassWrapper()</code>,
<code>makeMultilabelClassifierChainsWrapper()</code>,
<code>makeMultilabelDBRWrapper()</code>,
<code>makeMultilabelNestedStackingWrapper()</code>,
<code>makeMultilabelStackingWrapper()</code>,
<code>makeOverBaggingWrapper()</code>,
<code>makePreprocWrapper()</code>,
<code>makePreprocWrapperCaret()</code>,
<code>makeRemoveConstantFeaturesWrapper()</code>,
<code>makeSMOTEWrapper()</code>,
<code>makeTuneWrapper()</code>,
<code>makeUndersampleWrapper()</code>,
<code>makeWeightedClassesWrapper()</code>
</p>
<p>Other multilabel: 
<code>getMultilabelBinaryPerformances()</code>,
<code>makeMultilabelClassifierChainsWrapper()</code>,
<code>makeMultilabelDBRWrapper()</code>,
<code>makeMultilabelNestedStackingWrapper()</code>,
<code>makeMultilabelStackingWrapper()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">if (requireNamespace("rpart")) {
d = getTaskData(yeast.task)
# drop some labels so example runs faster
d = d[seq(1, nrow(d), by = 20), c(1:2, 15:17)]
task = makeMultilabelTask(data = d, target = c("label1", "label2"))
lrn = makeLearner("classif.rpart")
lrn = makeMultilabelBinaryRelevanceWrapper(lrn)
lrn = setPredictType(lrn, "prob")
# train, predict and evaluate
mod = train(lrn, task)
pred = predict(mod, task)
performance(pred, measure = list(multilabel.hamloss, multilabel.subset01, multilabel.f1))
# the next call basically has the same structure for any multilabel meta wrapper
getMultilabelBinaryPerformances(pred, measures = list(mmce, auc))
# above works also with predictions from resample!
}
</code></pre>


</div>