<div class="container">

<table style="width: 100%;"><tr>
<td>reduceBatchmarkResults</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Reduce results of a batch-distributed benchmark.</h2>

<h3>Description</h3>

<p>This creates a BenchmarkResult from a batchtools::ExperimentRegistry.
To setup the benchmark have a look at batchmark.
</p>


<h3>Usage</h3>

<pre><code class="language-R">reduceBatchmarkResults(
  ids = NULL,
  keep.pred = TRUE,
  keep.extract = FALSE,
  show.info = getMlrOption("show.info"),
  reg = batchtools::getDefaultRegistry()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ids</code></td>
<td>
<p>(data.frame or integer)<br>
A base::data.frame (or data.table::data.table)
with a column named “job.id”.
Alternatively, you may also pass a vector of integerish job ids.
If not set, defaults to all successfully terminated jobs (return value of batchtools::findDone.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.pred</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Keep the prediction data in the <code>pred</code> slot of the result object.
If you do many experiments (on larger data sets) these objects might unnecessarily increase
object size / mem usage, if you do not really need them.
The default is set to <code>TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>keep.extract</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Keep the <code>extract</code> slot of the result object. When creating a lot of
benchmark results with extensive tuning, the resulting R objects can become
very large in size. That is why the tuning results stored in the <code>extract</code>
slot are removed by default (<code>keep.extract = FALSE</code>). Note that when
<code>keep.extract = FALSE</code> you will not be able to conduct analysis in the
tuning results.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show.info</code></td>
<td>
<p>(<code>logical(1)</code>)<br>
Print verbose output on console?
Default is set via configureMlr.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reg</code></td>
<td>
<p>(batchtools::ExperimentRegistry)<br>
Registry, created by batchtools::makeExperimentRegistry. If not explicitly passed,
uses the last created registry.</p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>(BenchmarkResult).
</p>


<h3>See Also</h3>

<p>Other benchmark: 
<code>BenchmarkResult</code>,
<code>batchmark()</code>,
<code>benchmark()</code>,
<code>convertBMRToRankMatrix()</code>,
<code>friedmanPostHocTestBMR()</code>,
<code>friedmanTestBMR()</code>,
<code>generateCritDifferencesData()</code>,
<code>getBMRAggrPerformances()</code>,
<code>getBMRFeatSelResults()</code>,
<code>getBMRFilteredFeatures()</code>,
<code>getBMRLearnerIds()</code>,
<code>getBMRLearnerShortNames()</code>,
<code>getBMRLearners()</code>,
<code>getBMRMeasureIds()</code>,
<code>getBMRMeasures()</code>,
<code>getBMRModels()</code>,
<code>getBMRPerformances()</code>,
<code>getBMRPredictions()</code>,
<code>getBMRTaskDescs()</code>,
<code>getBMRTaskIds()</code>,
<code>getBMRTuneResults()</code>,
<code>plotBMRBoxplots()</code>,
<code>plotBMRRanksAsBarChart()</code>,
<code>plotBMRSummary()</code>,
<code>plotCritDifferences()</code>
</p>


</div>