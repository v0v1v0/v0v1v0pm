<div class="container">

<table style="width: 100%;"><tr>
<td>regress</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2> Switch function for least squares and parsimonious monomvn regressions </h2>

<h3>Description</h3>

<p>This function fits the specified ordinary least squares or
parsimonious regression (plsr, pcr, ridge, and lars methods)
depending on the arguments provided, and returns estimates of
coefficients and (co-)variances in a <code>monomvn</code> friendly
format
</p>


<h3>Usage</h3>

<pre><code class="language-R">regress(X, y, method = c("lsr", "plsr", "pcr", "lasso", "lar",
     "forward.stagewise", "stepwise", "ridge", "factor"), p = 0,
     ncomp.max = Inf, validation = c("CV", "LOO", "Cp"),
     verb = 0, quiet = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p><code>data.frame</code>, <code>matrix</code>, or vector of inputs <code>X</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p> matrix of responses <code>y</code> of row-length equal to the
leading dimension (rows) of <code>X</code>, i.e., <code>nrow(y) ==
      nrow(X)</code>; if <code>y</code> is a vector, then <code>nrow</code> may be
interpreted as <code>length</code> </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>
<p> describes the type of <em>parsimonious</em>
(or <em>shrinkage</em>) regression, or ordinary least squares.
From the <span class="pkg">pls</span> package we have <code>"plsr"</code>
(plsr, the default) for  partial least squares and
<code>"pcr"</code> (pcr) for standard principal
component regression.  From the <span class="pkg">lars</span> package (see the
<code>"type"</code> argument to lars)
we have <code>"lasso"</code> for L1-constrained regression, <code>"lar"</code>
for least angle regression, <code>"forward.stagewise"</code> and
<code>"stepwise"</code> for fast implementations of classical forward
selection of covariates.  From the <span class="pkg">MASS</span> package we have
<code>"ridge"</code> as implemented by the <code>lm.ridge</code>
function.  The <code>"factor"</code> method treats the first <code>p</code>
columns of <code>y</code> as known factors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p> when performing regressions, <code>0 &lt;= p &lt;= 1</code>
is the proportion of the
number of columns to rows in the design matrix before an
alternative regression <code>method</code> (except <code>"lsr"</code>)
is performed as if  least-squares regression “failed”.
Least-squares regression is
known to fail when the number of columns is greater than or
equal to the number of rows.
The default setting, <code>p = 0</code>, forces the specified
<code>method</code> to be used for <em>every</em> regression unless
<code>method = "lsr"</code> is specified but is unstable.
Intermediate settings of <code>p</code> allow the user
to specify that least squares regressions are preferred only
when there are sufficiently more rows in the design matrix
(<code>X</code>) than columns. When <code>method = "factor"</code> the <code>p</code>
argument represents an integer (positive) number of initial columns
of <code>y</code> to treat as known factors</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncomp.max</code></td>
<td>
<p> maximal number of (principal) components to consider
in a <code>method</code>—only meaningful for the <code>"plsr"</code> or
<code>"pcr"</code> methods.  Large settings can cause the execution to be
slow as they drastically increase the cross-validation (CV) time</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>validation</code></td>
<td>
<p> method for cross validation when applying 
a <em>parsimonious</em> regression method.  The default setting
of <code>"CV"</code> (randomized 10-fold cross-validation) is the faster method, 
but does not yield a deterministic result and does not apply for
regressions on less than ten responses. <code>"LOO"</code>
(leave-one-out cross-validation)
is deterministic, always applicable, and applied automatically whenever 
<code>"CV"</code> cannot be used.  When standard least squares is
appropriate, the methods implemented the
<span class="pkg">lars</span> package (e.g. lasso) support model choice via the
<code>"Cp"</code> statistic, which defaults to the <code>"CV"</code> method
when least squares fails.  This argument is ignored for the
<code>"ridge"</code> method; see details below</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>verb</code></td>
<td>
<p> whether or not to print progress indicators.  The default
(<code>verb = 0</code>) keeps quiet.  This argument is provided for
<code>monomvn</code> and is not intended to be set by the user
via this interface </p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>quiet</code></td>
<td>
<p> causes <code>warning</code>s about regressions to be silenced
when <code>TRUE</code></p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>All <code>method</code>s (except <code>"lsr"</code>) require a scheme for
estimating the amount of variability explained by increasing numbers
of non-zero coefficients (or principal components) in the model.
Towards this end, the <span class="pkg">pls</span> and <span class="pkg">lars</span> packages support
10-fold cross validation (CV) or leave-one-out (LOO) CV estimates of
root mean squared error.  See <span class="pkg">pls</span> and <span class="pkg">lars</span> for
more details.  The <code>regress</code> function uses CV in all cases
except when <code>nrow(X) &lt;= 10</code>, in which case CV fails and
LOO is used.  Whenever <code>nrow(X) &lt;= 3</code> <code>pcr</code>
fails,  so <code>plsr</code> is used instead.
If <code>quiet = FALSE</code> then a <code>warning</code>
is given whenever the first choice for a regression fails.
</p>
<p>For <span class="pkg">pls</span> methods, RMSEs
are calculated for a number of components in <code>1:ncomp.max</code> where
a <code>NULL</code> value for <code>ncomp.max</code> it is replaced with
</p>
<p><code>ncomp.max &lt;- min(ncomp.max, ncol(y), nrow(X)-1)</code>
</p>
<p>which is the max allowed by the <span class="pkg">pls</span> package.
</p>
<p>Simple heuristics are used to select a small number of components
(<code>ncomp</code> for <span class="pkg">pls</span>), or number of coefficients (for
<span class="pkg">lars</span>) which explains a large amount of the variability (RMSE).
The <span class="pkg">lars</span> methods use a “one-standard error rule” outlined
in Section 7.10, page 216 of HTF below.  The
<span class="pkg">pls</span> package does not currently support the calculation of
standard errors for CV estimates of RMSE, so a simple linear penalty
for increasing <code>ncomp</code> is used instead.  The ridge constant
(lambda) for <code>lm.ridge</code> is set using the <code>optimize</code>
function on the <code>GCV</code> output.
</p>


<h3>Value</h3>

<p><code>regress</code> returns a <code>list</code> containing
the components listed below.
</p>
<table>
<tr style="vertical-align: top;">
<td><code>call </code></td>
<td>
<p>a copy of the function call as used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method </code></td>
<td>
<p>a copy of the <code>method</code> input argument</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncomp </code></td>
<td>
<p>depends on the <code>method</code> used: is <code>NA</code> when
<code>method = "lsr"</code>; is the number of principal
components for <code>method = "pcr"</code> and <code>method = "plsr"</code>;
is the number of non-zero components in the coefficient vector
(<code>$b</code>, not counting the intercept) for any of the
<code>lars</code> methods; and gives the chosen
<code class="reqn">\lambda</code> penalty parameter for <code>method = "ridge"</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda </code></td>
<td>
<p>if <code>method</code> is one of <code>c("lasso",
      "forward.stagewise", "ridge")</code>, then this field records the
<code class="reqn">\lambda</code> penalty parameter used</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>b </code></td>
<td>
<p>matrix containing the estimated regression coefficients,
with <code>ncol(b) = ncol(y)</code> and the intercept
in the first row</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>S </code></td>
<td>
<p>(biased corrected) maximum likelihood estimate of residual
covariance matrix</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>The CV in <span class="pkg">plsr</span> and <span class="pkg">lars</span> are random in nature, and so
can be dependent on the random seed.  Use <code>validation="LOO"</code> for
deterministic (but slower) result
</p>
<p>Be warned that the <span class="pkg">lars</span> implementation of
<code>"forward.stagewise"</code> can sometimes get stuck in
(what seems like) an infinite loop.
This is not a bug in the <code>regress</code> function;
the bug has been reported to the authors of <span class="pkg">lars</span>
</p>


<h3>Author(s)</h3>

<p> Robert B. Gramacy <a href="mailto:rbg@vt.edu">rbg@vt.edu</a> </p>


<h3>References</h3>

<p>Bjorn-Helge Mevik and Ron Wehrens (2007).
<em>The <span class="pkg">pls</span> Package: Principal Component and Partial
Least Squares Regression in R.</em> 
Journal of Statistical Software <b>18</b>(2)
</p>
<p>Bradley Efron, Trevor Hastie, Ian Johnstone and Robert Tibshirani
(2003).
<em>Least Angle Regression (with discussion).</em>
Annals of Statistics <b>32</b>(2); see also <br><a href="https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf">https://hastie.su.domains/Papers/LARS/LeastAngle_2002.pdf</a>
</p>
<p><a href="https://bobby.gramacy.com/r_packages/monomvn/">https://bobby.gramacy.com/r_packages/monomvn/</a>
</p>


<h3>See Also</h3>

 <p><code>monomvn</code>, <code>blasso</code>,
<code>lars</code> in the <span class="pkg">lars</span> library,
<code>lm.ridge</code> in the <span class="pkg">MASS</span> library,
<code>plsr</code> and <code>pcr</code> in the
<span class="pkg">pls</span> library
</p>


<h3>Examples</h3>

<pre><code class="language-R">## following the lars diabetes example
data(diabetes)
attach(diabetes)

## Ordinary Least Squares regression
reg.ols &lt;- regress(x, y)

## Lasso regression
reg.lasso &lt;- regress(x, y, method="lasso")

## partial least squares regression
reg.plsr &lt;- regress(x, y, method="plsr")

## ridge regression
reg.ridge &lt;- regress(x, y, method="ridge")

## compare the coefs
data.frame(ols=reg.ols$b, lasso=reg.lasso$b,
           plsr=reg.plsr$b, ridge=reg.ridge$b)

## summarize the posterior distribution of lambda2 and s2
detach(diabetes)
</code></pre>


</div>