<div class="container">

<table style="width: 100%;"><tr>
<td>igd</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Inverted Generational Distance (IGD and IGD+) and Averaged Hausdorff Distance</h2>

<h3>Description</h3>

<p>Functions to compute the inverted generational distance (IGD and IGD+) and
the averaged Hausdorff distance between nondominated sets of points.
</p>


<h3>Usage</h3>

<pre><code class="language-R">igd(x, reference, maximise = FALSE)

igd_plus(x, reference, maximise = FALSE)

avg_hausdorff_dist(x, reference, maximise = FALSE, p = 1L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p><code>matrix()</code>|<code>data.frame()</code><br> Matrix or data frame of numerical
values, where each row gives the coordinates of a point.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>reference</code></td>
<td>
<p><code>matrix</code>|<code>data.frame</code><br> Reference set as a matrix or
data.frame of numerical values.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maximise</code></td>
<td>
<p><code>logical()</code><br> Whether the objectives must be maximised
instead of minimised. Either a single logical value that applies to all
objectives or a vector of logical values, with one value per objective.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>p</code></td>
<td>
<p><code>integer(1)</code><br> Hausdorff distance parameter (default: <code>1L</code>).</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The generational distance (GD) of a set <code class="reqn">A</code> is defined as the distance
between each point <code class="reqn">a \in A</code> and the closest point <code class="reqn">r</code> in a
reference set <code class="reqn">R</code>, averaged over the size of <code class="reqn">A</code>. Formally,
</p>
<p style="text-align: center;"><code class="reqn">GD_p(A,R) = \left(\frac{1}{|A|}\sum_{a\in A}\min_{r\in R} d(a,r)^p\right)^{\frac{1}{p}} </code>
</p>

<p>where the distance in our implementation is the Euclidean distance:
</p>
<p style="text-align: center;"><code class="reqn">d(a,r) = \sqrt{\sum_{k=1}^M (a_k - r_k)^2} </code>
</p>

<p>The inverted generational distance (IGD) is calculated as <code class="reqn">IGD_p(A,R) = GD_p(R,A)</code>.
</p>
<p>The modified inverted generational distanced (IGD+) was proposed by
Ishibuchi et al. (2015) to ensure that IGD+ is weakly Pareto compliant,
similarly to <code>epsilon_additive()</code> or <code>epsilon_mult()</code>. It modifies the
distance measure as:
</p>
<p style="text-align: center;"><code class="reqn">d^+(r,a) = \sqrt{\sum_{k=1}^M (\max\{r_k - a_k, 0\})^2}</code>
</p>

<p>The average Hausdorff distance (<code class="reqn">\Delta_p</code>) was proposed by
Schütze et al. (2012) and it is calculated as:
</p>
<p style="text-align: center;"><code class="reqn">\Delta_p(A,R) = \max\{ IGD_p(A,R), IGD_p(R,A) \}</code>
</p>

<p>IGDX (Zhou et al. 2009) is the application of IGD to decision vectors
instead of objective vectors to measure closeness and diversity in decision
space. One can use the functions <code>igd()</code> or <code>igd_plus()</code> (recommended)
directly, just passing the decision vectors as <code>data</code>.
</p>
<p>There are different formulations of the GD and IGD metrics in the literature
that differ on the value of <code class="reqn">p</code>, on the distance metric used and on
whether the term <code class="reqn">|A|^{-1}</code> is inside (as above) or outside the exponent
<code class="reqn">1/p</code>.  GD was first proposed by Van Veldhuizen and Lamont (1998) with <code class="reqn">p=2</code> and
the term <code class="reqn">|A|^{-1}</code> outside the exponent. IGD seems to have been
mentioned first by Coello Coello and Reyes-Sierra (2004), however, some people also used the
name D-metric for the same concept with <code class="reqn">p=1</code> and later papers have
often used IGD/GD with <code class="reqn">p=1</code>. Schütze et al. (2012) proposed to
place the term <code class="reqn">|A|^{-1}</code> inside the exponent, as in the formulation
shown above.  This has a significant effect for GD and less so for IGD given
a constant reference set. IGD+ also follows this formulation.  We refer to
Ishibuchi et al. (2015) and Bezerra et al. (2017) for a more detailed
historical perspective and a comparison of the various variants.
</p>
<p>Following Ishibuchi et al. (2015), we always use <code class="reqn">p=1</code> in our
implementation of IGD and IGD+ because (1) it is the setting most used in
recent works; (2) it makes irrelevant whether the term <code class="reqn">|A|^{-1}</code> is
inside or outside the exponent <code class="reqn">1/p</code>; and (3) the meaning of IGD becomes
the average Euclidean distance from each reference point to its nearest
objective vector. It is also slightly faster to compute.
</p>
<p>GD should never be used directly to compare the quality of approximations to
a Pareto front, as it often contradicts Pareto optimality (it is not weakly
Pareto-compliant). We recommend IGD+ instead of IGD, since the latter
contradicts Pareto optimality in some cases (see examples below) whereas
IGD+ is weakly Pareto-compliant, but we implement IGD here because it is
still popular due to historical reasons.
</p>
<p>The average Hausdorff distance (<code class="reqn">\Delta_p(A,R)</code>) is also not weakly
Pareto-compliant, as shown in the examples below.
</p>


<h3>Value</h3>

<p><code>numeric(1)</code><br> A single numerical value.
</p>


<h3>Author(s)</h3>

<p>Manuel López-Ibáñez
</p>


<h3>References</h3>

<p>Leonardo
C.
T. Bezerra, Manuel López-Ibáñez, Thomas Stützle (2017).
“An Empirical Assessment of the Properties of Inverted Generational Distance Indicators on Multi- and Many-objective Optimization.”
In Heike Trautmann, Günter Rudolph, Kathrin Klamroth, Oliver Schütze, Margaret
M. Wiecek, Yaochu Jin, Christian Grimme (eds.), <em> Evolutionary Multi-criterion Optimization, EMO 2017</em>, volume 10173 of <em>Lecture Notes in Computer Science</em>, 31–45.
Springer International Publishing,  Cham, Switzerland.
doi: <a href="https://doi.org/10.1007/978-3-319-54157-0_3">10.1007/978-3-319-54157-0_3</a>.<br><br> Carlos
A. Coello Coello, Margarita Reyes-Sierra (2004).
“A Study of the Parallelization of a Coevolutionary Multi-objective Evolutionary Algorithm.”
In Raúl Monroy, Gustavo Arroyo-Figueroa, Luis
Enrique Sucar, Humberto Sossa (eds.), <em>Proceedings of MICAI</em>, volume 2972 of <em>Lecture Notes in Artificial Intelligence</em>, 688–697.
Springer,  Heidelberg, Germany.<br><br> Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, Yusuke Nojima (2015).
“Modified Distance Calculation in Generational Distance and Inverted Generational Distance.”
In António Gaspar-Cunha, Carlos
Henggeler Antunes, Carlos
A. Coello Coello (eds.), <em> Evolutionary Multi-criterion Optimization, EMO 2015 Part I</em>, volume 9018 of <em>Lecture Notes in Computer Science</em>, 110–125.
Springer,  Heidelberg, Germany.<br><br> Oliver Schütze, X Esquivel, A Lara, Carlos
A. Coello Coello (2012).
“Using the Averaged Hausdorff Distance as a Performance Measure in Evolutionary Multiobjective Optimization.”
<em>IEEE Transactions on Evolutionary Computation</em>, <b>16</b>(4), 504–522.<br><br> David
A. Van Veldhuizen, Gary
B. Lamont (1998).
“Evolutionary Computation and Convergence to a Pareto Front.”
In John
R. Koza (ed.), <em>Late Breaking Papers at the Genetic Programming 1998 Conference</em>, 221–228.<br><br> A Zhou, Qingfu Zhang, Yaochu Jin (2009).
“Approximating the set of Pareto-optimal solutions in both the decision and objective spaces by an estimation of distribution algorithm.”
<em>IEEE Transactions on Evolutionary Computation</em>, <b>13</b>(5), 1167–1189.
doi: <a href="https://doi.org/10.1109/TEVC.2009.2021467">10.1109/TEVC.2009.2021467</a>.
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Example 4 from Ishibuchi et al. (2015)
ref &lt;- matrix(c(10,0,6,1,2,2,1,6,0,10), ncol=2, byrow=TRUE)
A &lt;- matrix(c(4,2,3,3,2,4), ncol=2, byrow=TRUE)
B &lt;- matrix(c(8,2,4,4,2,8), ncol=2, byrow=TRUE)
if (requireNamespace("graphics", quietly = TRUE)) {
   plot(ref, xlab=expression(f[1]), ylab=expression(f[2]),
        panel.first=grid(nx=NULL), pch=23, bg="gray", cex=1.5)
   points(A, pch=1, cex=1.5)
   points(B, pch=19, cex=1.5)
   legend("topright", legend=c("Reference", "A", "B"), pch=c(23,1,19),
          pt.bg="gray", bg="white", bty = "n", pt.cex=1.5, cex=1.2)
}
cat("A is better than B in terms of Pareto optimality,\n however, IGD(A)=",
    igd(A, ref), "&gt; IGD(B)=", igd(B, ref),
    "and AvgHausdorff(A)=", avg_hausdorff_dist(A, ref),
    "&gt; AvgHausdorff(A)=", avg_hausdorff_dist(B, ref),
    ", which both contradict Pareto optimality.\nBy contrast, IGD+(A)=",
    igd_plus(A, ref), "&lt; IGD+(B)=", igd_plus(B, ref), ", which is correct.\n")
# A less trivial example.
extdata_path &lt;- system.file(package="moocore","extdata")
path.A1 &lt;- file.path(extdata_path, "ALG_1_dat.xz")
path.A2 &lt;- file.path(extdata_path, "ALG_2_dat.xz")
A1 &lt;- read_datasets(path.A1)[,1:2]
A2 &lt;- read_datasets(path.A2)[,1:2]
ref &lt;- filter_dominated(rbind(A1, A2))
igd(A1, ref)
igd(A2, ref)

# IGD+ (Pareto compliant)
igd_plus(A1, ref)
igd_plus(A2, ref)

# Average Haussdorff distance
avg_hausdorff_dist(A1, ref)
avg_hausdorff_dist(A2, ref)
</code></pre>


</div>