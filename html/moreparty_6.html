<div class="container">

<table style="width: 100%;"><tr>
<td>fastcforest</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Parallelized conditional inference random forest
</h2>

<h3>Description</h3>

<p>Parallelized version of <code>cforest</code> function from <code>party</code> package, which is an implementation of the random forest and bagging ensemble algorithms utilizing conditional inference trees as base learners.
</p>


<h3>Usage</h3>

<pre><code class="language-R">fastcforest(formula, data = list(), subset = NULL, weights = NULL,
            controls = party::cforest_unbiased(),
            xtrafo = ptrafo, ytrafo = ptrafo, scores = NULL,
            parallel = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>

<p>a symbolic description of the model to be fit. Note that symbols like <code>:</code> and <code>-</code> will not work and the tree will make use of all variables listed on the rhs of <code>formula</code>
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>a data frame containing the variables in the model
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>

<p>an optional vector specifying a subset of observations to be used in the fitting process
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>

<p>an optional vector of weights to be used in the fitting process. Non-negative integer valued weights are allowed as well as non-negative real weights. Observations are sampled (with or without replacement) according to probabilities <code>weights / sum(weights)</code>. The fraction of observations to be sampled (without replacement) is computed based on the sum of the weights if all weights are integer-valued and based on the number of weights greater zero else. Alternatively, <code>weights</code> can be a double matrix defining case weights for all <code>ncol(weights)</code> trees in the forest directly. This requires more storage but gives the user more control.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>controls</code></td>
<td>

<p>an object of class <code>ForestControl-class</code>, which can be obtained using <code>cforest_control</code> (and its convenience interfaces <code>cforest_unbiased</code> and <code>cforest_classical</code>).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>xtrafo</code></td>
<td>

<p>a function to be applied to all input variables. By default, the <code>ptrafo</code> function is applied.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ytrafo</code></td>
<td>

<p>a function to be applied to all response variables. By default, the <code>ptrafo</code> function is applied.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>scores</code></td>
<td>

<p>an optional named list of scores to be attached to ordered factors
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>parallel</code></td>
<td>

<p>Logical indicating whether or not to run <code>fastcforest</code> in parallel using a backend provided by the <code>foreach</code> package. Default is <code>TRUE</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>See <code>cforest</code> documentation for details.
The code for parallelization is inspired by <a href="https://stackoverflow.com/questions/36272816/train-a-cforest-in-parallel">https://stackoverflow.com/questions/36272816/train-a-cforest-in-parallel</a>
</p>


<h3>Value</h3>

<p>An object of class <code>RandomForest-class</code>.
</p>


<h3>Author(s)</h3>

<p>Nicolas Robette
</p>


<h3>References</h3>

<p>Leo Breiman (2001). Random Forests. <em>Machine Learning</em>, 45(1), 5–32.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin Radespiel-Troeger
(2004). Bagging Survival Trees. <em>Statistics in Medicine</em>, <b>23</b>(1), 77–91.
</p>
<p>Torsten Hothorn, Peter Buhlmann, Sandrine Dudoit, Annette Molinaro
and Mark J. van der Laan (2006a). Survival Ensembles. <em>Biostatistics</em>,
<b>7</b>(3), 355–373.
</p>
<p>Torsten Hothorn, Kurt Hornik and Achim Zeileis (2006b). Unbiased
Recursive Partitioning: A Conditional Inference Framework.
<em>Journal of Computational and Graphical Statistics</em>, <b>15</b>(3),
651–674.  Preprint available from
<a href="https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf">https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf</a>
</p>
<p>Carolin Strobl, Anne-Laure Boulesteix, Achim Zeileis and Torsten Hothorn (2007).
Bias in Random Forest Variable Importance Measures: Illustrations, Sources and
a Solution. <em>BMC Bioinformatics</em>, <b>8</b>, 25.
<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25</a>
</p>
<p>Carolin Strobl, James Malley and Gerhard Tutz (2009).
An Introduction to Recursive Partitioning: Rationale, Application, and Characteristics of
Classification and Regression Trees, Bagging, and Random forests.
<em>Psychological Methods</em>, <b>14</b>(4), 323–348.
</p>


<h3>See Also</h3>

<p><code>cforest</code>, <code>fastvarImp</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">  ## classification
  data(iris)
  iris2 = iris
  iris2$Species = factor(iris$Species=="versicolor")
  iris.cf = fastcforest(Species~., data=iris2, parallel=FALSE)
</code></pre>


</div>