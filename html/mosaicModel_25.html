<div class="container">

<table style="width: 100%;"><tr>
<td>mod_error</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Mean square prediction error</h2>

<h3>Description</h3>

<p>Compares model predictions to the actual value of the response variable.
To do this, testing data must be provided with <em>both</em> the input variables and the
corresponding response variable. The measure calculated for a quantitative response
variable is the mean square prediction error (MSPE).
For categorical response variables, an analog of MSPE can be calculated (see details)
but by default, a mean log-likelihood (mean per case) is computed instead.
</p>


<h3>Usage</h3>

<pre><code class="language-R">mod_error(model, testdata, error_type = c("default", "mse", "sse", "mad",
  "LL", "mLL", "dev", "class_error"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>The model whose prediction error is to be estimated.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>testdata</code></td>
<td>
<p>A data frame giving both model inputs and the actual value of the response
variable. If no testing data is provided, the training data will be used and a warning issued.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>error_type</code></td>
<td>
<p>The measure of error you are interested in. By default, this is mean-square error for
regression models and log-likelihood for classifiers. The choices are:
</p>

<ul>
<li> <p><code>"mse"</code> – mean square error
</p>
</li>
<li> <p><code>"sse"</code> – sum of square errors
</p>
</li>
<li> <p><code>"mad"</code> – mean absolute deviation
</p>
</li>
<li> <p><code>"LL"</code>  – log-likelihood
</p>
</li>
<li> <p><code>"mLL"</code> – mean log-likehood (per case in the testing data)
</p>
</li>
<li> <p><code>"dev"</code> – deviance. (Plus a constant, which is often zero. The constant is fixed for a given testing data set,
regardless of the model. So differences between deviances of two models are correct.)
</p>
</li>
<li> <p><code>"class_error"</code> – classification error rate.
</p>
</li>
</ul>
</td>
</tr>
</table>
<h3>Details</h3>

<p>When the response variable is categorical, the model
(called a 'classifier' in such situations) must be capable of
computing <em>probabilities</em> for each output rather than just a bare category.
This is true for many commonly encountered classifier model architectures.
</p>
<p>The analog of the mean squared error for classifiers is the mean of (1-p)^2, where p is the
probability assigned by the model to the actual output. This is a rough approximation
to the log-likelihood. By default, the log-likelihood will be calculated, but for pedagogical
reasons you may prefer (1-p)^2, in which case set <code>error_type = "mse"</code>. Classifiers can assign a probability
of zero to the actual output, in which case the log-likelihood is <code>-Inf</code>. The <code>"mse"</code> error type avoids this.
</p>


<h3>Examples</h3>

<pre><code class="language-R">mod &lt;- lm(mpg ~ hp + wt, data = mtcars)
mod_error(mod) # In-sample prediction error.
## Not run: 
classifier &lt;- rpart::rpart(Species ~ ., data = iris)
mod_error(classifier)
mod_error(classifier, error_type = "LL") 
# More typically
inds &lt;- sample(1:nrow(iris), size = 100)
Training &lt;- iris[inds, ]
Testing  &lt;- iris[ - inds, ]
classifier &lt;- rpart::rpart(Species ~ ., data = Training)
# This may well assign zero probability to events that appeared in the
# Testing data 
mod_error(classifier, testdata = Testing)
mod_error(classifier, testdata = Testing, error_type = "mse")

## End(Not run)
</code></pre>


</div>