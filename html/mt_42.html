<div class="container">

<table style="width: 100%;"><tr>
<td>fs.pls</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Feature Selection Using PLS  
</h2>

<h3>Description</h3>

<p>Feature selection using coefficient of regression and VIP values of PLS.
</p>


<h3>Usage</h3>

<pre><code class="language-R">  fs.pls(x,y, pls="simpls",ncomp=10,...)
  fs.plsvip(x,y, ncomp=10,...)
  fs.plsvip.1(x,y, ncomp=10,...)
  fs.plsvip.2(x,y, ncomp=10,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A data frame or matrix of data set. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A factor or vector of class.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pls</code></td>
<td>

<p>A method for calculating PLS scores and loadings. The following methods are supported:
</p>

<ul>
<li> <p><code>simpls:</code> SIMPLS algorithm. 
</p>
</li>
<li> <p><code>kernelpls:</code> kernel algorithm.
</p>
</li>
<li> <p><code>oscorespls:</code> orthogonal scores algorithm. 
</p>
</li>
</ul>
<p>For details, see <code>simpls.fit</code>, <code>kernelpls.fit</code> and
<code>oscorespls.fit</code> in package <span class="pkg">pls</span>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncomp</code></td>
<td>

<p>The number of components to be used.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>fs.pls</code> ranks the features by regression coefficient of PLS. Since the 
coefficient is a matrix due to the dummy multiple response variables designed 
for the classification (category) problem, the Mahalanobis distance of 
coefficient is applied to select the features. (Other ways, for example, the sum 
of absolute values of coefficient, or squared root of coefficient, can be used.)
</p>
<p><code>fs.plsvip</code> and <code>fs.plsvip.1</code> carry out feature selection based on the 
the Mahalanobis distance and absolute values of PLS's VIP, respectively. 
</p>
<p><code>fs.plsvip.2</code> is similar to <code>fs.plsvip</code> and <code>fs.plsvip.1</code>, but 
the category response is not treated as dummy multiple response matrix. 
</p>


<h3>Value</h3>

<p>A list with components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>fs.rank</code></td>
<td>
<p>A vector of feature ranking scores.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fs.order</code></td>
<td>
<p>A vector of feature order from best to worst.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>stats</code></td>
<td>
<p>A vector of measurements.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>See Also</h3>

<p><code>feat.rank.re</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">## prepare data set
data(abr1)
cls &lt;- factor(abr1$fact$class)
dat &lt;- abr1$pos
## dat &lt;- abr1$pos[,110:1930]

## fill zeros with NAs
dat &lt;- mv.zene(dat)

## missing values summary
mv &lt;- mv.stats(dat, grp=cls) 
mv    ## View the missing value pattern

## filter missing value variables
## dim(dat)
dat &lt;- dat[,mv$mv.var &lt; 0.15]
## dim(dat)

## fill NAs with mean
dat &lt;- mv.fill(dat,method="mean")

## log transformation
dat &lt;- preproc(dat, method="log10")

## select class "1" and "2" for feature ranking
ind &lt;- grepl("1|2", cls)
mat &lt;- dat[ind,,drop=FALSE] 
mat &lt;- as.matrix(mat)
grp &lt;- cls[ind, drop=TRUE]   

## apply PLS methods for feature selection
res.pls      &lt;- fs.pls(mat,grp, ncomp=4)
res.plsvip   &lt;- fs.plsvip(mat,grp, ncomp=4)
res.plsvip.1 &lt;- fs.plsvip.1(mat,grp, ncomp=4)
res.plsvip.2 &lt;- fs.plsvip.2(mat,grp, ncomp=4)

## check differences among these methods
fs.order &lt;- data.frame(pls      = res.pls$fs.order,
                       plsvip   = res.plsvip$fs.order,
                       plsvip.1 = res.plsvip.1$fs.order,
                       plsvip.2 = res.plsvip.2$fs.order)
head(fs.order, 20)

</code></pre>


</div>