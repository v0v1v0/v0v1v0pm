<div class="container">

<table style="width: 100%;"><tr>
<td>plsc</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Classification with PLSDA
</h2>

<h3>Description</h3>

<p>Classification with partial least squares (PLS) or PLS plus linear discriminant 
analysis (LDA).
</p>


<h3>Usage</h3>

<pre><code class="language-R">plsc(x, ...)

plslda(x, ...)

## Default S3 method:
plsc(x, y, pls="simpls",ncomp=10, tune=FALSE,...)

## S3 method for class 'formula'
plsc(formula, data = NULL, ..., subset, na.action = na.omit)

## Default S3 method:
plslda(x, y, pls="simpls",ncomp=10, tune=FALSE,...)

## S3 method for class 'formula'
plslda(formula, data = NULL, ..., subset, na.action = na.omit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>

<p>A formula of the form <code>groups ~ x1 + x2 + ...</code>  That is, the
response is the grouping factor and the right hand side specifies
the (non-factor) discriminators.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>Data frame from which variables specified in <code>formula</code> are
preferentially to be taken.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A matrix or data frame containing the explanatory variables if no formula is
given as the principal argument.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A factor specifying the class for each observation if no formula principal 
argument is given.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pls</code></td>
<td>

<p>A method for calculating PLS scores and loadings. The following methods are supported:
</p>

<ul>
<li> <p><code>simpls:</code> SIMPLS algorithm. 
</p>
</li>
<li> <p><code>kernelpls:</code> kernel algorithm.
</p>
</li>
<li> <p><code>oscorespls:</code> orthogonal scores algorithm. 
</p>
</li>
</ul>
<p>For details, see <code>simpls.fit</code>, <code>kernelpls.fit</code> and
<code>oscorespls.fit</code> in package <span class="pkg">pls</span>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncomp</code></td>
<td>

<p>The number of components to be used in the classification.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tune</code></td>
<td>

<p>A logical value indicating whether the best number of components should be tuned.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>Arguments passed to or from other methods.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>subset</code></td>
<td>

<p>An index vector specifying the cases to be used in the training
sample.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>na.action</code></td>
<td>

<p>A function to specify the action to be taken if <code>NA</code>s are found. The 
default action is <code>na.omit</code>, which leads to rejection of cases with 
missing values on any required variable. An alternative is <code>na.fail</code>, 
which causes an error if <code>NA</code> cases are found. 
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>plcs</code> implements PLS for classification. In details, the categorical response 
vector <code>y</code> is converted into a numeric matrix for regression by PLS and the 
output of PLS is convert to posteriors by <code>softmax</code> method. 
The classification results are obtained based on the posteriors. <code>plslda</code> 
combines PLS and LDA for classification, in which, PLS is for dimension reduction
and LDA is for classification based on the data transformed by PLS. 
</p>
<p>Three PLS functions,<code>simpls.fit</code>, 
<code>kernelpls.fit</code> and  <code>oscorespls.fit</code>, are  
implemented in package <span class="pkg">pls</span>.
</p>


<h3>Value</h3>

<p>An object of class <code>plsc</code> or <code>plslda</code> containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A matrix of the latent components or scores from PLS.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cl</code></td>
<td>

<p>The observed class labels of training data. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>

<p>The predicted class labels of training data. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>conf</code></td>
<td>

<p>The confusion matrix based on training data. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>acc</code></td>
<td>

<p>The accuracy rate of training data. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>posterior</code></td>
<td>

<p>The posterior probabilities for the predicted classes. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ncomp</code></td>
<td>

<p>The number of latent component used for classification. 
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pls.method</code></td>
<td>

<p>The PLS algorithm used.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pls.out</code></td>
<td>

<p>The output of PLS.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lda.out</code></td>
<td>

<p>The output of LDA used only by <code>plslda</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>

<p>The (matched) function call.
</p>
</td>
</tr>
</table>
<h3>Note</h3>

<p>Two functions may be called giving either a formula and
optional data frame, or a matrix and grouping factor as the first
two arguments. 
</p>


<h3>Author(s)</h3>

<p>Wanchang Lin 
</p>


<h3>References</h3>

<p>Martens, H. and Nas, T. (1989) <em>Multivariate calibration.</em>
John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code>kernelpls.fit</code>, <code>simpls.fit</code>, 
<code>oscorespls.fit</code>, <code>predict.plsc</code>,
<code>plot.plsc</code>, <code>tune.func</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">library(pls)  
data(abr1)
cl   &lt;- factor(abr1$fact$class)
dat  &lt;- preproc(abr1$pos , y=cl, method=c("log10"),add=1)[,110:500]

## divide data as training and test data
idx &lt;- sample(1:nrow(dat), round((2/3)*nrow(dat)), replace=FALSE) 

## construct train and test data 
train.dat  &lt;- dat[idx,]
train.t    &lt;- cl[idx]
test.dat   &lt;- dat[-idx,]        
test.t     &lt;- cl[-idx] 

## apply plsc and plslda
(res   &lt;- plsc(train.dat,train.t, ncomp = 20, tune = FALSE))
## Estimate the mean squared error of prediction (MSEP), root mean squared error
## of prediction (RMSEP) and R^2 (coefficient of multiple determination) for 
## fitted PLSR model 
MSEP(res$pls.out)
RMSEP(res$pls.out)
R2(res$pls.out)

(res.1  &lt;- plslda(train.dat,train.t, ncomp = 20, tune = FALSE))
## Estimate the mean squared error of prediction (MSEP), root mean squared error
## of prediction (RMSEP) and R^2 (coefficient of multiple determination) for 
## fitted PLSR model 
MSEP(res.1$pls.out)
RMSEP(res.1$pls.out)
R2(res.1$pls.out)

## Not run: 
## with function of tuning component numbers
(z.plsc   &lt;- plsc(train.dat,train.t, ncomp = 20, tune = TRUE))
(z.plslda &lt;- plslda(train.dat,train.t, ncomp = 20, tune = TRUE))

## check nomp tuning results
z.plsc$ncomp
plot(z.plsc$acc.tune)
z.plslda$ncomp
plot(z.plslda$acc.tune)

## plot
plot(z.plsc,dimen=c(1,2,3),main = "Training data",abbrev = TRUE)
plot(z.plslda,dimen=c(1,2,3),main = "Training data",abbrev = TRUE)

## predict test data
pred.plsc   &lt;- predict(z.plsc, test.dat)$class
pred.plslda &lt;- predict(z.plslda, test.dat)$class

## classification rate and confusion matrix
cl.rate(test.t, pred.plsc)
cl.rate(test.t, pred.plslda)


## End(Not run)
</code></pre>


</div>