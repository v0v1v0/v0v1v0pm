<div class="container">

<table style="width: 100%;"><tr>
<td>journals</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Prevalence of Statistical Reporting Errors</h2>

<h3>Description</h3>

<p>This data set, "journals" provides a summary of statistical reporting errors (i.e., inconsistencies between reported test statistic
and reported p-value) of 16,695 research articles reporting results
from null hypothesis significance testing (NHST). The selected articles were published in
eight major journals in psychology between 1985 to 2013:
</p>

<ul>
<li> <p><em>Developmental Psychology</em> (DP)
</p>
</li>
<li> <p><em>Frontiers in Psychology</em> (FP)
</p>
</li>
<li> <p><em>Journal of Applied Psychology</em> (JAP)
</p>
</li>
<li> <p><em>Journal of Consulting and Clinical Psychology</em> (JCCP)
</p>
</li>
<li> <p><em>Journal of Experimental Psychology: General</em> (JEPG)
</p>
</li>
<li> <p><em>Journal of Personality and Social Psychology</em> (JPSP)
</p>
</li>
<li> <p><em>Public Library of Science</em> (PLoS)
</p>
</li>
<li> <p><em>Psychological Science</em> (PS)
</p>
</li>
</ul>
<p>In total, Nuijten et al. (2016) recomputed 258,105 p-values with the <code>R</code> software
package <code>statcheck</code> which extracts statistics from articles and recomputes the p-values.
The anonymized dataset and the data documentation was openly available on the
Open Science Framework (<a href="https://osf.io/d3ukb/">https://osf.io/d3ukb/</a>; <a href="https://osf.io/c6ap2/">https://osf.io/c6ap2/</a>).
</p>


<h3>Usage</h3>

<pre><code class="language-R">data(journals)
</code></pre>


<h3>Format</h3>

<p>A <code>data.frame</code> with 8 rows and 14 variables:</p>

<table>
<tr>
<td style="text-align: left;">
   Variable Name </td>
<td style="text-align: left;"> Description </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>journal</code> </td>
<td style="text-align: left;"> The journal name a research article was published in. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>articles_downloaded</code> </td>
<td style="text-align: left;"> The number of articles downloaded per journal. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>articles_with_NHST</code> </td>
<td style="text-align: left;"> The number of articles with NHST results. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>perc_articles_with_NHST</code> </td>
<td style="text-align: left;"> The percentage of all downloaded articles that had NHST results. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>nr_NHST</code> </td>
<td style="text-align: left;"> The total number of NHST results. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>mean_nr_NHST_per_article_with_NHST</code> </td>
<td style="text-align: left;"> The mean number of NHST results per article that had at least one NHST result. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>mean_nr_NHST_per_article_all_included</code> </td>
<td style="text-align: left;"> The mean number of NHST results in all downloaded articles. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>errors</code> </td>
<td style="text-align: left;"> The total number of errors. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>dec_errors</code> </td>
<td style="text-align: left;"> The total number of decision errors (i.e., an error that may have changed the statistical conclusion of the result). </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>perc_errors</code> </td>
<td style="text-align: left;"> The percentage of all results that was an error. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>perc_dec_errors</code> </td>
<td style="text-align: left;"> The percentage of all results that was a decision error. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>perc_articles_with_errors</code> </td>
<td style="text-align: left;"> The percentage of all articles that had at least one error. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>perc_articles_with_dec_errors</code> </td>
<td style="text-align: left;"> The percentage of all articles that had at least one error. </td>
</tr>
<tr>
<td style="text-align: left;">
   <code>APAfactor</code> </td>
<td style="text-align: left;"> APA factor: number of detected NHST results / total number of detected p values. </td>
</tr>
<tr>
<td style="text-align: left;">
</td>
</tr>
</table>
<h3>References</h3>

<p>Nuijten MB, Hartgerink CH, van Assen MA, Epskamp S, Wicherts JM (2016).
“The prevalence of statistical reporting errors in psychology (1985–2013).”
<em>Behavior Research Methods</em>, <b>48</b>, 1205–1226.
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(journals)
# Prior specification 
# We assign a uniform Beta distribution on each binomial probability
a &lt;- rep(1, 8)  
b &lt;- rep(1, 8)  

x &lt;- journals$errors 
n  &lt;- journals$nr_NHST
factor_levels &lt;- levels(journals$journal)

# restricted hypothesis
Hr1 &lt;- c('JAP , PS , JCCP , PLOS , DP , FP , JEPG &lt; JPSP')
out &lt;- binom_bf_informed(x=x, n=n, Hr=Hr1, a=a, b=b, 
factor_levels=factor_levels, niter = 2e3)

summary(out)
</code></pre>


</div>