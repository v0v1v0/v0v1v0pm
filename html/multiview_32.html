<div class="container">

<table style="width: 100%;"><tr>
<td>view.contribution</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Evaluate the contribution of data views in making prediction</h2>

<h3>Description</h3>

<p>Evaluate the contribution of each data view in making prediction. The function has two options.
If <code>force</code> is set to <code>NULL</code>, the data view contribution is benchmarked by the null model.
If <code>force</code> is set to a list of data views, the contribution is benchmarked by the model fit on
this list of data views, and the function evaluates the marginal contribution of each additional data
view on top of this benchmarking list of views.
The function returns a table showing the percentage improvement in reducing error as compared to the bechmarking model
made by each data view.
</p>


<h3>Usage</h3>

<pre><code class="language-R">view.contribution(
  x_list,
  y,
  family = gaussian(),
  rho,
  s = c("lambda.min", "lambda.1se"),
  eval_data = c("train", "test"),
  weights = NULL,
  type.measure = c("default", "mse", "deviance", "class", "auc", "mae", "C"),
  x_list_test = NULL,
  test_y = NULL,
  nfolds = 10,
  foldid = NULL,
  force = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x_list</code></td>
<td>
<p>a list of <code>x</code> matrices with same number of rows
<code>nobs</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>the quantitative response with length equal to <code>nobs</code>, the
(same) number of rows in each <code>x</code> matrix</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>A description of the error distribution and link
function to be used in the model. This is the result of a call to
a family function. Default is stats::gaussian. (See
stats::family for details on family functions.)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rho</code></td>
<td>
<p>the weight on the agreement penalty, default 0. <code>rho=0</code>
is a form of early fusion, and <code>rho=1</code> is a form of late fusion.
We recommend trying a few values of <code>rho</code> including 0, 0.1, 0.25,
0.5, and 1 first; sometimes <code>rho</code> larger than 1 can also be
helpful.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>s</code></td>
<td>
<p>Value(s) of the penalty parameter <code>lambda</code> at which
predictions are required. Default is the value <code>s="lambda.1se"</code> stored
on the CV <code>object</code>. Alternatively <code>s="lambda.min"</code> can be used. If
<code>s</code> is numeric, it is taken as the value(s) of <code>lambda</code> to be
used. (For historical reasons we use the symbol 's' rather than 'lambda' to
reference this parameter)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>eval_data</code></td>
<td>
<p>If <code>train</code>, we evaluate the contribution of data views based on training data
using cross validation error; if <code>test</code>, we evaluate the contribution of data views based on test data.
Default is <code>train</code>. If set to <code>test</code>, users need to provide the test data, i.e.
<code>x_list_test</code> and <code>y_test</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>weights</code></td>
<td>
<p>Observation weights; defaults to 1 per observation</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>type.measure</code></td>
<td>
<p>loss to use for cross-validation. Currently
five options, not all available for all models. The default is
<code>type.measure="deviance"</code>, which uses squared-error for gaussian
models (a.k.a <code>type.measure="mse"</code> there), deviance for logistic
and poisson regression, and partial-likelihood for the Cox model.
<code>type.measure="class"</code> applies to binomial and multinomial
logistic regression only, and gives misclassification error.
<code>type.measure="auc"</code> is for two-class logistic regression only,
and gives area under the ROC curve. <code>type.measure="mse"</code> or
<code>type.measure="mae"</code> (mean absolute error) can be used by all
models except the <code>"cox"</code>; they measure the deviation from the
fitted mean to the response.  <code>type.measure="C"</code> is Harrel's
concordance measure, only available for <code>cox</code> models.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x_list_test</code></td>
<td>
<p>A list of <code>x</code> matrices in the test data for evaluation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test_y</code></td>
<td>
<p>The quantitative response in the test data with length equal to the
number of rows in each <code>x</code> matrix of the test data.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nfolds</code></td>
<td>
<p>number of folds - default is 10. Although <code>nfolds</code>
can be as large as the sample size (leave-one-out CV), it is not
recommended for large datasets. Smallest value allowable is
<code>nfolds=3</code></p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>foldid</code></td>
<td>
<p>an optional vector of values between 1 and <code>nfold</code>
identifying what fold each observation is in. If supplied,
<code>nfold</code> can be missing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>force</code></td>
<td>
<p>If <code>NULL</code>, the data view contribution is benchmarked by the null model.
If users want to benchmark by the model fit on a specified list of data views, <code>force</code> needs to
be set to this list of benchmarking data views, i.e. a list of <code>x</code> matrices. The function then
evaluates the marginal contribution of each additional data, i.e. the data views in <code>x_list</code> but not in
<code>force</code>, on top of the benchmarking views.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Other arguments that can be passed to <code>multiview</code></p>
</td>
</tr>
</table>
<h3>Value</h3>

<p>a data frame consisting of the view, error metric, and percentage improvement.
</p>


<h3>Examples</h3>

<pre><code class="language-R">set.seed(3)
# Simulate data based on the factor model
x = matrix(rnorm(200*20), 200, 20)
z = matrix(rnorm(200*20), 200, 20)
w = matrix(rnorm(200*20), 200, 20)
U = matrix(rep(0, 200*10), 200, 10) # latent factors
for (m in seq(10)){
    u = rnorm(200)
    x[, m] = x[, m] + u
    z[, m] = z[, m] + u
    w[, m] = w[, m] + u
    U[, m] = U[, m] + u}
beta_U = c(rep(2, 5),rep(-2, 5))
y = U %*% beta_U + 3 * rnorm(100)

# Split training and test sets
smp_size_train = floor(0.9 * nrow(x))
train_ind = sort(sample(seq_len(nrow(x)), size = smp_size_train))
test_ind = setdiff(seq_len(nrow(x)), train_ind)
train_X = scale(x[train_ind, ])
test_X = scale(x[test_ind, ])
train_Z &lt;- scale(z[train_ind, ])
test_Z &lt;- scale(z[test_ind, ])
train_W &lt;- scale(w[train_ind, ])
test_W &lt;- scale(w[test_ind, ])
train_y &lt;- y[train_ind, ]
test_y &lt;- y[test_ind, ]
foldid = sample(rep_len(1:10, dim(train_X)[1]))

# Benchmarked by the null model:
rho = 0.3
view.contribution(x_list=list(x=train_X,z=train_Z), train_y, rho = rho,
                  eval_data = 'train', family = gaussian())
view.contribution(x_list=list(x=train_X,z=train_Z), train_y, rho = rho,
                  eval_data = 'test', family = gaussian(),
                  x_list_test=list(x=test_X,z=test_Z), test_y=test_y)

# Force option -- benchmarked by the model train on a specified list of data views:
view.contribution(x_list=list(x=train_X,z=train_Z,w=train_W), train_y, rho = rho,
                  eval_data = 'train', family = gaussian(), force=list(x=train_X))

</code></pre>


</div>