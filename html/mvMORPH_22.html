<div class="container">

<table style="width: 100%;"><tr>
<td>mvols</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Fit linear model using Ordinary Least Squares to multivariate (high-dimensional) data sets

</h2>

<h3>Description</h3>

<p>This function uses maximum likelihood (or restricted likelihood) and penalized likelihood approaches to fit linear models with independent observations (this is the multivariate (and penalized) counterpart to the base <code>lm</code> function). <code>mvols</code> uses a penalized-likelihood (PL) approach (see descriptions in Clavel et al. 2019) to fit linear models to high-dimensional data sets (where the number of variables <em>p</em> is approaching or is larger than the number of observations <em>n</em>). The PL approach generally provides improved estimates compared to ML. OLS is a special case  of GLS linear models (and a wrapper of <code>mvgls</code>) and can be used with all the package functions working on <code>mvgls</code> class objects.
</p>



<h3>Usage</h3>

<pre><code class="language-R">mvols(formula, data, method=c("PL-LOOCV","LL"),
      REML=TRUE, ...)
</code></pre>


<h3>Arguments</h3>

 
<table>
<tr style="vertical-align: top;">
<td><code>formula</code></td>
<td>

<p>An object of class "<code>formula</code>" (a two-sided linear formula describing the model to be fitted. See for instance ?<code>lm</code>)
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>data</code></td>
<td>

<p>An optional list, data.frame or environment containing the variables in the model. If not found in <em>data</em> the variables are taken from the current environment. Prefer <code>list</code> for blocks of multivariate responses unless you're specifying the response variables by their names using <code>cbind</code> with data.frame.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>method</code></td>
<td>

<p>The method used to fit the model. "PL-LOOCV" (or equivalently just "LOOCV") is the nominal leave one out cross-validation of the penalized log-likelihood, "LL" is the log-likelihood (used in the conventional ML and REML estimation). Two approximated LOOCV methods are also available: "H&amp;L" and "Mahalanobis". The method "H&amp;L" is a fast LOOCV approach based on Hoffbeck and Landgrebe (1996) tricks, and "Mahalanobis" is an approximation of the LOOCV score proposed by Theiler (2012). Both "H&amp;L" and "Mahalanobis" work only with the "RidgeArch" penalty and for intercept only models (i.e. of the form Y~1, see also details).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>REML</code></td>
<td>

<p>Use REML (default) or ML for estimating the parameters.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>

<p>Options to be passed through. For instance the type of penalization: 
<code>penalty="RidgeArch"</code> (default), <code>penalty="RidgeAlt"</code>, or <code>penalty="LASSO"</code>. The target matrices used by "RidgeArch" and "RidgeAlt" penalizations: <code>target="unitVariance"</code>, <code>target="Variance"</code> or <code>target="null"</code>... etc. (see details). One can also define contrasts options as for the <code>lm</code> function.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><code>mvols</code> allows fitting multivariate linear models to multivariate (possibly high-dimensional, i.e. where the number of variables <em>p</em> is larger than <em>n</em>) datasets. Models estimated using penalized likelihood (e.g., method="PL-LOOCV") are generally more accurate than those estimated by maximum likelihood methods (method="LL") when the number of traits approach the number of observations. PL is the only solution when <em>p</em>&gt;<em>n</em>.  Models fit can be compared using the GIC or EIC criterion (see ?<code>GIC</code> and ?<code>EIC</code>) and hypothesis testing can be performed using the <code>manova.gls</code> function.
</p>
<p>The various <em>arguments</em> that can be passed through <b>"..."</b>:
</p>
<p><b>"penalty"</b> - The "penalty" argument allows specifying the type of penalization used for regularization (described in Clavel et al. 2019). The various penalizations are: <code>penalty="RidgeArch"</code> (the default), <code>penalty="RidgeAlt"</code> and <code>penalty="LASSO"</code>. The "RidgeArch" penalization shrink linearly the "sample"" covariance matrix toward a given target matrix with a specific structure (see below for <code>target</code>). This penalization is generally fast and the tuning parameter is bounded between 0 and 1 (see van Wieringen &amp; Peeters 2016, Clavel et al. 2019). The "RidgeAlt" penalization scheme uses a quadratic ridge penalty to shrink the covariance matrix toward a specified target matrix (see <code>target</code> below and also see van Wieringen &amp; Peeters 2016). Finally, the "LASSO" regularize the covariance matrix by estimating a sparse estimate of its inverse - the precision matrix (Friedman et al. 2008). Solving the LASSO penalization is computationally intensive. Moreover, this penalization scheme is not invariant to arbitrary rotations of the data.
</p>
<p><b>"target"</b> - This argument allows specifying the target matrix toward which the covariance matrix is shrunk for "Ridge" penalties. <code>target="unitVariance"</code> (for a diagonal target matrix proportional to the identity) and <code>target="Variance"</code> (for a diagonal matrix with unequal variance) can be used with both "RidgeArch" and "RidgeAlt" penalties. <code>target="null"</code> (a null target matrix) is only available for "RidgeAlt". Penalization with the "Variance" target shrinks the eigenvectors of the covariance matrix and is therefore not rotation invariant. See details on the various target properties in Clavel et al. (2019).
</p>
<p><b>"weights"</b> - A (named) vector of weights (variances) for all the observations. If provided, a weighted least squares (WLS) rather than OLS fit is performed.
</p>
<p><b>"echo"</b> - Whether the results must be returned or not.
</p>
<p><b>"grid_search"</b> - A logical indicating whether or not a preliminary grid search must be performed to find the best starting values for optimizing the log-likelihood (or penalized log-likelihood). User-specified starting values can be provided through the <b>start</b> argument. Default is <code>TRUE</code>.
</p>
<p><b>"tol"</b> - Minimum value for the regularization parameter. Singularities can occur with a zero value in high-dimensional cases. (default is <code>NULL</code>)
</p>



<h3>Value</h3>

<p>An object of class '<code>mvols</code>'. It contains a list including the same components as the <code>mvgls</code> function (see ?mvgls).
</p>


<h3>Note</h3>

<p>This function is a wrapper to the <code>mvgls</code> function (it uses gls with a diagonal covariance). For these reasons, the function can be used with all the methods working with <code>mvgls</code> class objects.
</p>


<h3>Author(s)</h3>

<p>Julien Clavel

</p>


<h3>References</h3>

<p>Clavel, J., Aristide, L., Morlon, H., 2019. A Penalized Likelihood framework for high-dimensional phylogenetic comparative methods and an application to new-world monkeys brain evolution. Systematic Biology 68(1): 93-116.
</p>
<p>Clavel, J., Morlon, H. 2020. Reliable phylogenetic regressions for multivariate comparative data: illustration with the MANOVA and application to the effect of diet on mandible morphology in phyllostomid bats. Systematic Biology 69(5): 927-943.
</p>
<p>Friedman J., Hastie T., Tibshirani R. 2008. Sparse inverse covariance estimation with the graphical lasso. Biostatistics. 9:432-441.
</p>
<p>Hoffbeck J.P., Landgrebe D.A. 1996. Covariance matrix estimation and classification with limited training data. IEEE Trans. Pattern Anal. Mach. Intell. 18:763-767.
</p>
<p>Theiler J. 2012. The incredible shrinking covariance estimator. In: Automatic Target Recognition XXII. Proc. SPIE 8391, Baltimore, p. 83910P.
</p>
<p>van Wieringen W.N., Peeters C.F.W. 2016. Ridge estimation of inverse covariance matrices from high-dimensional data. Comput. Stat. Data Anal. 103:284-303.
</p>



<h3>See Also</h3>

<p><code>manova.gls</code>
<code>mvgls</code>
<code>EIC</code>
<code>GIC</code>
<code>mvgls.pca</code>
<code>fitted.mvgls</code>
<code>residuals.mvgls</code>
<code>coef.mvgls</code>
<code>vcov.mvgls</code>
<code>predict.mvgls</code>

</p>


<h3>Examples</h3>

<pre><code class="language-R">
set.seed(1)
n &lt;- 32 # number of species
p &lt;- 50 # number of traits (p&gt;n)

tree &lt;- pbtree(n=n, scale=1) # phylogenetic tree
R &lt;- crossprod(matrix(runif(p*p), ncol=p)) # a random covariance matrix
# simulate a BM dataset
Y &lt;- mvSIM(tree, model="BM1", nsim=1, param=list(sigma=R, theta=rep(0,p))) 
data=list(Y=Y)

fit1 &lt;- mvgls(Y~1, data=data, tree, model="BM", penalty="RidgeArch")

# compare to OLS?
fit2 &lt;- mvols(Y~1, data=data, penalty="RidgeArch")

GIC(fit1); GIC(fit2); 

## Fit a model by Maximum Likelihood (rather than Penalized likelihood) when p&lt;&lt;n
fit_ml &lt;- mvols(Y[,1:2]~1, data=data, method="LL")
summary(fit_ml)




</code></pre>


</div>