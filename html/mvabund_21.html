<div class="container">

<table style="width: 100%;"><tr>
<td>cv.glm1path</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Fits a path of Generalised Linear Models with LASSO (or L1) penalties, and finds the best model by corss-validation.
</h2>

<h3>Description</h3>

<p>Fits a sequence (path) of generalised linear models with LASSO penalties, using an iteratively reweighted local linearisation approach. The whole path of models is returned, as well as the one that minimises predictive log-likelihood on random test observations. Can handle negative binomial family, even with overdispersion parameter unknown, as well as other GLM families. 
</p>


<h3>Usage</h3>

<pre><code class="language-R">cv.glm1path(object, block = NULL, best="min", plot=TRUE, prop.test=0.2, n.split = 10,
    seed=NULL, show.progress=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>object</code></td>
<td>

<p>Output from a <code>glm1path</code> fit.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>block</code></td>
<td>

<p>A factor specifying a blocking variable, where training/test splits randomly assign blocks of observations to different groups rather than breaking up observations within blocks. Default (<code>NULL</code>) will randomly split rows into test and training groups.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>best</code></td>
<td>

<p>How should the best-fitting model be determined? <code>"1se"</code> uses the one standard error rule, <code>"min"</code> (or any other value) will return the model with best predictive performance. WARNING: David needs to check se calculatios...
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>

<p>Logical value indicating whether to plot the predictive log-likelihood as a function of model complexity.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>prop.test</code></td>
<td>

<p>The proportion of observations (or blocks) to assign as test observations. Default value of 0.2 gives a 80:20 training:test split.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>n.split</code></td>
<td>

<p>The number of random training/test splits to use. Default is 10 but the more the merrier (and the slower).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>seed</code></td>
<td>

<p>A vector of seeds to use for the random test/training splits. This is useful if you want to be able to exactly replicate analyses, without Monte Carlo variation in the splits. Default will not used fixed seeds.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>show.progress</code></td>
<td>
<p>Logical argument, if TRUE, console will report when a run for a seed has been completed. This option has been included because this function can take yonks to run on large datasets.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>...</code></td>
<td>
<p>Further arguments passed through to <code>glm1path</code>.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>This function fits a series of LASSO-penalised generalised linear models, with different values for the LASSO penalty, as for <code>glm1path</code>. The main difference is that the best fitting model is selected by cross-validation, using <code>n.test</code> different random training/test splits to estimate predictive performance on new (test) data. Mean predictive log-likelihood (per test observation) is used as the criterion for choosing the best model, which has connections with the Kullback-Leibler distance. The <code>best</code> argument controls whether to select the model that maximises predictive log-likelihood, or the smallest model within 1se of the maximum (the '1 standard error rule').
</p>
<p>All other details of this function are as for <code>glm1path</code>.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>coefficients</code></td>
<td>
<p>Vector of model coefficients for the best-fitting model (as judged by predictive log-likelihood)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambda</code></td>
<td>
<p>The value of the LASOS penalty parameter, lambda, for the best-fitting model (as judged by predictive log-likelihood)</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>glm1.best</code></td>
<td>
<p>The glm1 fit for the best-fitting model (as judged by predictive log-likelihood). For what this contains see <code>glm1</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>all.coefficients</code></td>
<td>
<p>A matrix where each column represents the model coefficients for a fit along the path specified by <code>lambdas</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>lambdas</code></td>
<td>
<p>A vector specifying the path of values for the LASSO penalty, arranged from largest (strongest penalty, smallest fitted model) to smallest (giving the largest fitted model).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>logL</code></td>
<td>
<p>A vector of log-likelihood values for each model along the path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>A vector giving the number of non-zero parameter estimates (a crude measure of degrees of freedom) for each model along the path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>bics</code></td>
<td>
<p>A vector of BIC values for each model along the path. Calculated using a penalty on model complexity as specified by input argument <code>k</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>counter</code></td>
<td>
<p>A vector counting how many iterations until convergence, for each model along the path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>check</code></td>
<td>
<p>A vector of logical values specifying whether or not Karush-Kuhn-Tucker conditions are satisfied at the solution.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>phis</code></td>
<td>
<p>For negative binomial regression - a vector of overdispersion parameters, for each model along the path.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>
<p>The vector of values for the response variable specified as an input argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>
<p>The design matrix of p explanatory variables specified as an input argument.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>penalty</code></td>
<td>
<p>The vector to be multiplied by each lambda to make the penalty for each fitted model.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>family</code></td>
<td>
<p>The family argument specified as input.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ll.cv</code></td>
<td>
<p>The mean predictive log-likelihood, averaged over all observations and then over all training/test splits.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>se</code></td>
<td>
<p>Estimated standard error of the mean predictive log-likelihood.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>David I. Warton &lt;David.Warton@unsw.edu.au&gt;
</p>


<h3>References</h3>

<p>Osborne, M.R., Presnell, B. and Turlach, B.A. (2000) On the LASSO and its dual. Journal of Computational and Graphical Statistics, 9, 319-337.
</p>


<h3>See Also</h3>

<p><code>glm1path</code>, \codeglm1, <code>glm</code>, <code>family</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(spider)
Alopacce &lt;- spider$abund[,1]
X &lt;- model.matrix(~.,data=spider$x) # to get design matrix with intercept term

# fit a LASSO-penalised negative binomial regression:
ft = glm1path(Alopacce,X,lam.min=0.1)
coef(ft)

# now estimate the best-fitting model by cross-validation:
cvft = cv.glm1path(ft)
coef(cvft)

</code></pre>


</div>