<div class="container">

<table style="width: 100%;"><tr>
<td>Squared multivariate correlation between two sets of variables</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Squared multivariate correlation between two sets of variables
</h2>

<h3>Description</h3>

<p>Squared multivariate correlation between two sets of variables.
</p>


<h3>Usage</h3>

<pre><code class="language-R">sq.correl(y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y</code></td>
<td>

<p>A numerical matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>

<p>A numerical matrix.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Mardia, Kent and Bibby (1979, pg. 171) defined two squared multiple correlation coefficient between the dependent variable <code class="reqn">\bf Y</code> and the independent variable <code class="reqn">\bf X</code>. They mention that these are a similar measure of the coefficient determination in the univariate regression. Assume that the multivariate regression model is written as <code class="reqn">{\bf Y}={\bf XB}+{\bf U}</code>, where <code class="reqn">\bf U</code> is the matrix of residuals. Then, they write <code class="reqn">{\bf D}=\left({\bf Y}^T{\bf Y}\right)^{-1}\hat{\bf U}^T\hat{\bf U}</code>, with <code class="reqn">\hat{\bf U}^T\hat{\bf U}={\bf Y}^T{\bf PY}</code> and <code class="reqn">\bf P</code> is <code class="reqn">{\bf P}={\bf I}_n-{\bf X}\left({\bf X}^T{\bf X}\right)^{-1}{\bf X}^T</code>. The matrix <code class="reqn">\bf D</code> is a generalization of <code class="reqn">1-R^2</code> in the univariate case. Mardia, Kent and Bibby (1979, pg. 171) mentioned that the dependent variable <code class="reqn">\bf Y</code> has to be centred.
</p>
<p>The squared multivariate correlation should lie between 0 and 1 and this property is satisfied by the trace correlation <code class="reqn">r_T</code> and the determinant correlation <code class="reqn">r_D</code>, defined as
<code class="reqn">r^2_T=d^{-1}\text{tr}\left({\bf I}-{\bf D}\right)</code> and <code class="reqn">r^2_D=\text{det}\left({\bf I}-{\bf D}\right)</code>
respectively, where <code class="reqn">d</code> denotes the dimensionality of <code class="reqn">\bf Y</code>. So, high values indicate high proportion of variance of the dependent variables explained. Alternatively, one can calculate the trace and the determinant of the matrix <code class="reqn">{\bf E}=\left({\bf Y}^T{\bf Y}\right)^{-1}\hat{\bf Y}^T\hat{\bf Y}</code>. Try something else also, use the function "sq.correl()" in a univariate regression example and then calculate the <code class="reqn">R^2</code> for the same dataset. Try this example again but without centering the dependent variable. In addition, take two variables and calculate their squared correlation coefficient and then square it and using "sq.correl()".
</p>


<h3>Value</h3>

<p>A vector with two values, the trace and determinant <code class="reqn">R^2</code>.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> rv, dcor
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">sq.correl( as.matrix(iris[, 1:2]), as.matrix(iris[, 3:4]) )
</code></pre>


</div>