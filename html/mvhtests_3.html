<div class="container">

<table style="width: 100%;"><tr>
<td>Exponential empirical likelihood hypothesis testing for two mean vectors</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Exponential empirical likelihood hypothesis testing for two mean vectors
</h2>

<h3>Description</h3>

<p>Exponential empirical likelihood hypothesis testing for two mean vectors.
</p>


<h3>Usage</h3>

<pre><code class="language-R">eel.test2(y1, y2, tol = 1e-07, R = 0, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>y1</code></td>
<td>

<p>A matrix containing the Euclidean data of the first group.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y2</code></td>
<td>

<p>A matrix containing the Euclidean data of the second group.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>The tolerance level used to terminate the Newton-Raphson algorithm.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>

<p>If R is 0, the classical chi-square distribution is used, if R = 1,
the corrected chi-square distribution (James, 1954) is used and if R = 2,
the modified F distribution (Krishnamoorthy and Yanping, 2006) is used.
If R is greater than 3 bootstrap calibration is performed.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>graph</code></td>
<td>

<p>A boolean variable which is taken into consideration only when bootstrap
calibration is performed. IF TRUE the histogram of the bootstrap test
statistic values is plotted.
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Exponential empirical likelihood or exponential tilting was first introduced by Efron (1981) as a way to perform a "tilted" version of the bootstrap for the one sample mean hypothesis testing. Similarly to the empirical likelihood, positive weights <code class="reqn">p_i</code>, which sum to one, are allocated to the observations, such that the weighted sample mean <code class="reqn">{\bf \bar{x}}</code> is equal to some population mean <code class="reqn">\pmb{\mu}</code>, under the <code class="reqn">H_0</code>. Under <code class="reqn">H_1</code> the weights are equal to <code class="reqn">\frac{1}{n}</code>, where <code class="reqn">n</code> is the sample size. Following Efron (1981), the choice of <code class="reqn">p_is</code> will minimize the Kullback-Leibler distance from <code class="reqn">H_0</code> to <code class="reqn">H_1</code>
</p>
<p style="text-align: center;"><code class="reqn">
D\left(L_0,L_1\right)=\sum_{i=1}^np_i\log\left(np_i\right),
</code>
</p>

<p>subject to the constraint <code class="reqn">\sum_{i=1}^np_i{\bf x}_i=\pmb{\mu}</code>. The probabilities take the form
</p>
<p style="text-align: center;"><code class="reqn">
p_i=\frac{e^{\pmb{\lambda}^T{\bf x}_i}}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}
</code>
</p>

<p>and the constraint becomes
</p>
<p style="text-align: center;"><code class="reqn">
\frac{\sum_{i=1}^ne^{\pmb{\lambda}^T{\bf x}_i}\left({\bf x}_i-\pmb{\mu}\right)}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}=0 \Rightarrow \frac{\sum_{i=1}^n{\bf x}_ie^{\pmb{\lambda}^T{\bf x}_i}}{\sum_{j=1}^ne^{\pmb{\lambda}^T{\bf x}_j}}-\pmb{\mu}=0.
</code>
</p>

<p>Similarly to empirical likelihood a numerical search over <code class="reqn">\pmb{\lambda}</code> is required.
</p>
<p>We can derive the asymptotic form of the test statistic in the two sample means case but in a simpler form, generalizing the approach of Jing and Robinson (1997) to the multivariate case as follows. The three constraints are
</p>
<p style="text-align: center;"><code class="reqn">
{\begin{array}{ccc}
\left(\sum_{j=1}^{n_1}e^{\pmb {\lambda}_1^T{\bf x}_j}\right)^{-1}\left(\sum_{i=1}^{n_1}{\bf x}_ie^{\pmb{\lambda}_1^T
{\bf x}_i}\right) -\pmb{\mu} &amp; = &amp; {\bf 0} \\
\left(\sum_{j=1}^{n_2}e^{\pmb {\lambda}_2^T{\bf y}_j}\right)^{-1}\left(\sum_{i=1}^{n_2}{\bf y}_ie^{\pmb{\lambda}_2^T
{\bf y}_i}\right) -\pmb{\mu} &amp; = &amp; {\bf 0} \\
n_1\pmb{\lambda}_1+n_2\pmb{\lambda}_2 &amp; = &amp; {\bf 0}.
\end{array}}
</code>
</p>

<p>Similarly to EL the sum of a linear combination of the <code class="reqn">\pmb{\lambda}s</code> is set to zero. We can equate the first two constraints of
</p>
<p style="text-align: center;"><code class="reqn">
\left(\sum_{j=1}^{n_1}e^{\pmb {\lambda}_1^T{\bf x}_j}\right)^{-1}\left(\sum_{i=1}^{n_1}{\bf x}_ie^{\pmb{\lambda}_1^T
{\bf x}_i}\right)=
\left(\sum_{j=1}^{n_2}e^{\pmb {\lambda}_2^T{\bf y}_j}\right)^{-1}\left(\sum_{i=1}^{n_2}{\bf y}_ie^{\pmb{\lambda}_2^T
{\bf y}_i}\right).
</code>
</p>

<p>Also, we can write the third constraint of as <code class="reqn">\pmb{\lambda}_2=-\frac{n_1}{n_2}\pmb{\lambda}_1</code> and thus rewrite the first two constraints as
</p>
<p style="text-align: center;"><code class="reqn">
\left(\sum_{j=1}^{n_1}e^{\pmb{\lambda}^T{\bf x}_j}\right)^{-1}\left(\sum_{i=1}^{n_1}{\bf x}_ie^{\pmb{\lambda}^T
{\bf x}_i}\right) =
\left(\sum_{j=1}^{n_2}e^{-\frac{n_1}{n_2}\pmb{\lambda}^T{\bf y}_j}\right)^{-1}\left(\sum_{i=1}^{n_2}{\bf y}_ie^{-\frac{n_1}{n_2}\pmb{\lambda}^T
{\bf y}_i}\right).
</code>
</p>

<p>This trick allows us to avoid the estimation of the common mean. It is not possible though to do this in the empirical likelihood method. Instead of minimisation of the sum of the one-sample test statistics from the common mean, we can define the probabilities by searching for the <code class="reqn">\pmb{\lambda}</code> which makes the last equation hold true. The third constraint of is a convenient constraint, but Jing and Robinson (1997) mention that even though as a constraint is simple it does not lead to second-order accurate confidence intervals unless the two sample sizes are equal. Asymptotically, the test statistic follows a <code class="reqn">\chi^2_d</code> under the null hypothesis.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>

<p>The empirical likelihood test statistic value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modif.test</code></td>
<td>

<p>The modified test statistic, either via the chi-square or the F distribution.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>dof</code></td>
<td>

<p>The degrees of freedom of the chi-square or the F distribution.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pvalue</code></td>
<td>

<p>The asymptotic or the bootstrap p-value.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mu</code></td>
<td>

<p>The estimated common mean vector.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>runtime</code></td>
<td>

<p>The runtime of the bootstrap calibration.
</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron B. (1981) Nonparametric standard errors and confidence intervals. Canadian Journal of
Statistics, 9(2): 139–158.
</p>
<p>Jing B.Y. and Wood A.T.A. (1996). Exponential empirical likelihood is
not Bartlett correctable. Annals of Statistics, 24(1): 365–369.
</p>
<p>Jing B.Y. and Robinson J. (1997). Two-sample nonparametric tilting method. Australian Journal of
Statistics, 39(1): 25–34.
</p>
<p>Owen A.B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>
<p>Preston S.P. and Wood A.T.A. (2010). Two-Sample Bootstrap Hypothesis Tests
for Three-Dimensional Labelled Landmark Data. Scandinavian Journal of
Statistics 37(4): 568–587.
</p>
<p>Tsagris M., Preston S. and Wood A.T.A. (2017). Nonparametric hypothesis
testing for equality of means on the simplex.
Journal of Statistical Computation and Simulation, 87(2): 406–422.
</p>


<h3>See Also</h3>

<p><code>el.test2, maovjames, maov, hotel2T2,
james
</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">y1 = as.matrix(iris[1:25, 1:4])
y2 = as.matrix(iris[26:50, 1:4])
eel.test2(y1, y2)
eel.test2(y1, y2 )
eel.test2( y1, y2 )
</code></pre>


</div>